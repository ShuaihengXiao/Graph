{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1b2cd63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.sparse as sp\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "731cfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_adj(edge_df):\n",
    "        \n",
    "        graph = nx.from_edgelist([(cust,opp) for cust, opp in zip(edge_df['cust_id'],edge_df['opp_id'])])\n",
    "        \n",
    "        return nx.adjacency_matrix(graph),graph.number_of_nodes()\n",
    "    \n",
    "    @staticmethod\n",
    "    def del_nodes(node_df,edge_df):\n",
    "        # node_lookup: store node index\n",
    "        node_lookup = pd.DataFrame({'node': node_df.index,}, index=node_df.cust_id)\n",
    "\n",
    "        # delete no-edge-node \n",
    "        diff_node = list(set(node_df['cust_id'])-(set(node_df['cust_id']) - set(edge_df['cust_id']) - set(edge_df['opp_id'])))\n",
    "\n",
    "        node_df = node_df.iloc[node_lookup.iloc[diff_node]['node']].reset_index(drop=True)\n",
    "        return node_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def sklearn_normalize(matrix):\n",
    "        \n",
    "        return normalize(matrix)\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_sparse_tensor(matrix, cuda: bool = False,):\n",
    "        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\n",
    "\n",
    "        Args:\n",
    "            matrix: Sparse matrix to convert.\n",
    "            cuda: Whether to move the resulting tensor to GPU.\n",
    "\n",
    "        Returns:\n",
    "            sparse_tensor: Resulting sparse tensor (on CPU or on GPU).\n",
    "\n",
    "        \"\"\"\n",
    "        if sp.issparse(matrix):\n",
    "            coo = matrix.tocoo()\n",
    "            indices = torch.LongTensor(np.vstack([coo.row, coo.col]))\n",
    "            values = torch.FloatTensor(coo.data)\n",
    "            shape = torch.Size(coo.shape)\n",
    "            sparse_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "        elif torch.is_tensor(matrix):\n",
    "            row, col = matrix.nonzero().t()\n",
    "            indices = torch.stack([row, col])\n",
    "            values = matrix[row, col]\n",
    "            shape = torch.Size(matrix.shape)\n",
    "            sparse_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "        else:\n",
    "            raise ValueError(f\"matrix must be scipy.sparse or torch.Tensor (got {type(matrix)} instead).\")\n",
    "        if cuda:\n",
    "            sparse_tensor = sparse_tensor.cuda()\n",
    "        return sparse_tensor.coalesce()\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_adj(adj : sp.csr_matrix):\n",
    "        \"\"\"Normalize adjacency matrix and convert it to a sparse tensor.\"\"\"\n",
    "        if sp.isspmatrix(adj):\n",
    "            adj = adj.tolil()\n",
    "            adj.setdiag(1)\n",
    "            adj = adj.tocsr()\n",
    "            deg = np.ravel(adj.sum(1))\n",
    "            deg_sqrt_inv = 1 / np.sqrt(deg)\n",
    "            adj_norm = adj.multiply(deg_sqrt_inv[:, None]).multiply(deg_sqrt_inv[None, :])\n",
    "        elif torch.is_tensor(adj):\n",
    "            deg = adj.sum(1)\n",
    "            deg_sqrt_inv = 1 / torch.sqrt(deg)\n",
    "            adj_norm = adj * deg_sqrt_inv[:, None] * deg_sqrt_inv[None, :]\n",
    "        return preprocessing.to_sparse_tensor(adj_norm)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19664b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from graphSAGE_v0.random_graph import random_graph_gcn\n",
    "node_df, edge_df = random_graph_gcn(1000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65921661",
   "metadata": {},
   "outputs": [],
   "source": [
    "A,N = preprocessing.get_adj(edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8198742",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = preprocessing.del_nodes(node_df,edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "735bc0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = preprocessing.sklearn_normalize(node_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4205ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = preprocessing.to_sparse_tensor(sp.csr_matrix(x_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aaa61ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeSampler(torch.utils.data.Dataset):\n",
    "    \"\"\"Sample edges and non-edges uniformly from a graph.\n",
    "\n",
    "    Args:\n",
    "        A: adjacency matrix.\n",
    "        num_pos: number of edges per batch.\n",
    "        num_neg: number of non-edges per batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, A, num_pos=1000, num_neg=1000):\n",
    "        self.num_pos = num_pos\n",
    "        self.num_neg = num_neg\n",
    "        self.A = A\n",
    "        self.edges = np.transpose(A.nonzero())\n",
    "        self.num_nodes = A.shape[0]\n",
    "        self.num_edges = self.edges.shape[0]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        np.random.seed(key)\n",
    "        edges_idx = np.random.randint(0, self.num_edges, size=self.num_pos, dtype=np.int64)\n",
    "        next_edges = self.edges[edges_idx, :]\n",
    "\n",
    "        # Select num_neg non-edges\n",
    "        generated = False\n",
    "        while not generated:\n",
    "            candidate_ne = np.random.randint(0, self.num_nodes, size=(2*self.num_neg, 2), dtype=np.int64)\n",
    "            cne1, cne2 = candidate_ne[:, 0], candidate_ne[:, 1]\n",
    "#             to_keep = (1 - self.A[cne1, cne2]).astype(np.bool).A1 * (cne1 != cne2)\n",
    "            to_keep = np.multiply((1 - self.A[cne1, cne2]).astype(np.bool),np.matrix((cne1 != cne2).astype(np.bool)))\n",
    "            to_keep = np.ravel(to_keep)\n",
    "            next_nonedges = candidate_ne[to_keep][:self.num_neg]\n",
    "            generated = to_keep.sum() >= self.num_neg\n",
    "        return torch.LongTensor(next_edges), torch.LongTensor(next_nonedges)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2**32\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        edges, nonedges = batch[0]\n",
    "        return (edges, nonedges)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_edge_sampler(A, num_pos=1000, num_neg=1000):\n",
    "        data_source = EdgeSampler(A, num_pos, num_neg)\n",
    "        return torch.utils.data.DataLoader(data_source, collate_fn = EdgeSampler.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "049b5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [128]    # hidden sizes of the GNN\n",
    "weight_decay = 1e-2     # strength of L2 regularization on GNN weights\n",
    "dropout = 0.5           # whether to use dropout\n",
    "batch_norm = True       # whether to use batch norm\n",
    "lr = 1e-3               # learning rate\n",
    "max_epochs = 500        # number of epochs to train\n",
    "display_step = 25       # how often to compute validation loss\n",
    "balance_loss = True     # whether to use balanced loss\n",
    "stochastic_loss = True  # whether to use stochastic or full-batch training\n",
    "batch_size = 20000      # batch size (only for stochastic training)\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cc6e8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = EdgeSampler.get_edge_sampler(A, batch_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2cede91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_or_dense_dropout(x, p=0.5, training=True):\n",
    "    if isinstance(x, (torch.sparse.FloatTensor, torch.cuda.sparse.FloatTensor)):\n",
    "        new_values = F.dropout(x.values(), p=p, training=training)\n",
    "#         return torch.cuda.sparse.FloatTensor(x.indices(), new_values, x.size())\n",
    "        return torch.sparse.FloatTensor(x.indices(), new_values, x.size())\n",
    "    else:\n",
    "        return F.dropout(x, p=p, training=training)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"Graph convolution layer.\n",
    "\n",
    "    Args:\n",
    "        in_features: Size of each input sample.\n",
    "        out_features: Size of each output sample.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        return adj @ (x @ self.weight) + self.bias\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"Graph convolution network.\n",
    "\n",
    "    References:\n",
    "        \"Semi-superivsed learning with graph convolutional networks\",\n",
    "        Kipf and Welling, ICLR 2017\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.5, batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        layer_dims = np.concatenate([hidden_dims, [output_dim]]).astype(np.int32)\n",
    "        self.layers = nn.ModuleList([GraphConvolution(input_dim, layer_dims[0])])\n",
    "        for idx in range(len(layer_dims) - 1):\n",
    "            self.layers.append(GraphConvolution(layer_dims[idx], layer_dims[idx + 1]))\n",
    "        if batch_norm:\n",
    "            self.batch_norm = [\n",
    "                nn.BatchNorm1d(dim, affine=False, track_running_stats=False) for dim in hidden_dims\n",
    "            ]\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        for idx, gcn in enumerate(self.layers):\n",
    "            if self.dropout != 0:\n",
    "                x = sparse_or_dense_dropout(x, p=self.dropout, training=self.training)\n",
    "            x = gcn(x, adj)\n",
    "            if idx != len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                if self.batch_norm is not None:\n",
    "                    x = self.batch_norm[idx](x)\n",
    "        return x\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"Return the weight matrices of the model.\"\"\"\n",
    "        return [w for n, w in self.named_parameters() if 'bias' not in n]\n",
    "\n",
    "    def get_biases(self):\n",
    "        \"\"\"Return the bias vectors of the model.\"\"\"\n",
    "        return [w for n, w in self.named_parameters() if 'bias' in n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec5b20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GCN(x_norm.shape[1], hidden_sizes, K, batch_norm = batch_norm, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe2d75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_norm = preprocessing.normalize_adj(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d8114978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BerpoDecoder(nn.Module):\n",
    "    def __init__(self, num_nodes, num_edges, balance_loss=False):\n",
    "        super(BerpoDecoder).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_edges = num_edges\n",
    "        self.num_possible_edges = num_nodes**2 - num_nodes\n",
    "        self.num_nonedges = self.num_possible_edges - self.num_edges\n",
    "        self.balance_loss = balance_loss\n",
    "        edge_proba = num_edges / (num_nodes**2 - num_nodes)\n",
    "        self.eps = -np.log(1 - edge_proba)\n",
    "\n",
    "    def forward_batch(self, emb, idx):\n",
    "        \"\"\"Compute probabilities of given edges.\n",
    "\n",
    "        Args:\n",
    "            emb: embedding matrix, shape (num_nodes, emb_dim)\n",
    "            idx: edge indices, shape (batch_size, 2)\n",
    "\n",
    "        Returns:\n",
    "            edge_probs: Bernoulli distribution for given edges, shape (batch_size)\n",
    "        \"\"\"\n",
    "        e1, e2 = idx.t()\n",
    "        logits = torch.sum(emb[e1] * emb[e2], dim=1)\n",
    "        logits += self.eps\n",
    "        probs = 1 - torch.exp(-logits)\n",
    "        return td.Bernoulli(probs=probs)\n",
    "\n",
    "    def forward_full(self, emb):\n",
    "        \"\"\"Compute probabilities for all edges.\n",
    "\n",
    "        Args:\n",
    "            emb: embedding matrix, shape (num_nodes, emb_dim)\n",
    "\n",
    "        Returns:\n",
    "            edge_probs: Bernoulli distribution for all edges, shape (num_nodes, num_nodes)\n",
    "        \"\"\"\n",
    "        logits = emb @ emb.t()\n",
    "        logits += self.eps\n",
    "        probs = 1 - torch.exp(-logits)\n",
    "        return td.Bernoulli(probs=probs)\n",
    "\n",
    "    def loss_batch(self, emb, ones_idx, zeros_idx):\n",
    "        \"\"\"Compute BerPo loss for a batch of edges and non-edges.\"\"\"\n",
    "        # Loss for edges\n",
    "        e1, e2 = ones_idx[:, 0], ones_idx[:, 1]\n",
    "        edge_dots = torch.sum(emb[e1] * emb[e2], dim=1)\n",
    "        loss_edges = -torch.mean(torch.log(-torch.expm1(-self.eps - edge_dots)))\n",
    "\n",
    "        # Loss for non-edges\n",
    "        ne1, ne2 = zeros_idx[:, 0], zeros_idx[:, 1]\n",
    "        loss_nonedges = torch.mean(torch.sum(emb[ne1] * emb[ne2], dim=1))\n",
    "        if self.balance_loss:\n",
    "            neg_scale = 1.0\n",
    "        else:\n",
    "            neg_scale = self.num_nonedges / self.num_edges\n",
    "        return (loss_edges + neg_scale * loss_nonedges) / (1 + neg_scale)\n",
    "\n",
    "    def loss_full(self, emb, adj):\n",
    "        \"\"\"Compute BerPo loss for all edges & non-edges in a graph.\"\"\"\n",
    "        e1, e2 = adj.nonzero()\n",
    "        edge_dots = torch.sum(emb[e1] * emb[e2], dim=1)\n",
    "        loss_edges = -torch.sum(torch.log(-torch.expm1(-self.eps - edge_dots)))\n",
    "\n",
    "        # Correct for overcounting F_u * F_v for edges and nodes with themselves\n",
    "        self_dots_sum = torch.sum(emb * emb)\n",
    "        correction = self_dots_sum + torch.sum(edge_dots)\n",
    "        sum_emb = torch.sum(emb, dim=0, keepdim=True).t()\n",
    "        loss_nonedges = torch.sum(emb @ sum_emb) - correction\n",
    "\n",
    "        if self.balance_loss:\n",
    "            neg_scale = 1.0\n",
    "        else:\n",
    "            neg_scale = self.num_nonedges / self.num_edges\n",
    "        return (loss_edges / self.num_edges + neg_scale * loss_nonedges / self.num_nonedges) / (1 + neg_scale)\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_reg_loss(model, scale=1e-5):\n",
    "        \"\"\"Get L2 loss for model weights.\"\"\"\n",
    "        loss = 0.0\n",
    "        for w in model.get_weights():\n",
    "            loss += w.pow(2.).sum()\n",
    "        return loss * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1c641277",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = BerpoDecoder(N, A.nnz, balance_loss=balance_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e791bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(gnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ce66eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss.full = 1.2935\n",
      "Epoch   25, loss.full = 1.4707\n",
      "Epoch   50, loss.full = 1.6305\n",
      "Epoch   75, loss.full = 1.6337\n",
      "Epoch  100, loss.full = 1.6109\n",
      "Epoch  125, loss.full = 1.5485\n",
      "Epoch  150, loss.full = 1.4205\n",
      "Epoch  175, loss.full = 1.3047\n",
      "Epoch  200, loss.full = 1.1131\n",
      "Epoch  225, loss.full = 0.9874\n",
      "Epoch  250, loss.full = 0.9004\n",
      "Epoch  275, loss.full = 0.8407\n",
      "Epoch  300, loss.full = 0.8115\n",
      "Epoch  325, loss.full = 0.7853\n",
      "Epoch  350, loss.full = 0.7517\n",
      "Epoch  375, loss.full = 0.7434\n",
      "Epoch  400, loss.full = 0.7369\n",
      "Epoch  425, loss.full = 0.7265\n",
      "Epoch  450, loss.full = 0.7155\n",
      "Epoch  475, loss.full = 0.7087\n",
      "Epoch  500, loss.full = 0.7083\n"
     ]
    }
   ],
   "source": [
    "for epoch, batch in enumerate(sampler):\n",
    "    #print(epoch, batch)\n",
    "    if epoch > max_epochs:\n",
    "        break\n",
    "    if epoch % 25 == 0:\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            gnn.eval()\n",
    "            # Compute validation loss\n",
    "            Z = F.relu(gnn(x_norm, adj_norm))\n",
    "            val_loss = decoder.loss_full(Z, A)\n",
    "#             print(f'Epoch {epoch:4d}, loss.full = {val_loss:.4f}, nmi = {get_nmi():.2f}')\n",
    "            print(f'Epoch {epoch:4d}, loss.full = {val_loss:.4f}')\n",
    "            # Check if it's time for early stopping / to save the model\n",
    "#             early_stopping.next_step()\n",
    "#             if early_stopping.should_save():\n",
    "#                  model_saver.save()\n",
    "#             if early_stopping.should_stop():\n",
    "#                 print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "#                 break\n",
    "            \n",
    "    # Training step\n",
    "    gnn.train()\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    \n",
    "    Z = F.relu(gnn(x_norm, adj_norm))\n",
    " \n",
    "    ones_idx, zeros_idx = batch\n",
    "    \n",
    "    if stochastic_loss:\n",
    "        loss = decoder.loss_batch(Z, ones_idx, zeros_idx)\n",
    "    else:\n",
    "        loss = decoder.loss_full(Z, A)\n",
    "    loss += BerpoDecoder.l2_reg_loss(gnn, scale=weight_decay)\n",
    "  \n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8c0cefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_model(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                node_df,\n",
    "                edge_df,\n",
    "                hidden_sizes = [128],\n",
    "                num_communities = 20,\n",
    "                weight_decay = 1e-2,\n",
    "                dropout = 0.5,\n",
    "                batch_norm = True,\n",
    "                lr = 1e-3,\n",
    "                max_epochs = 500,\n",
    "                balance_loss = True,\n",
    "                stochastic_loss = True,\n",
    "                batch_size = 20000):\n",
    "        \n",
    "        self.node_df = node_df\n",
    "        self.edge_df = edge_df\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_communities = num_communities\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.lr = lr\n",
    "        self.max_epochs = max_epochs\n",
    "        self.balance_loss = balance_loss\n",
    "        self.stochastic_loss = stochastic_loss\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "        print('preprocessing step')\n",
    "        self.A,self.N = preprocessing.get_adj(self.edge_df)\n",
    "        self.node_df = preprocessing.del_nodes(self.node_df,self.edge_df)\n",
    "        self.x_norm = preprocessing.sklearn_normalize(self.node_df)\n",
    "        self.x_norm = preprocessing.to_sparse_tensor(sp.csr_matrix(self.x_norm))\n",
    "        self.sampler = EdgeSampler.get_edge_sampler(self.A, self.batch_size, self.batch_size)\n",
    "        self.gnn = GCN(self.x_norm.shape[1], \n",
    "                  self.hidden_sizes, \n",
    "                  self.num_communities, \n",
    "                  batch_norm = self.batch_norm, \n",
    "                  dropout = self.dropout)\n",
    "        self.adj_norm = preprocessing.normalize_adj(self.A)\n",
    "        self.decoder = BerpoDecoder(self.N, self.A.nnz, balance_loss = self.balance_loss)\n",
    "        self.opt = torch.optim.Adam(self.gnn.parameters(), lr = self.lr)\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch, batch in enumerate(self.sampler):\n",
    "\n",
    "            if epoch > self.max_epochs:\n",
    "                break\n",
    "            if epoch % 25 == 0:\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.gnn.eval()\n",
    "\n",
    "                    self.Z = F.relu(self.gnn(self.x_norm, self.adj_norm))\n",
    "                    val_loss = self.decoder.loss_full(self.Z, self.A)\n",
    "\n",
    "                    print(f'Epoch {epoch:4d}, loss.full = {val_loss:.4f}')\n",
    "\n",
    "            # Training step\n",
    "            self.gnn.train()\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            self.Z = F.relu(self.gnn(self.x_norm, self.adj_norm))\n",
    "\n",
    "            ones_idx, zeros_idx = batch\n",
    "            if self.stochastic_loss:\n",
    "                loss = self.decoder.loss_batch(self.Z, ones_idx, zeros_idx)\n",
    "                loss = self.decoder.loss_full(self.Z, self.A)\n",
    "            loss += BerpoDecoder.l2_reg_loss(self.gnn, scale = self.weight_decay)\n",
    "\n",
    "            loss.backward()\n",
    "            self.opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7bf94a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing step\n"
     ]
    }
   ],
   "source": [
    "model = run_model(node_df,edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "482a1f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss.full = 1.2404\n",
      "Epoch   25, loss.full = 1.3781\n",
      "Epoch   50, loss.full = 1.2884\n",
      "Epoch   75, loss.full = 1.2509\n",
      "Epoch  100, loss.full = 1.1021\n",
      "Epoch  125, loss.full = 0.9847\n",
      "Epoch  150, loss.full = 0.9233\n",
      "Epoch  175, loss.full = 0.8705\n",
      "Epoch  200, loss.full = 0.8095\n",
      "Epoch  225, loss.full = 0.7736\n",
      "Epoch  250, loss.full = 0.7499\n",
      "Epoch  275, loss.full = 0.7294\n",
      "Epoch  300, loss.full = 0.7161\n",
      "Epoch  325, loss.full = 0.7119\n",
      "Epoch  350, loss.full = 0.7002\n",
      "Epoch  375, loss.full = 0.6919\n",
      "Epoch  400, loss.full = 0.6874\n",
      "Epoch  425, loss.full = 0.6892\n",
      "Epoch  450, loss.full = 0.6834\n",
      "Epoch  475, loss.full = 0.6808\n",
      "Epoch  500, loss.full = 0.6784\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de6a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NOCD import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41b54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing step\n"
     ]
    }
   ],
   "source": [
    "model = run_model(node_df,edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ad451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss.full = 1.3085\n",
      "Epoch   25, loss.full = 1.1963\n",
      "Epoch   50, loss.full = 1.1764\n",
      "Epoch   75, loss.full = 1.1371\n",
      "Epoch  100, loss.full = 1.0589\n",
      "Epoch  125, loss.full = 0.9658\n",
      "Epoch  150, loss.full = 0.9092\n",
      "Epoch  175, loss.full = 0.8569\n",
      "Epoch  200, loss.full = 0.8026\n",
      "Epoch  225, loss.full = 0.7747\n",
      "Epoch  250, loss.full = 0.7477\n",
      "Epoch  275, loss.full = 0.7308\n",
      "Epoch  300, loss.full = 0.7150\n",
      "Epoch  325, loss.full = 0.7089\n",
      "Epoch  350, loss.full = 0.6964\n",
      "Epoch  375, loss.full = 0.6942\n",
      "Epoch  400, loss.full = 0.6869\n",
      "Epoch  425, loss.full = 0.6863\n",
      "Epoch  450, loss.full = 0.6827\n",
      "Epoch  475, loss.full = 0.6844\n",
      "Epoch  500, loss.full = 0.6805\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc98ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
