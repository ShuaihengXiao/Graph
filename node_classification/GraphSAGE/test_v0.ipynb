{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be73968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6609bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a78b1fb2bab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a42744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182189a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb75566",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df,edge_df = random_graph.random_graph_gcn(1000,10000,0.5,nums_features=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2675b842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(node_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5e3b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cust_id', 'opp_id', 'weight'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb95e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f061e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing..\n",
      "data preprocessing complete!\n",
      "-----------------------*-----------------------\n",
      "after filtering single nodes\n",
      "num of train instances: 557\n",
      "num of test instances: 443\n",
      "-----------------------*-----------------------\n",
      "model structure\n",
      "GraphSage(\n",
      "  in_features=120, num_neighbors_list=[10, 10]\n",
      "  (gcn): ModuleList(\n",
      "    (0): SageGCN(\n",
      "      in_features=120, out_features=512, aggr_hidden_method=concat\n",
      "      (aggregator): NeighborAggregator(in_features=120, out_features=256, aggr_method=max)\n",
      "      (linear_1): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): SageGCN(\n",
      "      in_features=128, out_features=8, aggr_hidden_method=concat\n",
      "      (aggregator): NeighborAggregator(in_features=128, out_features=4, aggr_method=max)\n",
      "      (linear_1): Linear(in_features=6, out_features=2, bias=True)\n",
      "      (linear_2): Linear(in_features=2, out_features=2, bias=True)\n",
      "      (bn_1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a = sage_v2.run_model(node_df,edge_df,aggr_neighbor_method='max',\n",
    "    aggr_hidden_method='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2bb92b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training..\n",
      "training through cpu\n",
      "Epoch 000 Batch 000 Loss: 26.9460\n",
      "Epoch 000 Batch 001 Loss: 56.6050\n",
      "Epoch 000 Batch 002 Loss: 0.6904\n",
      "Epoch 000 Batch 003 Loss: 0.6900\n",
      "Epoch 000 Batch 004 Loss: 0.6918\n",
      "Epoch 000 Batch 005 Loss: 0.6877\n",
      "Epoch 000 Batch 006 Loss: 0.6811\n",
      "Epoch 000 Batch 007 Loss: 0.6871\n",
      "Epoch 000 Batch 008 Loss: 0.6897\n",
      "Epoch 000 Batch 009 Loss: 0.6892\n",
      "Epoch 000 Batch 010 Loss: 0.6833\n",
      "Epoch 000 Batch 011 Loss: 0.6943\n",
      "Epoch 000 Batch 012 Loss: 0.7081\n",
      "Epoch 000 Batch 013 Loss: 0.6805\n",
      "Epoch 000 Batch 014 Loss: 0.6723\n",
      "Epoch 000 Batch 015 Loss: 0.6953\n",
      "Epoch 000 Batch 016 Loss: 0.7041\n",
      "Epoch 000 Batch 017 Loss: 0.6868\n",
      "Epoch 000 Batch 018 Loss: 0.7143\n",
      "Epoch 000 Batch 019 Loss: 0.6959\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.42437922954559326\n",
      "-----------------------*-----------------------\n",
      "Epoch 001 Batch 000 Loss: 0.7360\n",
      "Epoch 001 Batch 001 Loss: 0.7051\n",
      "Epoch 001 Batch 002 Loss: 0.7041\n",
      "Epoch 001 Batch 003 Loss: 0.6932\n",
      "Epoch 001 Batch 004 Loss: 0.6829\n",
      "Epoch 001 Batch 005 Loss: 0.6743\n",
      "Epoch 001 Batch 006 Loss: 0.6677\n",
      "Epoch 001 Batch 007 Loss: 0.6398\n",
      "Epoch 001 Batch 008 Loss: 0.6918\n",
      "Epoch 001 Batch 009 Loss: 0.8029\n",
      "Epoch 001 Batch 010 Loss: 0.5518\n",
      "Epoch 001 Batch 011 Loss: 0.6633\n",
      "Epoch 001 Batch 012 Loss: 0.6226\n",
      "Epoch 001 Batch 013 Loss: 0.4451\n",
      "Epoch 001 Batch 014 Loss: 0.8136\n",
      "Epoch 001 Batch 015 Loss: 0.8693\n",
      "Epoch 001 Batch 016 Loss: 0.4260\n",
      "Epoch 001 Batch 017 Loss: 0.7700\n",
      "Epoch 001 Batch 018 Loss: 0.6211\n",
      "Epoch 001 Batch 019 Loss: 0.6211\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 002 Batch 000 Loss: 0.6211\n",
      "Epoch 002 Batch 001 Loss: 0.6211\n",
      "Epoch 002 Batch 002 Loss: 0.6691\n",
      "Epoch 002 Batch 003 Loss: 0.9057\n",
      "Epoch 002 Batch 004 Loss: 0.6664\n",
      "Epoch 002 Batch 005 Loss: 0.7489\n",
      "Epoch 002 Batch 006 Loss: 0.8184\n",
      "Epoch 002 Batch 007 Loss: 0.5582\n",
      "Epoch 002 Batch 008 Loss: 0.7248\n",
      "Epoch 002 Batch 009 Loss: 0.6619\n",
      "Epoch 002 Batch 010 Loss: 0.6628\n",
      "Epoch 002 Batch 011 Loss: 0.6870\n",
      "Epoch 002 Batch 012 Loss: 0.6447\n",
      "Epoch 002 Batch 013 Loss: 0.6474\n",
      "Epoch 002 Batch 014 Loss: 0.6672\n",
      "Epoch 002 Batch 015 Loss: 0.6855\n",
      "Epoch 002 Batch 016 Loss: 0.6680\n",
      "Epoch 002 Batch 017 Loss: 0.6337\n",
      "Epoch 002 Batch 018 Loss: 0.6674\n",
      "Epoch 002 Batch 019 Loss: 0.7048\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 003 Batch 000 Loss: 0.6858\n",
      "Epoch 003 Batch 001 Loss: 0.6859\n",
      "Epoch 003 Batch 002 Loss: 0.7063\n",
      "Epoch 003 Batch 003 Loss: 0.6658\n",
      "Epoch 003 Batch 004 Loss: 0.6456\n",
      "Epoch 003 Batch 005 Loss: 0.7070\n",
      "Epoch 003 Batch 006 Loss: 0.6652\n",
      "Epoch 003 Batch 007 Loss: 0.6436\n",
      "Epoch 003 Batch 008 Loss: 0.6866\n",
      "Epoch 003 Batch 009 Loss: 0.6869\n",
      "Epoch 003 Batch 010 Loss: 0.6638\n",
      "Epoch 003 Batch 011 Loss: 0.6635\n",
      "Epoch 003 Batch 012 Loss: 0.6140\n",
      "Epoch 003 Batch 013 Loss: 0.7148\n",
      "Epoch 003 Batch 014 Loss: 0.7161\n",
      "Epoch 003 Batch 015 Loss: 0.6354\n",
      "Epoch 003 Batch 016 Loss: 0.6347\n",
      "Epoch 003 Batch 017 Loss: 0.7188\n",
      "Epoch 003 Batch 018 Loss: 0.7478\n",
      "Epoch 003 Batch 019 Loss: 0.6621\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 004 Batch 000 Loss: 0.6351\n",
      "Epoch 004 Batch 001 Loss: 0.6352\n",
      "Epoch 004 Batch 002 Loss: 0.7170\n",
      "Epoch 004 Batch 003 Loss: 0.6894\n",
      "Epoch 004 Batch 004 Loss: 0.7156\n",
      "Epoch 004 Batch 005 Loss: 0.6883\n",
      "Epoch 004 Batch 006 Loss: 0.7362\n",
      "Epoch 004 Batch 007 Loss: 0.6644\n",
      "Epoch 004 Batch 008 Loss: 0.6654\n",
      "Epoch 004 Batch 009 Loss: 0.6858\n",
      "Epoch 004 Batch 010 Loss: 0.7407\n",
      "Epoch 004 Batch 011 Loss: 0.7178\n",
      "Epoch 004 Batch 012 Loss: 0.6719\n",
      "Epoch 004 Batch 013 Loss: 0.6743\n",
      "Epoch 004 Batch 014 Loss: 0.6965\n",
      "Epoch 004 Batch 015 Loss: 0.6868\n",
      "Epoch 004 Batch 016 Loss: 0.7026\n",
      "Epoch 004 Batch 017 Loss: 0.6943\n",
      "Epoch 004 Batch 018 Loss: 0.6848\n",
      "Epoch 004 Batch 019 Loss: 0.6861\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 005 Batch 000 Loss: 0.6863\n",
      "Epoch 005 Batch 001 Loss: 0.6642\n",
      "Epoch 005 Batch 002 Loss: 0.6738\n",
      "Epoch 005 Batch 003 Loss: 0.6965\n",
      "Epoch 005 Batch 004 Loss: 0.6594\n",
      "Epoch 005 Batch 005 Loss: 0.6527\n",
      "Epoch 005 Batch 006 Loss: 0.6461\n",
      "Epoch 005 Batch 007 Loss: 0.6399\n",
      "Epoch 005 Batch 008 Loss: 0.6344\n",
      "Epoch 005 Batch 009 Loss: 0.6616\n",
      "Epoch 005 Batch 010 Loss: 0.6266\n",
      "Epoch 005 Batch 011 Loss: 0.6630\n",
      "Epoch 005 Batch 012 Loss: 0.6225\n",
      "Epoch 005 Batch 013 Loss: 0.8016\n",
      "Epoch 005 Batch 014 Loss: 0.5751\n",
      "Epoch 005 Batch 015 Loss: 0.7167\n",
      "Epoch 005 Batch 016 Loss: 0.7181\n",
      "Epoch 005 Batch 017 Loss: 0.8145\n",
      "Epoch 005 Batch 018 Loss: 0.8085\n",
      "Epoch 005 Batch 019 Loss: 0.6218\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 006 Batch 000 Loss: 0.7479\n",
      "Epoch 006 Batch 001 Loss: 0.6630\n",
      "Epoch 006 Batch 002 Loss: 0.6981\n",
      "Epoch 006 Batch 003 Loss: 0.6616\n",
      "Epoch 006 Batch 004 Loss: 0.6921\n",
      "Epoch 006 Batch 005 Loss: 0.7736\n",
      "Epoch 006 Batch 006 Loss: 0.7608\n",
      "Epoch 006 Batch 007 Loss: 0.6860\n",
      "Epoch 006 Batch 008 Loss: 0.7182\n",
      "Epoch 006 Batch 009 Loss: 0.6360\n",
      "Epoch 006 Batch 010 Loss: 0.6863\n",
      "Epoch 006 Batch 011 Loss: 0.6871\n",
      "Epoch 006 Batch 012 Loss: 0.6749\n",
      "Epoch 006 Batch 013 Loss: 0.6884\n",
      "Epoch 006 Batch 014 Loss: 0.6887\n",
      "Epoch 006 Batch 015 Loss: 0.6838\n",
      "Epoch 006 Batch 016 Loss: 0.6673\n",
      "Epoch 006 Batch 017 Loss: 0.6809\n",
      "Epoch 006 Batch 018 Loss: 0.7128\n",
      "Epoch 006 Batch 019 Loss: 0.7151\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 007 Batch 000 Loss: 0.6576\n",
      "Epoch 007 Batch 001 Loss: 0.6437\n",
      "Epoch 007 Batch 002 Loss: 0.6603\n",
      "Epoch 007 Batch 003 Loss: 0.6853\n",
      "Epoch 007 Batch 004 Loss: 0.6508\n",
      "Epoch 007 Batch 005 Loss: 0.6661\n",
      "Epoch 007 Batch 006 Loss: 0.5754\n",
      "Epoch 007 Batch 007 Loss: 0.6627\n",
      "Epoch 007 Batch 008 Loss: 0.6618\n",
      "Epoch 007 Batch 009 Loss: 0.6616\n",
      "Epoch 007 Batch 010 Loss: 0.6964\n",
      "Epoch 007 Batch 011 Loss: 0.7357\n",
      "Epoch 007 Batch 012 Loss: 0.5868\n",
      "Epoch 007 Batch 013 Loss: 0.5447\n",
      "Epoch 007 Batch 014 Loss: 0.7478\n",
      "Epoch 007 Batch 015 Loss: 0.6651\n",
      "Epoch 007 Batch 016 Loss: 0.7971\n",
      "Epoch 007 Batch 017 Loss: 0.5350\n",
      "Epoch 007 Batch 018 Loss: 0.6658\n",
      "Epoch 007 Batch 019 Loss: 0.7540\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 008 Batch 000 Loss: 0.7522\n",
      "Epoch 008 Batch 001 Loss: 0.7486\n",
      "Epoch 008 Batch 002 Loss: 0.7035\n",
      "Epoch 008 Batch 003 Loss: 0.7004\n",
      "Epoch 008 Batch 004 Loss: 0.6619\n",
      "Epoch 008 Batch 005 Loss: 0.7617\n",
      "Epoch 008 Batch 006 Loss: 0.6616\n",
      "Epoch 008 Batch 007 Loss: 0.6620\n",
      "Epoch 008 Batch 008 Loss: 0.6365\n",
      "Epoch 008 Batch 009 Loss: 0.7123\n",
      "Epoch 008 Batch 010 Loss: 0.6186\n",
      "Epoch 008 Batch 011 Loss: 0.7520\n",
      "Epoch 008 Batch 012 Loss: 0.6053\n",
      "Epoch 008 Batch 013 Loss: 0.7250\n",
      "Epoch 008 Batch 014 Loss: 0.7226\n",
      "Epoch 008 Batch 015 Loss: 0.6343\n",
      "Epoch 008 Batch 016 Loss: 0.6853\n",
      "Epoch 008 Batch 017 Loss: 0.6220\n",
      "Epoch 008 Batch 018 Loss: 0.6691\n",
      "Epoch 008 Batch 019 Loss: 0.6685\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 009 Batch 000 Loss: 0.6678\n",
      "Epoch 009 Batch 001 Loss: 0.6298\n",
      "Epoch 009 Batch 002 Loss: 0.6458\n",
      "Epoch 009 Batch 003 Loss: 0.6646\n",
      "Epoch 009 Batch 004 Loss: 0.7109\n",
      "Epoch 009 Batch 005 Loss: 0.5886\n",
      "Epoch 009 Batch 006 Loss: 0.5553\n",
      "Epoch 009 Batch 007 Loss: 0.7505\n",
      "Epoch 009 Batch 008 Loss: 0.6930\n",
      "Epoch 009 Batch 009 Loss: 0.6616\n",
      "Epoch 009 Batch 010 Loss: 0.6617\n",
      "Epoch 009 Batch 011 Loss: 0.6619\n",
      "Epoch 009 Batch 012 Loss: 0.6982\n",
      "Epoch 009 Batch 013 Loss: 0.6989\n",
      "Epoch 009 Batch 014 Loss: 0.7726\n",
      "Epoch 009 Batch 015 Loss: 0.7702\n",
      "Epoch 009 Batch 016 Loss: 0.6963\n",
      "Epoch 009 Batch 017 Loss: 0.6616\n",
      "Epoch 009 Batch 018 Loss: 0.7870\n",
      "Epoch 009 Batch 019 Loss: 0.6908\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 010 Batch 000 Loss: 0.7159\n",
      "Epoch 010 Batch 001 Loss: 0.6633\n",
      "Epoch 010 Batch 002 Loss: 0.6422\n",
      "Epoch 010 Batch 003 Loss: 0.6446\n",
      "Epoch 010 Batch 004 Loss: 0.6462\n",
      "Epoch 010 Batch 005 Loss: 0.6469\n",
      "Epoch 010 Batch 006 Loss: 0.6663\n",
      "Epoch 010 Batch 007 Loss: 0.6269\n",
      "Epoch 010 Batch 008 Loss: 0.6860\n",
      "Epoch 010 Batch 009 Loss: 0.6651\n",
      "Epoch 010 Batch 010 Loss: 0.7302\n",
      "Epoch 010 Batch 011 Loss: 0.6865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 Batch 012 Loss: 0.6646\n",
      "Epoch 010 Batch 013 Loss: 0.6866\n",
      "Epoch 010 Batch 014 Loss: 0.7088\n",
      "Epoch 010 Batch 015 Loss: 0.7084\n",
      "Epoch 010 Batch 016 Loss: 0.7502\n",
      "Epoch 010 Batch 017 Loss: 0.7059\n",
      "Epoch 010 Batch 018 Loss: 0.6670\n",
      "Epoch 010 Batch 019 Loss: 0.6331\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 011 Batch 000 Loss: 0.6854\n",
      "Epoch 011 Batch 001 Loss: 0.6853\n",
      "Epoch 011 Batch 002 Loss: 0.6195\n",
      "Epoch 011 Batch 003 Loss: 0.7195\n",
      "Epoch 011 Batch 004 Loss: 0.7025\n",
      "Epoch 011 Batch 005 Loss: 0.6684\n",
      "Epoch 011 Batch 006 Loss: 0.6854\n",
      "Epoch 011 Batch 007 Loss: 0.6854\n",
      "Epoch 011 Batch 008 Loss: 0.7024\n",
      "Epoch 011 Batch 009 Loss: 0.6518\n",
      "Epoch 011 Batch 010 Loss: 0.6343\n",
      "Epoch 011 Batch 011 Loss: 0.7033\n",
      "Epoch 011 Batch 012 Loss: 0.7039\n",
      "Epoch 011 Batch 013 Loss: 0.6298\n",
      "Epoch 011 Batch 014 Loss: 0.6857\n",
      "Epoch 011 Batch 015 Loss: 0.6457\n",
      "Epoch 011 Batch 016 Loss: 0.7707\n",
      "Epoch 011 Batch 017 Loss: 0.7493\n",
      "Epoch 011 Batch 018 Loss: 0.7667\n",
      "Epoch 011 Batch 019 Loss: 0.6486\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 012 Batch 000 Loss: 0.7027\n",
      "Epoch 012 Batch 001 Loss: 0.6853\n",
      "Epoch 012 Batch 002 Loss: 0.6555\n",
      "Epoch 012 Batch 003 Loss: 0.6854\n",
      "Epoch 012 Batch 004 Loss: 0.6301\n",
      "Epoch 012 Batch 005 Loss: 0.6711\n",
      "Epoch 012 Batch 006 Loss: 0.6853\n",
      "Epoch 012 Batch 007 Loss: 0.6542\n",
      "Epoch 012 Batch 008 Loss: 0.6688\n",
      "Epoch 012 Batch 009 Loss: 0.7207\n",
      "Epoch 012 Batch 010 Loss: 0.7037\n",
      "Epoch 012 Batch 011 Loss: 0.6488\n",
      "Epoch 012 Batch 012 Loss: 0.6477\n",
      "Epoch 012 Batch 013 Loss: 0.6460\n",
      "Epoch 012 Batch 014 Loss: 0.7074\n",
      "Epoch 012 Batch 015 Loss: 0.6646\n",
      "Epoch 012 Batch 016 Loss: 0.6412\n",
      "Epoch 012 Batch 017 Loss: 0.5913\n",
      "Epoch 012 Batch 018 Loss: 0.7145\n",
      "Epoch 012 Batch 019 Loss: 0.5806\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 013 Batch 000 Loss: 0.5743\n",
      "Epoch 013 Batch 001 Loss: 0.5347\n",
      "Epoch 013 Batch 002 Loss: 0.6618\n",
      "Epoch 013 Batch 003 Loss: 0.7006\n",
      "Epoch 013 Batch 004 Loss: 0.7443\n",
      "Epoch 013 Batch 005 Loss: 0.7479\n",
      "Epoch 013 Batch 006 Loss: 0.6648\n",
      "Epoch 013 Batch 007 Loss: 0.6223\n",
      "Epoch 013 Batch 008 Loss: 0.7086\n",
      "Epoch 013 Batch 009 Loss: 0.7085\n",
      "Epoch 013 Batch 010 Loss: 0.5369\n",
      "Epoch 013 Batch 011 Loss: 0.7938\n",
      "Epoch 013 Batch 012 Loss: 0.8331\n",
      "Epoch 013 Batch 013 Loss: 0.6234\n",
      "Epoch 013 Batch 014 Loss: 0.7016\n",
      "Epoch 013 Batch 015 Loss: 0.5885\n",
      "Epoch 013 Batch 016 Loss: 0.8050\n",
      "Epoch 013 Batch 017 Loss: 0.5942\n",
      "Epoch 013 Batch 018 Loss: 0.7586\n",
      "Epoch 013 Batch 019 Loss: 0.5095\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 014 Batch 000 Loss: 0.7513\n",
      "Epoch 014 Batch 001 Loss: 0.6619\n",
      "Epoch 014 Batch 002 Loss: 0.7454\n",
      "Epoch 014 Batch 003 Loss: 0.6100\n",
      "Epoch 014 Batch 004 Loss: 0.7138\n",
      "Epoch 014 Batch 005 Loss: 0.6633\n",
      "Epoch 014 Batch 006 Loss: 0.7343\n",
      "Epoch 014 Batch 007 Loss: 0.6422\n",
      "Epoch 014 Batch 008 Loss: 0.6436\n",
      "Epoch 014 Batch 009 Loss: 0.6442\n",
      "Epoch 014 Batch 010 Loss: 0.6862\n",
      "Epoch 014 Batch 011 Loss: 0.7697\n",
      "Epoch 014 Batch 012 Loss: 0.6261\n",
      "Epoch 014 Batch 013 Loss: 0.6858\n",
      "Epoch 014 Batch 014 Loss: 0.7050\n",
      "Epoch 014 Batch 015 Loss: 0.7043\n",
      "Epoch 014 Batch 016 Loss: 0.7035\n",
      "Epoch 014 Batch 017 Loss: 0.6854\n",
      "Epoch 014 Batch 018 Loss: 0.6528\n",
      "Epoch 014 Batch 019 Loss: 0.6375\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 015 Batch 000 Loss: 0.7016\n",
      "Epoch 015 Batch 001 Loss: 0.6690\n",
      "Epoch 015 Batch 002 Loss: 0.6688\n",
      "Epoch 015 Batch 003 Loss: 0.7193\n",
      "Epoch 015 Batch 004 Loss: 0.6854\n",
      "Epoch 015 Batch 005 Loss: 0.6348\n",
      "Epoch 015 Batch 006 Loss: 0.7202\n",
      "Epoch 015 Batch 007 Loss: 0.6156\n",
      "Epoch 015 Batch 008 Loss: 0.6306\n",
      "Epoch 015 Batch 009 Loss: 0.7055\n",
      "Epoch 015 Batch 010 Loss: 0.7273\n",
      "Epoch 015 Batch 011 Loss: 0.7072\n",
      "Epoch 015 Batch 012 Loss: 0.6230\n",
      "Epoch 015 Batch 013 Loss: 0.6431\n",
      "Epoch 015 Batch 014 Loss: 0.6642\n",
      "Epoch 015 Batch 015 Loss: 0.6637\n",
      "Epoch 015 Batch 016 Loss: 0.6877\n",
      "Epoch 015 Batch 017 Loss: 0.6377\n",
      "Epoch 015 Batch 018 Loss: 0.7673\n",
      "Epoch 015 Batch 019 Loss: 0.7151\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 016 Batch 000 Loss: 0.6886\n",
      "Epoch 016 Batch 001 Loss: 0.6116\n",
      "Epoch 016 Batch 002 Loss: 0.6112\n",
      "Epoch 016 Batch 003 Loss: 0.6625\n",
      "Epoch 016 Batch 004 Loss: 0.6352\n",
      "Epoch 016 Batch 005 Loss: 0.7180\n",
      "Epoch 016 Batch 006 Loss: 0.6336\n",
      "Epoch 016 Batch 007 Loss: 0.6328\n",
      "Epoch 016 Batch 008 Loss: 0.6617\n",
      "Epoch 016 Batch 009 Loss: 0.6923\n",
      "Epoch 016 Batch 010 Loss: 0.5681\n",
      "Epoch 016 Batch 011 Loss: 0.6939\n",
      "Epoch 016 Batch 012 Loss: 0.5954\n",
      "Epoch 016 Batch 013 Loss: 0.6274\n",
      "Epoch 016 Batch 014 Loss: 0.6264\n",
      "Epoch 016 Batch 015 Loss: 0.5885\n",
      "Epoch 016 Batch 016 Loss: 0.6629\n",
      "Epoch 016 Batch 017 Loss: 0.6635\n",
      "Epoch 016 Batch 018 Loss: 0.7051\n",
      "Epoch 016 Batch 019 Loss: 0.7477\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 017 Batch 000 Loss: 0.7059\n",
      "Epoch 017 Batch 001 Loss: 0.6641\n",
      "Epoch 017 Batch 002 Loss: 0.6232\n",
      "Epoch 017 Batch 003 Loss: 0.6636\n",
      "Epoch 017 Batch 004 Loss: 0.5442\n",
      "Epoch 017 Batch 005 Loss: 0.7433\n",
      "Epoch 017 Batch 006 Loss: 0.5447\n",
      "Epoch 017 Batch 007 Loss: 0.5839\n",
      "Epoch 017 Batch 008 Loss: 0.6637\n",
      "Epoch 017 Batch 009 Loss: 0.7863\n",
      "Epoch 017 Batch 010 Loss: 0.5830\n",
      "Epoch 017 Batch 011 Loss: 0.5429\n",
      "Epoch 017 Batch 012 Loss: 0.7047\n",
      "Epoch 017 Batch 013 Loss: 0.6230\n",
      "Epoch 017 Batch 014 Loss: 0.6641\n",
      "Epoch 017 Batch 015 Loss: 0.5817\n",
      "Epoch 017 Batch 016 Loss: 0.6227\n",
      "Epoch 017 Batch 017 Loss: 0.5382\n",
      "Epoch 017 Batch 018 Loss: 0.6653\n",
      "Epoch 017 Batch 019 Loss: 0.7535\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 018 Batch 000 Loss: 0.6658\n",
      "Epoch 018 Batch 001 Loss: 0.6219\n",
      "Epoch 018 Batch 002 Loss: 0.8407\n",
      "Epoch 018 Batch 003 Loss: 0.6653\n",
      "Epoch 018 Batch 004 Loss: 0.6650\n",
      "Epoch 018 Batch 005 Loss: 0.6647\n",
      "Epoch 018 Batch 006 Loss: 0.7060\n",
      "Epoch 018 Batch 007 Loss: 0.6230\n",
      "Epoch 018 Batch 008 Loss: 0.7042\n",
      "Epoch 018 Batch 009 Loss: 0.5439\n",
      "Epoch 018 Batch 010 Loss: 0.6633\n",
      "Epoch 018 Batch 011 Loss: 0.7024\n",
      "Epoch 018 Batch 012 Loss: 0.7017\n",
      "Epoch 018 Batch 013 Loss: 0.7392\n",
      "Epoch 018 Batch 014 Loss: 0.7000\n",
      "Epoch 018 Batch 015 Loss: 0.6255\n",
      "Epoch 018 Batch 016 Loss: 0.6621\n",
      "Epoch 018 Batch 017 Loss: 0.7329\n",
      "Epoch 018 Batch 018 Loss: 0.5576\n",
      "Epoch 018 Batch 019 Loss: 0.6617\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "Epoch 019 Batch 000 Loss: 0.6956\n",
      "Epoch 019 Batch 001 Loss: 0.6281\n",
      "Epoch 019 Batch 002 Loss: 0.5952\n",
      "Epoch 019 Batch 003 Loss: 0.6947\n",
      "Epoch 019 Batch 004 Loss: 0.6287\n",
      "Epoch 019 Batch 005 Loss: 0.7273\n",
      "Epoch 019 Batch 006 Loss: 0.7593\n",
      "Epoch 019 Batch 007 Loss: 0.6296\n",
      "Epoch 019 Batch 008 Loss: 0.5983\n",
      "Epoch 019 Batch 009 Loss: 0.6301\n",
      "Epoch 019 Batch 010 Loss: 0.6301\n",
      "Epoch 019 Batch 011 Loss: 0.7561\n",
      "Epoch 019 Batch 012 Loss: 0.6928\n",
      "Epoch 019 Batch 013 Loss: 0.6925\n",
      "Epoch 019 Batch 014 Loss: 0.5393\n",
      "Epoch 019 Batch 015 Loss: 0.6923\n",
      "Epoch 019 Batch 016 Loss: 0.6616\n",
      "Epoch 019 Batch 017 Loss: 0.6616\n",
      "Epoch 019 Batch 018 Loss: 0.7228\n",
      "Epoch 019 Batch 019 Loss: 0.7529\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5756207704544067\n",
      "-----------------------*-----------------------\n",
      "training complete!\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c21317c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.Tensor(np.array(node_df.iloc[:,3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "765116d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-33-d3a7aa59ba1d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-d3a7aa59ba1d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    b = a.max(dim = 1,1)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "b = a.max(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1aa37780",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.max(a,dim = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cc6f31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先做节点的lookup，然后根据lookup做adjacency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48fe3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = node_df.is_driver.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1ec358a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False,  True, False])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d1153df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 4996, 4997, 4999])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(train_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3972730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_lookup = pd.DataFrame({'node': node_df.index,}, index=node_df.cust_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f887e5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>is_driver</th>\n",
       "      <th>is_reported</th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>feat_94</th>\n",
       "      <th>feat_95</th>\n",
       "      <th>feat_96</th>\n",
       "      <th>feat_97</th>\n",
       "      <th>feat_98</th>\n",
       "      <th>feat_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>1537</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.246609</td>\n",
       "      <td>-0.899651</td>\n",
       "      <td>-1.913527</td>\n",
       "      <td>-1.003489</td>\n",
       "      <td>0.139554</td>\n",
       "      <td>-0.918374</td>\n",
       "      <td>0.687188</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.185621</td>\n",
       "      <td>1.912303</td>\n",
       "      <td>-0.016174</td>\n",
       "      <td>-0.208349</td>\n",
       "      <td>-1.036000</td>\n",
       "      <td>0.767648</td>\n",
       "      <td>-0.074362</td>\n",
       "      <td>-0.505138</td>\n",
       "      <td>0.682445</td>\n",
       "      <td>-0.028647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>1027</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.292257</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.735129</td>\n",
       "      <td>-0.967637</td>\n",
       "      <td>0.615207</td>\n",
       "      <td>0.621669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665107</td>\n",
       "      <td>0.918564</td>\n",
       "      <td>-0.452172</td>\n",
       "      <td>-0.927845</td>\n",
       "      <td>-0.621774</td>\n",
       "      <td>-0.532683</td>\n",
       "      <td>0.355407</td>\n",
       "      <td>-0.714893</td>\n",
       "      <td>-1.586593</td>\n",
       "      <td>-1.722829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.215422</td>\n",
       "      <td>1.396370</td>\n",
       "      <td>0.209891</td>\n",
       "      <td>1.307926</td>\n",
       "      <td>2.053852</td>\n",
       "      <td>1.586077</td>\n",
       "      <td>0.212647</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.091191</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.344992</td>\n",
       "      <td>-0.958287</td>\n",
       "      <td>0.858661</td>\n",
       "      <td>-0.486853</td>\n",
       "      <td>0.514684</td>\n",
       "      <td>2.220023</td>\n",
       "      <td>0.712551</td>\n",
       "      <td>0.970895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>392</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.010047</td>\n",
       "      <td>-0.607880</td>\n",
       "      <td>-0.402684</td>\n",
       "      <td>1.840380</td>\n",
       "      <td>1.172264</td>\n",
       "      <td>1.317641</td>\n",
       "      <td>1.540145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423845</td>\n",
       "      <td>-0.296341</td>\n",
       "      <td>0.979252</td>\n",
       "      <td>-1.851187</td>\n",
       "      <td>0.294535</td>\n",
       "      <td>-1.357963</td>\n",
       "      <td>-2.144651</td>\n",
       "      <td>-1.561343</td>\n",
       "      <td>0.833468</td>\n",
       "      <td>0.450707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>1161</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.519365</td>\n",
       "      <td>-0.773280</td>\n",
       "      <td>0.356547</td>\n",
       "      <td>0.573791</td>\n",
       "      <td>0.080191</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>-1.062258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739193</td>\n",
       "      <td>0.636596</td>\n",
       "      <td>-0.637909</td>\n",
       "      <td>-1.394872</td>\n",
       "      <td>1.661104</td>\n",
       "      <td>0.179958</td>\n",
       "      <td>0.102549</td>\n",
       "      <td>1.221791</td>\n",
       "      <td>-0.639723</td>\n",
       "      <td>-1.032725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>3956</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.234984</td>\n",
       "      <td>1.081217</td>\n",
       "      <td>0.166529</td>\n",
       "      <td>0.843327</td>\n",
       "      <td>0.774551</td>\n",
       "      <td>-1.301393</td>\n",
       "      <td>-0.283421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>-0.227668</td>\n",
       "      <td>0.166436</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>-0.567414</td>\n",
       "      <td>0.649924</td>\n",
       "      <td>-1.334092</td>\n",
       "      <td>-1.165842</td>\n",
       "      <td>0.154028</td>\n",
       "      <td>-0.273516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>2551</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.094179</td>\n",
       "      <td>-0.935690</td>\n",
       "      <td>0.093019</td>\n",
       "      <td>0.826318</td>\n",
       "      <td>-1.237072</td>\n",
       "      <td>-1.444378</td>\n",
       "      <td>-0.044890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086313</td>\n",
       "      <td>0.848850</td>\n",
       "      <td>-1.148518</td>\n",
       "      <td>1.417228</td>\n",
       "      <td>-0.872158</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>-0.390728</td>\n",
       "      <td>0.376389</td>\n",
       "      <td>-2.364137</td>\n",
       "      <td>-0.194123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.017524</td>\n",
       "      <td>0.731140</td>\n",
       "      <td>1.331603</td>\n",
       "      <td>1.294762</td>\n",
       "      <td>-0.124687</td>\n",
       "      <td>-0.146251</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.494351</td>\n",
       "      <td>0.138425</td>\n",
       "      <td>0.154949</td>\n",
       "      <td>-1.193503</td>\n",
       "      <td>1.469061</td>\n",
       "      <td>1.425772</td>\n",
       "      <td>1.734786</td>\n",
       "      <td>-2.439490</td>\n",
       "      <td>-2.064017</td>\n",
       "      <td>-0.558087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>3450</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.863371</td>\n",
       "      <td>0.289709</td>\n",
       "      <td>-0.373316</td>\n",
       "      <td>0.304859</td>\n",
       "      <td>-0.062951</td>\n",
       "      <td>-0.989711</td>\n",
       "      <td>-0.418393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248597</td>\n",
       "      <td>0.608841</td>\n",
       "      <td>-0.125896</td>\n",
       "      <td>0.929887</td>\n",
       "      <td>-0.965482</td>\n",
       "      <td>1.495577</td>\n",
       "      <td>2.600824</td>\n",
       "      <td>0.176063</td>\n",
       "      <td>0.033666</td>\n",
       "      <td>1.032951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2813</th>\n",
       "      <td>2813</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.455184</td>\n",
       "      <td>-1.236526</td>\n",
       "      <td>0.189599</td>\n",
       "      <td>0.317127</td>\n",
       "      <td>1.948111</td>\n",
       "      <td>0.671936</td>\n",
       "      <td>-1.159910</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.458579</td>\n",
       "      <td>-0.185077</td>\n",
       "      <td>-0.676118</td>\n",
       "      <td>0.974760</td>\n",
       "      <td>-0.571469</td>\n",
       "      <td>0.256291</td>\n",
       "      <td>0.402018</td>\n",
       "      <td>1.716964</td>\n",
       "      <td>-1.638575</td>\n",
       "      <td>-0.010560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id  is_driver  is_reported    feat_0    feat_1    feat_2    feat_3  \\\n",
       "1537     1537          1         True  1.246609 -0.899651 -1.913527 -1.003489   \n",
       "1027     1027          1        False -1.292257  0.001435  0.003674  0.735129   \n",
       "3           3          1         True  0.215422  1.396370  0.209891  1.307926   \n",
       "392       392          1        False -0.010047 -0.607880 -0.402684  1.840380   \n",
       "1161     1161          1         True -1.519365 -0.773280  0.356547  0.573791   \n",
       "...       ...        ...          ...       ...       ...       ...       ...   \n",
       "3956     3956          1        False -1.234984  1.081217  0.166529  0.843327   \n",
       "2551     2551          1        False  0.094179 -0.935690  0.093019  0.826318   \n",
       "1017     1017          1        False -0.017524  0.731140  1.331603  1.294762   \n",
       "3450     3450          1        False -0.863371  0.289709 -0.373316  0.304859   \n",
       "2813     2813          1        False  0.455184 -1.236526  0.189599  0.317127   \n",
       "\n",
       "        feat_4    feat_5    feat_6  ...   feat_90   feat_91   feat_92  \\\n",
       "1537  0.139554 -0.918374  0.687188  ... -1.185621  1.912303 -0.016174   \n",
       "1027 -0.967637  0.615207  0.621669  ...  0.665107  0.918564 -0.452172   \n",
       "3     2.053852  1.586077  0.212647  ... -2.091191  0.001300 -0.344992   \n",
       "392   1.172264  1.317641  1.540145  ...  0.423845 -0.296341  0.979252   \n",
       "1161  0.080191  0.038462 -1.062258  ...  0.739193  0.636596 -0.637909   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3956  0.774551 -1.301393 -0.283421  ...  0.537931 -0.227668  0.166436   \n",
       "2551 -1.237072 -1.444378 -0.044890  ... -0.086313  0.848850 -1.148518   \n",
       "1017 -0.124687 -0.146251 -0.003631  ... -1.494351  0.138425  0.154949   \n",
       "3450 -0.062951 -0.989711 -0.418393  ...  0.248597  0.608841 -0.125896   \n",
       "2813  1.948111  0.671936 -1.159910  ... -1.458579 -0.185077 -0.676118   \n",
       "\n",
       "       feat_93   feat_94   feat_95   feat_96   feat_97   feat_98   feat_99  \n",
       "1537 -0.208349 -1.036000  0.767648 -0.074362 -0.505138  0.682445 -0.028647  \n",
       "1027 -0.927845 -0.621774 -0.532683  0.355407 -0.714893 -1.586593 -1.722829  \n",
       "3    -0.958287  0.858661 -0.486853  0.514684  2.220023  0.712551  0.970895  \n",
       "392  -1.851187  0.294535 -1.357963 -2.144651 -1.561343  0.833468  0.450707  \n",
       "1161 -1.394872  1.661104  0.179958  0.102549  1.221791 -0.639723 -1.032725  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3956  0.111433 -0.567414  0.649924 -1.334092 -1.165842  0.154028 -0.273516  \n",
       "2551  1.417228 -0.872158  0.818930 -0.390728  0.376389 -2.364137 -0.194123  \n",
       "1017 -1.193503  1.469061  1.425772  1.734786 -2.439490 -2.064017 -0.558087  \n",
       "3450  0.929887 -0.965482  1.495577  2.600824  0.176063  0.033666  1.032951  \n",
       "2813  0.974760 -0.571469  0.256291  0.402018  1.716964 -1.638575 -0.010560  \n",
       "\n",
       "[79 rows x 103 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_df.iloc[node_lookup.iloc[diff_node]['node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11397cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_format_process(node_df,edge_df):\n",
    "    \"\"\"\n",
    "    output\n",
    "    x: node features \n",
    "    y: label \n",
    "    adjacency_dict: dict which store neighbor's info. {node_index:[neighbor1,neighbor2..]}\n",
    "    \n",
    "    \"\"\"\n",
    "    # node_lookup: store node index\n",
    "    node_lookup = pd.DataFrame({'node': node_df.index,}, index=node_df.cust_id)\n",
    "    \n",
    "    # 删除孤立节点\n",
    "    diff_node = list(set(node_df['cust_id'])-(set(node_df['cust_id']) - set(edge_df['cust_id']) - set(edge_df['opp_id'])))\n",
    "    \n",
    "    node_df = node_df.iloc[node_lookup.iloc[diff_node]['node']].reset_index(drop=True)\n",
    "    \n",
    "    node_lookup = pd.DataFrame({'node': node_df.index,}, index=node_df.cust_id)\n",
    "    #建立映射字典\n",
    "    adjacency_dict = defaultdict(list)\n",
    "    for cust,opp in zip(edge_df['cust_id'],edge_df['opp_id']):\n",
    "        adjacency_dict[node_lookup.loc[cust]['node']].append(node_lookup.loc[opp]['node'])\n",
    "    \n",
    "    #\n",
    "    x = node_df[set(node_df) - {'cust_id', 'is_driver', 'is_reported'}].to_numpy()\n",
    "    y = node_df.is_reported.to_numpy() * 1\n",
    "    \n",
    "    #\n",
    "    train_mask = node_df.is_driver.to_numpy()\n",
    "    test_mask = ~train_mask\n",
    "    return x,y,adjacency_dict,train_mask,test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e02f498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.56 s, sys: 24.3 ms, total: 4.58 s\n",
      "Wall time: 4.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x,y,adjacency_dict,train_mask,test_mask = data_format_process(node_df,edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d6d252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9d5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from net import GraphSage\n",
    "#from data import CoraData\n",
    "from sampling import multihop_sampling\n",
    "\n",
    "#from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b6a91c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSage(\n",
      "  in_features=100, num_neighbors_list=[10, 10]\n",
      "  (gcn): ModuleList(\n",
      "    (0): SageGCN(\n",
      "      in_features=100, out_features=128, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=100, out_features=128, aggr_method=mean)\n",
      "    )\n",
      "    (1): SageGCN(\n",
      "      in_features=128, out_features=2, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=128, out_features=2, aggr_method=mean)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = x.shape[1]    # 输入维度\n",
    "# Note: 采样的邻居阶数需要与GCN的层数保持一致\n",
    "HIDDEN_DIM = [128, 2]   # 隐藏单元节点数\n",
    "NUM_NEIGHBORS_LIST = [10, 10]   # 每阶采样邻居的节点数\n",
    "assert len(HIDDEN_DIM) == len(NUM_NEIGHBORS_LIST)\n",
    "BTACH_SIZE = 16     # 批处理大小\n",
    "EPOCHS = 20\n",
    "NUM_BATCH_PER_EPOCH = 20    # 每个epoch循环的批次数\n",
    "LEARNING_RATE = 0.01    # 学习率\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data = namedtuple('Data', ['x', 'y', 'adjacency_dict',\n",
    "#                            'train_mask', 'val_mask', 'test_mask'])\n",
    "\n",
    "# data = CoraData().data\n",
    "# x = data.x / data.x.sum(1, keepdims=True)  # 归一化数据，使得每一行和为1\n",
    "\n",
    "# train_index = np.where(data.train_mask)[0]\n",
    "# train_label = data.y\n",
    "#test_index = np.where(data.test_mask)[0]\n",
    "#x,y,adjacency_dict = data_format_process(node_df,edge_df)\n",
    "model = GraphSage(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                  num_neighbors_list=NUM_NEIGHBORS_LIST).to(DEVICE)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "59baf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = x.shape[0]\n",
    "train_label = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1a315e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Batch 000 Loss: 2.4502\n",
      "Epoch 000 Batch 001 Loss: 4.4109\n",
      "Epoch 000 Batch 002 Loss: 9.0068\n",
      "Epoch 000 Batch 003 Loss: 9.4030\n",
      "Epoch 000 Batch 004 Loss: 10.1300\n",
      "Epoch 000 Batch 005 Loss: 10.0865\n",
      "Epoch 000 Batch 006 Loss: 3.6541\n",
      "Epoch 000 Batch 007 Loss: 4.9708\n",
      "Epoch 000 Batch 008 Loss: 12.6908\n",
      "Epoch 000 Batch 009 Loss: 6.1296\n",
      "Epoch 000 Batch 010 Loss: 6.5989\n",
      "Epoch 000 Batch 011 Loss: 15.4881\n",
      "Epoch 000 Batch 012 Loss: 11.4488\n",
      "Epoch 000 Batch 013 Loss: 7.2513\n",
      "Epoch 000 Batch 014 Loss: 12.9341\n",
      "Epoch 000 Batch 015 Loss: 10.9460\n",
      "Epoch 000 Batch 016 Loss: 12.5450\n",
      "Epoch 000 Batch 017 Loss: 4.7782\n",
      "Epoch 000 Batch 018 Loss: 9.6153\n",
      "Epoch 000 Batch 019 Loss: 4.8151\n",
      "Epoch 001 Batch 000 Loss: 9.3114\n",
      "Epoch 001 Batch 001 Loss: 11.8764\n",
      "Epoch 001 Batch 002 Loss: 12.9300\n",
      "Epoch 001 Batch 003 Loss: 4.3249\n",
      "Epoch 001 Batch 004 Loss: 19.2987\n",
      "Epoch 001 Batch 005 Loss: 7.0307\n",
      "Epoch 001 Batch 006 Loss: 7.2952\n",
      "Epoch 001 Batch 007 Loss: 6.2444\n",
      "Epoch 001 Batch 008 Loss: 4.5227\n",
      "Epoch 001 Batch 009 Loss: 23.5503\n",
      "Epoch 001 Batch 010 Loss: 9.6702\n",
      "Epoch 001 Batch 011 Loss: 16.3119\n",
      "Epoch 001 Batch 012 Loss: 2.4185\n",
      "Epoch 001 Batch 013 Loss: 14.2488\n",
      "Epoch 001 Batch 014 Loss: 9.8275\n",
      "Epoch 001 Batch 015 Loss: 5.6653\n",
      "Epoch 001 Batch 016 Loss: 16.4512\n",
      "Epoch 001 Batch 017 Loss: 5.2266\n",
      "Epoch 001 Batch 018 Loss: 11.4921\n",
      "Epoch 001 Batch 019 Loss: 14.9963\n",
      "Epoch 002 Batch 000 Loss: 4.9628\n",
      "Epoch 002 Batch 001 Loss: 9.1457\n",
      "Epoch 002 Batch 002 Loss: 11.5188\n",
      "Epoch 002 Batch 003 Loss: 4.0618\n",
      "Epoch 002 Batch 004 Loss: 11.7006\n",
      "Epoch 002 Batch 005 Loss: 9.1666\n",
      "Epoch 002 Batch 006 Loss: 8.6090\n",
      "Epoch 002 Batch 007 Loss: 5.5191\n",
      "Epoch 002 Batch 008 Loss: 6.3404\n",
      "Epoch 002 Batch 009 Loss: 11.5143\n",
      "Epoch 002 Batch 010 Loss: 6.8987\n",
      "Epoch 002 Batch 011 Loss: 6.1730\n",
      "Epoch 002 Batch 012 Loss: 5.1478\n",
      "Epoch 002 Batch 013 Loss: 12.0259\n",
      "Epoch 002 Batch 014 Loss: 12.7178\n",
      "Epoch 002 Batch 015 Loss: 6.2829\n",
      "Epoch 002 Batch 016 Loss: 3.0276\n",
      "Epoch 002 Batch 017 Loss: 11.3981\n",
      "Epoch 002 Batch 018 Loss: 9.9737\n",
      "Epoch 002 Batch 019 Loss: 1.4751\n",
      "Epoch 003 Batch 000 Loss: 7.7319\n",
      "Epoch 003 Batch 001 Loss: 5.2533\n",
      "Epoch 003 Batch 002 Loss: 4.3818\n",
      "Epoch 003 Batch 003 Loss: 8.2228\n",
      "Epoch 003 Batch 004 Loss: 1.4059\n",
      "Epoch 003 Batch 005 Loss: 7.0332\n",
      "Epoch 003 Batch 006 Loss: 6.1052\n",
      "Epoch 003 Batch 007 Loss: 6.2613\n",
      "Epoch 003 Batch 008 Loss: 9.5983\n",
      "Epoch 003 Batch 009 Loss: 3.5002\n",
      "Epoch 003 Batch 010 Loss: 11.2803\n",
      "Epoch 003 Batch 011 Loss: 5.4312\n",
      "Epoch 003 Batch 012 Loss: 13.9670\n",
      "Epoch 003 Batch 013 Loss: 6.8019\n",
      "Epoch 003 Batch 014 Loss: 6.4410\n",
      "Epoch 003 Batch 015 Loss: 2.3871\n",
      "Epoch 003 Batch 016 Loss: 5.8468\n",
      "Epoch 003 Batch 017 Loss: 6.6270\n",
      "Epoch 003 Batch 018 Loss: 8.3966\n",
      "Epoch 003 Batch 019 Loss: 7.3848\n",
      "Epoch 004 Batch 000 Loss: 6.2465\n",
      "Epoch 004 Batch 001 Loss: 7.2740\n",
      "Epoch 004 Batch 002 Loss: 10.3009\n",
      "Epoch 004 Batch 003 Loss: 13.3247\n",
      "Epoch 004 Batch 004 Loss: 1.9222\n",
      "Epoch 004 Batch 005 Loss: 5.7561\n",
      "Epoch 004 Batch 006 Loss: 7.7885\n",
      "Epoch 004 Batch 007 Loss: 9.9571\n",
      "Epoch 004 Batch 008 Loss: 9.6494\n",
      "Epoch 004 Batch 009 Loss: 3.6563\n",
      "Epoch 004 Batch 010 Loss: 9.0685\n",
      "Epoch 004 Batch 011 Loss: 5.9895\n",
      "Epoch 004 Batch 012 Loss: 9.7198\n",
      "Epoch 004 Batch 013 Loss: 9.2368\n",
      "Epoch 004 Batch 014 Loss: 9.1969\n",
      "Epoch 004 Batch 015 Loss: 9.0327\n",
      "Epoch 004 Batch 016 Loss: 6.3285\n",
      "Epoch 004 Batch 017 Loss: 8.5833\n",
      "Epoch 004 Batch 018 Loss: 14.4679\n",
      "Epoch 004 Batch 019 Loss: 6.5331\n",
      "Epoch 005 Batch 000 Loss: 1.0242\n",
      "Epoch 005 Batch 001 Loss: 10.7167\n",
      "Epoch 005 Batch 002 Loss: 5.5258\n",
      "Epoch 005 Batch 003 Loss: 4.9581\n",
      "Epoch 005 Batch 004 Loss: 8.9772\n",
      "Epoch 005 Batch 005 Loss: 3.6157\n",
      "Epoch 005 Batch 006 Loss: 10.0048\n",
      "Epoch 005 Batch 007 Loss: 3.3783\n",
      "Epoch 005 Batch 008 Loss: 6.0659\n",
      "Epoch 005 Batch 009 Loss: 7.7076\n",
      "Epoch 005 Batch 010 Loss: 6.6115\n",
      "Epoch 005 Batch 011 Loss: 9.9669\n",
      "Epoch 005 Batch 012 Loss: 4.0493\n",
      "Epoch 005 Batch 013 Loss: 3.3520\n",
      "Epoch 005 Batch 014 Loss: 5.8713\n",
      "Epoch 005 Batch 015 Loss: 11.5357\n",
      "Epoch 005 Batch 016 Loss: 6.0143\n",
      "Epoch 005 Batch 017 Loss: 5.7266\n",
      "Epoch 005 Batch 018 Loss: 8.1089\n",
      "Epoch 005 Batch 019 Loss: 4.1375\n",
      "Epoch 006 Batch 000 Loss: 4.8675\n",
      "Epoch 006 Batch 001 Loss: 8.9933\n",
      "Epoch 006 Batch 002 Loss: 7.3629\n",
      "Epoch 006 Batch 003 Loss: 12.2329\n",
      "Epoch 006 Batch 004 Loss: 7.5958\n",
      "Epoch 006 Batch 005 Loss: 2.7468\n",
      "Epoch 006 Batch 006 Loss: 8.2654\n",
      "Epoch 006 Batch 007 Loss: 6.4054\n",
      "Epoch 006 Batch 008 Loss: 5.6775\n",
      "Epoch 006 Batch 009 Loss: 6.7337\n",
      "Epoch 006 Batch 010 Loss: 8.7708\n",
      "Epoch 006 Batch 011 Loss: 0.2911\n",
      "Epoch 006 Batch 012 Loss: 4.6575\n",
      "Epoch 006 Batch 013 Loss: 1.5020\n",
      "Epoch 006 Batch 014 Loss: 8.6712\n",
      "Epoch 006 Batch 015 Loss: 5.4964\n",
      "Epoch 006 Batch 016 Loss: 4.2787\n",
      "Epoch 006 Batch 017 Loss: 6.2481\n",
      "Epoch 006 Batch 018 Loss: 4.5380\n",
      "Epoch 006 Batch 019 Loss: 4.7673\n",
      "Epoch 007 Batch 000 Loss: 3.2508\n",
      "Epoch 007 Batch 001 Loss: 3.2890\n",
      "Epoch 007 Batch 002 Loss: 4.3213\n",
      "Epoch 007 Batch 003 Loss: 1.3561\n",
      "Epoch 007 Batch 004 Loss: 2.3959\n",
      "Epoch 007 Batch 005 Loss: 4.8618\n",
      "Epoch 007 Batch 006 Loss: 5.5565\n",
      "Epoch 007 Batch 007 Loss: 7.3273\n",
      "Epoch 007 Batch 008 Loss: 3.5230\n",
      "Epoch 007 Batch 009 Loss: 2.5136\n",
      "Epoch 007 Batch 010 Loss: 2.1679\n",
      "Epoch 007 Batch 011 Loss: 4.9196\n",
      "Epoch 007 Batch 012 Loss: 7.1786\n",
      "Epoch 007 Batch 013 Loss: 12.9222\n",
      "Epoch 007 Batch 014 Loss: 6.7856\n",
      "Epoch 007 Batch 015 Loss: 3.5478\n",
      "Epoch 007 Batch 016 Loss: 8.7699\n",
      "Epoch 007 Batch 017 Loss: 3.9812\n",
      "Epoch 007 Batch 018 Loss: 2.6240\n",
      "Epoch 007 Batch 019 Loss: 4.1738\n",
      "Epoch 008 Batch 000 Loss: 6.0664\n",
      "Epoch 008 Batch 001 Loss: 7.6133\n",
      "Epoch 008 Batch 002 Loss: 3.2832\n",
      "Epoch 008 Batch 003 Loss: 5.9473\n",
      "Epoch 008 Batch 004 Loss: 5.9033\n",
      "Epoch 008 Batch 005 Loss: 2.6127\n",
      "Epoch 008 Batch 006 Loss: 5.2576\n",
      "Epoch 008 Batch 007 Loss: 3.6203\n",
      "Epoch 008 Batch 008 Loss: 4.8325\n",
      "Epoch 008 Batch 009 Loss: 3.2446\n",
      "Epoch 008 Batch 010 Loss: 3.3239\n",
      "Epoch 008 Batch 011 Loss: 3.5930\n",
      "Epoch 008 Batch 012 Loss: 5.5646\n",
      "Epoch 008 Batch 013 Loss: 1.3015\n",
      "Epoch 008 Batch 014 Loss: 5.3254\n",
      "Epoch 008 Batch 015 Loss: 7.0525\n",
      "Epoch 008 Batch 016 Loss: 3.1795\n",
      "Epoch 008 Batch 017 Loss: 3.9371\n",
      "Epoch 008 Batch 018 Loss: 3.7106\n",
      "Epoch 008 Batch 019 Loss: 3.2954\n",
      "Epoch 009 Batch 000 Loss: 4.7038\n",
      "Epoch 009 Batch 001 Loss: 3.4986\n",
      "Epoch 009 Batch 002 Loss: 4.2659\n",
      "Epoch 009 Batch 003 Loss: 5.2357\n",
      "Epoch 009 Batch 004 Loss: 4.8325\n",
      "Epoch 009 Batch 005 Loss: 9.6498\n",
      "Epoch 009 Batch 006 Loss: 2.9371\n",
      "Epoch 009 Batch 007 Loss: 8.9983\n",
      "Epoch 009 Batch 008 Loss: 5.4946\n",
      "Epoch 009 Batch 009 Loss: 2.1408\n",
      "Epoch 009 Batch 010 Loss: 2.7685\n",
      "Epoch 009 Batch 011 Loss: 5.1485\n",
      "Epoch 009 Batch 012 Loss: 5.9319\n",
      "Epoch 009 Batch 013 Loss: 3.0369\n",
      "Epoch 009 Batch 014 Loss: 9.1906\n",
      "Epoch 009 Batch 015 Loss: 3.7753\n",
      "Epoch 009 Batch 016 Loss: 4.8580\n",
      "Epoch 009 Batch 017 Loss: 4.5643\n",
      "Epoch 009 Batch 018 Loss: 5.5219\n",
      "Epoch 009 Batch 019 Loss: 4.8023\n",
      "Epoch 010 Batch 000 Loss: 4.9949\n",
      "Epoch 010 Batch 001 Loss: 4.9131\n",
      "Epoch 010 Batch 002 Loss: 3.7966\n",
      "Epoch 010 Batch 003 Loss: 2.6503\n",
      "Epoch 010 Batch 004 Loss: 3.9170\n",
      "Epoch 010 Batch 005 Loss: 7.6151\n",
      "Epoch 010 Batch 006 Loss: 2.1859\n",
      "Epoch 010 Batch 007 Loss: 2.2681\n",
      "Epoch 010 Batch 008 Loss: 0.8346\n",
      "Epoch 010 Batch 009 Loss: 4.6813\n",
      "Epoch 010 Batch 010 Loss: 5.3404\n",
      "Epoch 010 Batch 011 Loss: 3.1951\n",
      "Epoch 010 Batch 012 Loss: 1.6256\n",
      "Epoch 010 Batch 013 Loss: 5.0655\n",
      "Epoch 010 Batch 014 Loss: 2.4070\n",
      "Epoch 010 Batch 015 Loss: 2.4886\n",
      "Epoch 010 Batch 016 Loss: 2.4089\n",
      "Epoch 010 Batch 017 Loss: 3.2987\n",
      "Epoch 010 Batch 018 Loss: 7.1799\n",
      "Epoch 010 Batch 019 Loss: 4.6083\n",
      "Epoch 011 Batch 000 Loss: 1.9575\n",
      "Epoch 011 Batch 001 Loss: 2.0849\n",
      "Epoch 011 Batch 002 Loss: 4.6559\n",
      "Epoch 011 Batch 003 Loss: 1.7653\n",
      "Epoch 011 Batch 004 Loss: 3.4736\n",
      "Epoch 011 Batch 005 Loss: 3.9713\n",
      "Epoch 011 Batch 006 Loss: 3.7144\n",
      "Epoch 011 Batch 007 Loss: 2.2698\n",
      "Epoch 011 Batch 008 Loss: 4.2744\n",
      "Epoch 011 Batch 009 Loss: 4.7808\n",
      "Epoch 011 Batch 010 Loss: 3.3959\n",
      "Epoch 011 Batch 011 Loss: 5.6418\n",
      "Epoch 011 Batch 012 Loss: 4.4270\n",
      "Epoch 011 Batch 013 Loss: 7.7722\n",
      "Epoch 011 Batch 014 Loss: 7.8439\n",
      "Epoch 011 Batch 015 Loss: 1.4693\n",
      "Epoch 011 Batch 016 Loss: 1.4765\n",
      "Epoch 011 Batch 017 Loss: 2.7731\n",
      "Epoch 011 Batch 018 Loss: 3.7774\n",
      "Epoch 011 Batch 019 Loss: 5.0884\n",
      "Epoch 012 Batch 000 Loss: 8.0416\n",
      "Epoch 012 Batch 001 Loss: 2.6772\n",
      "Epoch 012 Batch 002 Loss: 0.3566\n",
      "Epoch 012 Batch 003 Loss: 2.7777\n",
      "Epoch 012 Batch 004 Loss: 2.9289\n",
      "Epoch 012 Batch 005 Loss: 2.3767\n",
      "Epoch 012 Batch 006 Loss: 5.4977\n",
      "Epoch 012 Batch 007 Loss: 1.3173\n",
      "Epoch 012 Batch 008 Loss: 2.3711\n",
      "Epoch 012 Batch 009 Loss: 3.8738\n",
      "Epoch 012 Batch 010 Loss: 2.6138\n",
      "Epoch 012 Batch 011 Loss: 3.4148\n",
      "Epoch 012 Batch 012 Loss: 6.2849\n",
      "Epoch 012 Batch 013 Loss: 3.9738\n",
      "Epoch 012 Batch 014 Loss: 1.5748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 Batch 015 Loss: 3.4868\n",
      "Epoch 012 Batch 016 Loss: 0.6127\n",
      "Epoch 012 Batch 017 Loss: 4.1624\n",
      "Epoch 012 Batch 018 Loss: 3.3148\n",
      "Epoch 012 Batch 019 Loss: 3.9709\n",
      "Epoch 013 Batch 000 Loss: 4.4751\n",
      "Epoch 013 Batch 001 Loss: 1.4530\n",
      "Epoch 013 Batch 002 Loss: 3.1569\n",
      "Epoch 013 Batch 003 Loss: 3.4476\n",
      "Epoch 013 Batch 004 Loss: 1.9823\n",
      "Epoch 013 Batch 005 Loss: 1.9252\n",
      "Epoch 013 Batch 006 Loss: 5.0246\n",
      "Epoch 013 Batch 007 Loss: 1.3676\n",
      "Epoch 013 Batch 008 Loss: 4.0466\n",
      "Epoch 013 Batch 009 Loss: 3.6577\n",
      "Epoch 013 Batch 010 Loss: 2.6935\n",
      "Epoch 013 Batch 011 Loss: 4.9209\n",
      "Epoch 013 Batch 012 Loss: 4.6118\n",
      "Epoch 013 Batch 013 Loss: 3.3032\n",
      "Epoch 013 Batch 014 Loss: 3.7396\n",
      "Epoch 013 Batch 015 Loss: 4.2545\n",
      "Epoch 013 Batch 016 Loss: 6.0039\n",
      "Epoch 013 Batch 017 Loss: 2.8112\n",
      "Epoch 013 Batch 018 Loss: 3.1692\n",
      "Epoch 013 Batch 019 Loss: 3.1206\n",
      "Epoch 014 Batch 000 Loss: 3.2004\n",
      "Epoch 014 Batch 001 Loss: 3.6570\n",
      "Epoch 014 Batch 002 Loss: 0.9548\n",
      "Epoch 014 Batch 003 Loss: 1.9532\n",
      "Epoch 014 Batch 004 Loss: 2.1799\n",
      "Epoch 014 Batch 005 Loss: 5.3358\n",
      "Epoch 014 Batch 006 Loss: 3.6359\n",
      "Epoch 014 Batch 007 Loss: 3.8564\n",
      "Epoch 014 Batch 008 Loss: 2.8757\n",
      "Epoch 014 Batch 009 Loss: 4.2001\n",
      "Epoch 014 Batch 010 Loss: 3.3942\n",
      "Epoch 014 Batch 011 Loss: 4.0059\n",
      "Epoch 014 Batch 012 Loss: 1.7400\n",
      "Epoch 014 Batch 013 Loss: 1.7527\n",
      "Epoch 014 Batch 014 Loss: 3.9341\n",
      "Epoch 014 Batch 015 Loss: 2.5598\n",
      "Epoch 014 Batch 016 Loss: 3.6591\n",
      "Epoch 014 Batch 017 Loss: 2.3820\n",
      "Epoch 014 Batch 018 Loss: 2.3939\n",
      "Epoch 014 Batch 019 Loss: 2.5219\n",
      "Epoch 015 Batch 000 Loss: 1.3212\n",
      "Epoch 015 Batch 001 Loss: 5.6076\n",
      "Epoch 015 Batch 002 Loss: 3.4414\n",
      "Epoch 015 Batch 003 Loss: 3.8145\n",
      "Epoch 015 Batch 004 Loss: 2.9627\n",
      "Epoch 015 Batch 005 Loss: 2.8103\n",
      "Epoch 015 Batch 006 Loss: 2.6866\n",
      "Epoch 015 Batch 007 Loss: 1.7370\n",
      "Epoch 015 Batch 008 Loss: 2.7054\n",
      "Epoch 015 Batch 009 Loss: 3.8223\n",
      "Epoch 015 Batch 010 Loss: 1.5313\n",
      "Epoch 015 Batch 011 Loss: 2.4710\n",
      "Epoch 015 Batch 012 Loss: 3.2043\n",
      "Epoch 015 Batch 013 Loss: 3.5466\n",
      "Epoch 015 Batch 014 Loss: 0.4570\n",
      "Epoch 015 Batch 015 Loss: 2.3783\n",
      "Epoch 015 Batch 016 Loss: 5.0767\n",
      "Epoch 015 Batch 017 Loss: 2.2308\n",
      "Epoch 015 Batch 018 Loss: 3.3553\n",
      "Epoch 015 Batch 019 Loss: 1.4785\n",
      "Epoch 016 Batch 000 Loss: 2.9465\n",
      "Epoch 016 Batch 001 Loss: 1.3877\n",
      "Epoch 016 Batch 002 Loss: 2.0661\n",
      "Epoch 016 Batch 003 Loss: 1.4966\n",
      "Epoch 016 Batch 004 Loss: 4.0861\n",
      "Epoch 016 Batch 005 Loss: 2.8930\n",
      "Epoch 016 Batch 006 Loss: 2.0087\n",
      "Epoch 016 Batch 007 Loss: 2.6737\n",
      "Epoch 016 Batch 008 Loss: 1.2610\n",
      "Epoch 016 Batch 009 Loss: 3.8378\n",
      "Epoch 016 Batch 010 Loss: 1.8757\n",
      "Epoch 016 Batch 011 Loss: 0.9102\n",
      "Epoch 016 Batch 012 Loss: 3.0823\n",
      "Epoch 016 Batch 013 Loss: 1.0874\n",
      "Epoch 016 Batch 014 Loss: 2.8692\n",
      "Epoch 016 Batch 015 Loss: 2.9118\n",
      "Epoch 016 Batch 016 Loss: 1.5639\n",
      "Epoch 016 Batch 017 Loss: 2.2697\n",
      "Epoch 016 Batch 018 Loss: 1.8351\n",
      "Epoch 016 Batch 019 Loss: 1.6681\n",
      "Epoch 017 Batch 000 Loss: 3.1659\n",
      "Epoch 017 Batch 001 Loss: 1.8774\n",
      "Epoch 017 Batch 002 Loss: 1.7874\n",
      "Epoch 017 Batch 003 Loss: 1.5248\n",
      "Epoch 017 Batch 004 Loss: 2.2599\n",
      "Epoch 017 Batch 005 Loss: 1.1405\n",
      "Epoch 017 Batch 006 Loss: 0.4334\n",
      "Epoch 017 Batch 007 Loss: 2.1067\n",
      "Epoch 017 Batch 008 Loss: 0.8800\n",
      "Epoch 017 Batch 009 Loss: 3.7350\n",
      "Epoch 017 Batch 010 Loss: 2.8154\n",
      "Epoch 017 Batch 011 Loss: 1.5531\n",
      "Epoch 017 Batch 012 Loss: 1.6619\n",
      "Epoch 017 Batch 013 Loss: 3.5993\n",
      "Epoch 017 Batch 014 Loss: 1.6624\n",
      "Epoch 017 Batch 015 Loss: 1.5313\n",
      "Epoch 017 Batch 016 Loss: 2.3287\n",
      "Epoch 017 Batch 017 Loss: 2.9591\n",
      "Epoch 017 Batch 018 Loss: 1.7026\n",
      "Epoch 017 Batch 019 Loss: 3.1942\n",
      "Epoch 018 Batch 000 Loss: 1.8502\n",
      "Epoch 018 Batch 001 Loss: 2.0744\n",
      "Epoch 018 Batch 002 Loss: 2.7758\n",
      "Epoch 018 Batch 003 Loss: 1.5177\n",
      "Epoch 018 Batch 004 Loss: 2.3362\n",
      "Epoch 018 Batch 005 Loss: 1.0472\n",
      "Epoch 018 Batch 006 Loss: 0.4513\n",
      "Epoch 018 Batch 007 Loss: 2.5551\n",
      "Epoch 018 Batch 008 Loss: 1.2843\n",
      "Epoch 018 Batch 009 Loss: 1.6202\n",
      "Epoch 018 Batch 010 Loss: 1.7473\n",
      "Epoch 018 Batch 011 Loss: 2.9198\n",
      "Epoch 018 Batch 012 Loss: 1.3392\n",
      "Epoch 018 Batch 013 Loss: 1.6057\n",
      "Epoch 018 Batch 014 Loss: 0.8953\n",
      "Epoch 018 Batch 015 Loss: 0.3780\n",
      "Epoch 018 Batch 016 Loss: 3.0116\n",
      "Epoch 018 Batch 017 Loss: 1.4384\n",
      "Epoch 018 Batch 018 Loss: 2.4921\n",
      "Epoch 018 Batch 019 Loss: 1.6365\n",
      "Epoch 019 Batch 000 Loss: 2.3558\n",
      "Epoch 019 Batch 001 Loss: 0.7411\n",
      "Epoch 019 Batch 002 Loss: 2.1089\n",
      "Epoch 019 Batch 003 Loss: 0.9764\n",
      "Epoch 019 Batch 004 Loss: 2.8240\n",
      "Epoch 019 Batch 005 Loss: 2.7599\n",
      "Epoch 019 Batch 006 Loss: 2.7030\n",
      "Epoch 019 Batch 007 Loss: 2.2566\n",
      "Epoch 019 Batch 008 Loss: 2.7408\n",
      "Epoch 019 Batch 009 Loss: 2.1159\n",
      "Epoch 019 Batch 010 Loss: 0.4082\n",
      "Epoch 019 Batch 011 Loss: 0.5945\n",
      "Epoch 019 Batch 012 Loss: 1.3774\n",
      "Epoch 019 Batch 013 Loss: 1.9687\n",
      "Epoch 019 Batch 014 Loss: 1.0834\n",
      "Epoch 019 Batch 015 Loss: 2.4200\n",
      "Epoch 019 Batch 016 Loss: 1.6624\n",
      "Epoch 019 Batch 017 Loss: 2.0535\n",
      "Epoch 019 Batch 018 Loss: 1.5930\n",
      "Epoch 019 Batch 019 Loss: 2.4712\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(EPOCHS):\n",
    "    for batch in range(NUM_BATCH_PER_EPOCH):\n",
    "        batch_src_index = np.random.choice(train_index, size=(BTACH_SIZE,))\n",
    "        batch_src_label = torch.from_numpy(train_label[batch_src_index]).long().to(DEVICE)\n",
    "        batch_sampling_result = multihop_sampling(batch_src_index, NUM_NEIGHBORS_LIST, adjacency_dict)\n",
    "        batch_sampling_x = [torch.from_numpy(x[idx]).float().to(DEVICE) for idx in batch_sampling_result]\n",
    "        batch_train_logits = model(batch_sampling_x)\n",
    "        loss = criterion(batch_train_logits, batch_src_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # 反向传播计算参数的梯度\n",
    "        optimizer.step()  # 使用优化方法进行梯度更新\n",
    "        print(\"Epoch {:03d} Batch {:03d} Loss: {:.4f}\".format(e, batch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e66792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "node_n = 1000\n",
    "edge_n = 1000\n",
    "report_rate = 0.5\n",
    "driver_rate = 0.8\n",
    "nums_features = 10\n",
    "random_weight = True\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07774021",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "nodes = np.array(range(node_n))\n",
    "\n",
    "full_edges = np.array(range(node_n * node_n))\n",
    "edges = np.random.choice(\n",
    "    full_edges[full_edges // node_n > full_edges % node_n],\n",
    "    size=edge_n)\n",
    "seeds = np.random.choice(nodes, size = int(node_n*report_rate))\n",
    "seeds_driver = np.random.choice(nodes, size = int(node_n*driver_rate))\n",
    "edges = np.concatenate((\n",
    "        edges,\n",
    "        edges // node_n + edges % node_n * node_n, ))\n",
    "\n",
    "\n",
    "node_dataframe = pd.DataFrame({\n",
    "    'cust_id': nodes,\n",
    "    'is_driver': np.in1d(nodes, seeds_driver),\n",
    "    'is_reported': np.in1d(nodes, seeds),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac9fc003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>is_driver</th>\n",
       "      <th>is_reported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cust_id  is_driver  is_reported\n",
       "0          0       True         True\n",
       "1          1      False        False\n",
       "2          2       True        False\n",
       "3          3      False         True\n",
       "4          4       True        False\n",
       "..       ...        ...          ...\n",
       "995      995      False        False\n",
       "996      996       True        False\n",
       "997      997       True        False\n",
       "998      998      False         True\n",
       "999      999       True         True\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51942614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327827fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.Tensor(1000, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee27be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982c5db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0623,  0.1850, -0.0040,  ...,  0.0785, -0.0989, -0.1835],\n",
       "        [-0.0939, -0.0900,  0.0197,  ..., -0.2021, -0.1501,  0.0027],\n",
       "        [-0.0758, -0.0608,  0.0843,  ..., -0.0645, -0.2091,  0.1108],\n",
       "        ...,\n",
       "        [ 0.1986, -0.0360,  0.2097,  ..., -0.0338, -0.0786, -0.1887],\n",
       "        [-0.2112,  0.0564, -0.1614,  ...,  0.1193, -0.1239,  0.0435],\n",
       "        [ 0.2099,  0.1021, -0.1717,  ...,  0.1066,  0.1025, -0.0792]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.kaiming_uniform_(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e60a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0623,  0.1850, -0.0040,  ...,  0.0785, -0.0989, -0.1835],\n",
       "        [-0.0939, -0.0900,  0.0197,  ..., -0.2021, -0.1501,  0.0027],\n",
       "        [-0.0758, -0.0608,  0.0843,  ..., -0.0645, -0.2091,  0.1108],\n",
       "        ...,\n",
       "        [ 0.1986, -0.0360,  0.2097,  ..., -0.0338, -0.0786, -0.1887],\n",
       "        [-0.2112,  0.0564, -0.1614,  ...,  0.1193, -0.1239,  0.0435],\n",
       "        [ 0.2099,  0.1021, -0.1717,  ...,  0.1066,  0.1025, -0.0792]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ebbe50",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matmul(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-400fd7271ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc_node_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_node_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: matmul(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "src_node_features = 128\n",
    "torch.matmul(src_node_features, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f016cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor(np.random.rand(16,10,120))\n",
    "b = torch.Tensor(np.random.rand(120,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad9d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8823ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "532bd277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.sigmoid(input)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd2ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1,1,1,1])\n",
    "b = torch.Tensor([1,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc250a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(b,a).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0441e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import CoraData\n",
    "import sage_v2_cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0650c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cached file: /Users/shuaihengxiao/Desktop/graphSAGE_v0/data/cora/ch7_cached.pkl\n"
     ]
    }
   ],
   "source": [
    "data = CoraData(data_root=\"/Users/shuaihengxiao/Desktop/graphSAGE_v0/data/cora\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2143aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8844b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing..\n",
      "data preprocessing complete!\n",
      "-----------------------*-----------------------\n",
      "after filtering single nodes\n",
      "num of train instances: 1000\n",
      "num of test instances: 140\n",
      "-----------------------*-----------------------\n",
      "model structure\n",
      "GraphSage(\n",
      "  in_features=1433, num_neighbors_list=[10, 10]\n",
      "  (gcn): ModuleList(\n",
      "    (0): SageGCN(\n",
      "      in_features=1433, out_features=128, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=1433, out_features=128, aggr_method=mean)\n",
      "      (dropout1): Dropout(p=0.4, inplace=False)\n",
      "      (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (linear_2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): SageGCN(\n",
      "      in_features=128, out_features=7, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=128, out_features=7, aggr_method=mean)\n",
      "      (dropout1): Dropout(p=0.4, inplace=False)\n",
      "      (linear_1): Linear(in_features=7, out_features=14, bias=True)\n",
      "      (linear_2): Linear(in_features=14, out_features=7, bias=True)\n",
      "      (bn_1): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a = sage_v2_cora.run_model(data,aggr_neighbor_method='mean',\n",
    "    aggr_hidden_method='sum',batch_size=64,\n",
    "    epochs=35,\n",
    "    num_batch_per_epoch=50,residual_block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67db2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training..\n",
      "training through cpu\n",
      "Epoch 000 Batch 000 Loss: 8.8516\n",
      "Epoch 000 Batch 001 Loss: 3.7583\n",
      "Epoch 000 Batch 002 Loss: 2.1240\n",
      "Epoch 000 Batch 003 Loss: 1.2558\n",
      "Epoch 000 Batch 004 Loss: 1.5569\n",
      "Epoch 000 Batch 005 Loss: 1.8766\n",
      "Epoch 000 Batch 006 Loss: 2.0435\n",
      "Epoch 000 Batch 007 Loss: 1.6305\n",
      "Epoch 000 Batch 008 Loss: 0.3971\n",
      "Epoch 000 Batch 009 Loss: 1.5963\n",
      "Epoch 000 Batch 010 Loss: 1.9960\n",
      "Epoch 000 Batch 011 Loss: 0.3299\n",
      "Epoch 000 Batch 012 Loss: 1.2085\n",
      "Epoch 000 Batch 013 Loss: 1.3517\n",
      "Epoch 000 Batch 014 Loss: 1.0449\n",
      "Epoch 000 Batch 015 Loss: 0.9926\n",
      "Epoch 000 Batch 016 Loss: 1.5464\n",
      "Epoch 000 Batch 017 Loss: 0.6308\n",
      "Epoch 000 Batch 018 Loss: 0.5980\n",
      "Epoch 000 Batch 019 Loss: 0.5760\n",
      "Epoch 000 Batch 020 Loss: 1.1302\n",
      "Epoch 000 Batch 021 Loss: 1.3686\n",
      "Epoch 000 Batch 022 Loss: 1.1147\n",
      "Epoch 000 Batch 023 Loss: 0.7267\n",
      "Epoch 000 Batch 024 Loss: 1.7328\n",
      "Epoch 000 Batch 025 Loss: 0.5491\n",
      "Epoch 000 Batch 026 Loss: 0.4848\n",
      "Epoch 000 Batch 027 Loss: 1.0815\n",
      "Epoch 000 Batch 028 Loss: 0.4462\n",
      "Epoch 000 Batch 029 Loss: 0.1780\n",
      "Epoch 000 Batch 030 Loss: 0.7334\n",
      "Epoch 000 Batch 031 Loss: 0.9883\n",
      "Epoch 000 Batch 032 Loss: 0.9156\n",
      "Epoch 000 Batch 033 Loss: 0.8780\n",
      "Epoch 000 Batch 034 Loss: 0.8290\n",
      "Epoch 000 Batch 035 Loss: 0.5685\n",
      "Epoch 000 Batch 036 Loss: 0.0952\n",
      "Epoch 000 Batch 037 Loss: 0.5203\n",
      "Epoch 000 Batch 038 Loss: 0.4504\n",
      "Epoch 000 Batch 039 Loss: 0.4201\n",
      "Epoch 000 Batch 040 Loss: 0.6842\n",
      "Epoch 000 Batch 041 Loss: 0.3527\n",
      "Epoch 000 Batch 042 Loss: 0.7183\n",
      "Epoch 000 Batch 043 Loss: 0.5492\n",
      "Epoch 000 Batch 044 Loss: 0.4484\n",
      "Epoch 000 Batch 045 Loss: 0.4589\n",
      "Epoch 000 Batch 046 Loss: 0.1843\n",
      "Epoch 000 Batch 047 Loss: 0.8754\n",
      "Epoch 000 Batch 048 Loss: 0.3750\n",
      "Epoch 000 Batch 049 Loss: 0.3207\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8500000238418579\n",
      "-----------------------*-----------------------\n",
      "Epoch 001 Batch 000 Loss: 0.5046\n",
      "Epoch 001 Batch 001 Loss: 0.5170\n",
      "Epoch 001 Batch 002 Loss: 1.0596\n",
      "Epoch 001 Batch 003 Loss: 0.2707\n",
      "Epoch 001 Batch 004 Loss: 0.5004\n",
      "Epoch 001 Batch 005 Loss: 0.2499\n",
      "Epoch 001 Batch 006 Loss: 0.4410\n",
      "Epoch 001 Batch 007 Loss: 0.3566\n",
      "Epoch 001 Batch 008 Loss: 0.2701\n",
      "Epoch 001 Batch 009 Loss: 0.3471\n",
      "Epoch 001 Batch 010 Loss: 0.3224\n",
      "Epoch 001 Batch 011 Loss: 0.2263\n",
      "Epoch 001 Batch 012 Loss: 0.2291\n",
      "Epoch 001 Batch 013 Loss: 0.2575\n",
      "Epoch 001 Batch 014 Loss: 0.3002\n",
      "Epoch 001 Batch 015 Loss: 0.2145\n",
      "Epoch 001 Batch 016 Loss: 0.5914\n",
      "Epoch 001 Batch 017 Loss: 0.4177\n",
      "Epoch 001 Batch 018 Loss: 0.5692\n",
      "Epoch 001 Batch 019 Loss: 0.3937\n",
      "Epoch 001 Batch 020 Loss: 0.2370\n",
      "Epoch 001 Batch 021 Loss: 0.3139\n",
      "Epoch 001 Batch 022 Loss: 0.1452\n",
      "Epoch 001 Batch 023 Loss: 0.1364\n",
      "Epoch 001 Batch 024 Loss: 0.3867\n",
      "Epoch 001 Batch 025 Loss: 0.3356\n",
      "Epoch 001 Batch 026 Loss: 0.1620\n",
      "Epoch 001 Batch 027 Loss: 0.0765\n",
      "Epoch 001 Batch 028 Loss: 0.1449\n",
      "Epoch 001 Batch 029 Loss: 0.1949\n",
      "Epoch 001 Batch 030 Loss: 0.2226\n",
      "Epoch 001 Batch 031 Loss: 0.3314\n",
      "Epoch 001 Batch 032 Loss: 0.1967\n",
      "Epoch 001 Batch 033 Loss: 0.1524\n",
      "Epoch 001 Batch 034 Loss: 0.2249\n",
      "Epoch 001 Batch 035 Loss: 0.1773\n",
      "Epoch 001 Batch 036 Loss: 0.1409\n",
      "Epoch 001 Batch 037 Loss: 0.1393\n",
      "Epoch 001 Batch 038 Loss: 0.2459\n",
      "Epoch 001 Batch 039 Loss: 0.0777\n",
      "Epoch 001 Batch 040 Loss: 0.0912\n",
      "Epoch 001 Batch 041 Loss: 0.1790\n",
      "Epoch 001 Batch 042 Loss: 0.0698\n",
      "Epoch 001 Batch 043 Loss: 0.1963\n",
      "Epoch 001 Batch 044 Loss: 0.3392\n",
      "Epoch 001 Batch 045 Loss: 0.6977\n",
      "Epoch 001 Batch 046 Loss: 0.3155\n",
      "Epoch 001 Batch 047 Loss: 0.0565\n",
      "Epoch 001 Batch 048 Loss: 0.0555\n",
      "Epoch 001 Batch 049 Loss: 0.0873\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8785714507102966\n",
      "-----------------------*-----------------------\n",
      "Epoch 002 Batch 000 Loss: 0.0588\n",
      "Epoch 002 Batch 001 Loss: 0.0435\n",
      "Epoch 002 Batch 002 Loss: 1.1511\n",
      "Epoch 002 Batch 003 Loss: 0.1735\n",
      "Epoch 002 Batch 004 Loss: 0.1424\n",
      "Epoch 002 Batch 005 Loss: 0.1840\n",
      "Epoch 002 Batch 006 Loss: 0.0568\n",
      "Epoch 002 Batch 007 Loss: 0.0747\n",
      "Epoch 002 Batch 008 Loss: 0.0849\n",
      "Epoch 002 Batch 009 Loss: 0.2326\n",
      "Epoch 002 Batch 010 Loss: 0.2667\n",
      "Epoch 002 Batch 011 Loss: 0.2130\n",
      "Epoch 002 Batch 012 Loss: 0.1440\n",
      "Epoch 002 Batch 013 Loss: 0.1036\n",
      "Epoch 002 Batch 014 Loss: 0.0570\n",
      "Epoch 002 Batch 015 Loss: 0.1954\n",
      "Epoch 002 Batch 016 Loss: 0.0529\n",
      "Epoch 002 Batch 017 Loss: 0.4167\n",
      "Epoch 002 Batch 018 Loss: 0.2318\n",
      "Epoch 002 Batch 019 Loss: 0.0571\n",
      "Epoch 002 Batch 020 Loss: 0.0143\n",
      "Epoch 002 Batch 021 Loss: 0.1076\n",
      "Epoch 002 Batch 022 Loss: 0.0103\n",
      "Epoch 002 Batch 023 Loss: 0.0877\n",
      "Epoch 002 Batch 024 Loss: 0.0177\n",
      "Epoch 002 Batch 025 Loss: 0.0937\n",
      "Epoch 002 Batch 026 Loss: 0.0155\n",
      "Epoch 002 Batch 027 Loss: 0.1402\n",
      "Epoch 002 Batch 028 Loss: 0.0847\n",
      "Epoch 002 Batch 029 Loss: 0.2183\n",
      "Epoch 002 Batch 030 Loss: 0.0574\n",
      "Epoch 002 Batch 031 Loss: 0.0323\n",
      "Epoch 002 Batch 032 Loss: 0.1348\n",
      "Epoch 002 Batch 033 Loss: 0.1089\n",
      "Epoch 002 Batch 034 Loss: 0.4943\n",
      "Epoch 002 Batch 035 Loss: 0.4330\n",
      "Epoch 002 Batch 036 Loss: 0.0762\n",
      "Epoch 002 Batch 037 Loss: 0.1931\n",
      "Epoch 002 Batch 038 Loss: 0.0242\n",
      "Epoch 002 Batch 039 Loss: 0.1231\n",
      "Epoch 002 Batch 040 Loss: 1.2039\n",
      "Epoch 002 Batch 041 Loss: 0.2096\n",
      "Epoch 002 Batch 042 Loss: 0.0969\n",
      "Epoch 002 Batch 043 Loss: 0.0942\n",
      "Epoch 002 Batch 044 Loss: 0.0535\n",
      "Epoch 002 Batch 045 Loss: 0.1005\n",
      "Epoch 002 Batch 046 Loss: 0.0454\n",
      "Epoch 002 Batch 047 Loss: 0.0937\n",
      "Epoch 002 Batch 048 Loss: 0.0329\n",
      "Epoch 002 Batch 049 Loss: 0.0579\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 003 Batch 000 Loss: 0.1236\n",
      "Epoch 003 Batch 001 Loss: 0.0231\n",
      "Epoch 003 Batch 002 Loss: 0.0197\n",
      "Epoch 003 Batch 003 Loss: 0.0239\n",
      "Epoch 003 Batch 004 Loss: 0.0300\n",
      "Epoch 003 Batch 005 Loss: 0.1975\n",
      "Epoch 003 Batch 006 Loss: 0.0750\n",
      "Epoch 003 Batch 007 Loss: 0.0145\n",
      "Epoch 003 Batch 008 Loss: 0.1455\n",
      "Epoch 003 Batch 009 Loss: 0.0902\n",
      "Epoch 003 Batch 010 Loss: 0.1204\n",
      "Epoch 003 Batch 011 Loss: 0.0486\n",
      "Epoch 003 Batch 012 Loss: 0.0246\n",
      "Epoch 003 Batch 013 Loss: 0.0248\n",
      "Epoch 003 Batch 014 Loss: 0.1262\n",
      "Epoch 003 Batch 015 Loss: 0.1642\n",
      "Epoch 003 Batch 016 Loss: 0.0973\n",
      "Epoch 003 Batch 017 Loss: 0.2535\n",
      "Epoch 003 Batch 018 Loss: 0.1057\n",
      "Epoch 003 Batch 019 Loss: 0.1161\n",
      "Epoch 003 Batch 020 Loss: 0.0469\n",
      "Epoch 003 Batch 021 Loss: 0.0274\n",
      "Epoch 003 Batch 022 Loss: 0.0373\n",
      "Epoch 003 Batch 023 Loss: 0.0522\n",
      "Epoch 003 Batch 024 Loss: 0.0290\n",
      "Epoch 003 Batch 025 Loss: 0.0236\n",
      "Epoch 003 Batch 026 Loss: 0.0268\n",
      "Epoch 003 Batch 027 Loss: 0.0178\n",
      "Epoch 003 Batch 028 Loss: 0.0159\n",
      "Epoch 003 Batch 029 Loss: 0.0129\n",
      "Epoch 003 Batch 030 Loss: 0.0127\n",
      "Epoch 003 Batch 031 Loss: 0.0289\n",
      "Epoch 003 Batch 032 Loss: 0.0945\n",
      "Epoch 003 Batch 033 Loss: 0.0207\n",
      "Epoch 003 Batch 034 Loss: 0.0057\n",
      "Epoch 003 Batch 035 Loss: 0.0247\n",
      "Epoch 003 Batch 036 Loss: 0.0260\n",
      "Epoch 003 Batch 037 Loss: 0.1067\n",
      "Epoch 003 Batch 038 Loss: 0.0233\n",
      "Epoch 003 Batch 039 Loss: 0.0264\n",
      "Epoch 003 Batch 040 Loss: 0.1120\n",
      "Epoch 003 Batch 041 Loss: 0.0992\n",
      "Epoch 003 Batch 042 Loss: 0.0891\n",
      "Epoch 003 Batch 043 Loss: 0.0757\n",
      "Epoch 003 Batch 044 Loss: 0.0243\n",
      "Epoch 003 Batch 045 Loss: 0.1176\n",
      "Epoch 003 Batch 046 Loss: 0.0796\n",
      "Epoch 003 Batch 047 Loss: 0.0360\n",
      "Epoch 003 Batch 048 Loss: 0.0969\n",
      "Epoch 003 Batch 049 Loss: 0.0125\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8571428656578064\n",
      "-----------------------*-----------------------\n",
      "Epoch 004 Batch 000 Loss: 0.0126\n",
      "Epoch 004 Batch 001 Loss: 0.0222\n",
      "Epoch 004 Batch 002 Loss: 0.0147\n",
      "Epoch 004 Batch 003 Loss: 0.0109\n",
      "Epoch 004 Batch 004 Loss: 0.0212\n",
      "Epoch 004 Batch 005 Loss: 0.0494\n",
      "Epoch 004 Batch 006 Loss: 0.0685\n",
      "Epoch 004 Batch 007 Loss: 0.1013\n",
      "Epoch 004 Batch 008 Loss: 0.0112\n",
      "Epoch 004 Batch 009 Loss: 0.0584\n",
      "Epoch 004 Batch 010 Loss: 0.0049\n",
      "Epoch 004 Batch 011 Loss: 0.0323\n",
      "Epoch 004 Batch 012 Loss: 0.1099\n",
      "Epoch 004 Batch 013 Loss: 0.1857\n",
      "Epoch 004 Batch 014 Loss: 0.0361\n",
      "Epoch 004 Batch 015 Loss: 0.0181\n",
      "Epoch 004 Batch 016 Loss: 0.0969\n",
      "Epoch 004 Batch 017 Loss: 0.0059\n",
      "Epoch 004 Batch 018 Loss: 0.0135\n",
      "Epoch 004 Batch 019 Loss: 0.0234\n",
      "Epoch 004 Batch 020 Loss: 0.0384\n",
      "Epoch 004 Batch 021 Loss: 0.0300\n",
      "Epoch 004 Batch 022 Loss: 0.0323\n",
      "Epoch 004 Batch 023 Loss: 0.0438\n",
      "Epoch 004 Batch 024 Loss: 0.0187\n",
      "Epoch 004 Batch 025 Loss: 0.0484\n",
      "Epoch 004 Batch 026 Loss: 0.0294\n",
      "Epoch 004 Batch 027 Loss: 0.0138\n",
      "Epoch 004 Batch 028 Loss: 0.0124\n",
      "Epoch 004 Batch 029 Loss: 0.0386\n",
      "Epoch 004 Batch 030 Loss: 0.0272\n",
      "Epoch 004 Batch 031 Loss: 0.0076\n",
      "Epoch 004 Batch 032 Loss: 0.0259\n",
      "Epoch 004 Batch 033 Loss: 0.0049\n",
      "Epoch 004 Batch 034 Loss: 0.0551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 Batch 035 Loss: 0.0163\n",
      "Epoch 004 Batch 036 Loss: 0.0083\n",
      "Epoch 004 Batch 037 Loss: 0.0405\n",
      "Epoch 004 Batch 038 Loss: 0.0595\n",
      "Epoch 004 Batch 039 Loss: 0.0402\n",
      "Epoch 004 Batch 040 Loss: 0.0334\n",
      "Epoch 004 Batch 041 Loss: 0.0596\n",
      "Epoch 004 Batch 042 Loss: 0.0376\n",
      "Epoch 004 Batch 043 Loss: 0.0171\n",
      "Epoch 004 Batch 044 Loss: 0.0166\n",
      "Epoch 004 Batch 045 Loss: 0.1258\n",
      "Epoch 004 Batch 046 Loss: 0.0077\n",
      "Epoch 004 Batch 047 Loss: 0.0210\n",
      "Epoch 004 Batch 048 Loss: 0.0327\n",
      "Epoch 004 Batch 049 Loss: 0.0104\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 005 Batch 000 Loss: 0.0024\n",
      "Epoch 005 Batch 001 Loss: 0.0175\n",
      "Epoch 005 Batch 002 Loss: 0.0346\n",
      "Epoch 005 Batch 003 Loss: 0.0390\n",
      "Epoch 005 Batch 004 Loss: 0.0321\n",
      "Epoch 005 Batch 005 Loss: 0.0039\n",
      "Epoch 005 Batch 006 Loss: 0.0053\n",
      "Epoch 005 Batch 007 Loss: 0.0556\n",
      "Epoch 005 Batch 008 Loss: 0.0172\n",
      "Epoch 005 Batch 009 Loss: 0.1457\n",
      "Epoch 005 Batch 010 Loss: 0.0346\n",
      "Epoch 005 Batch 011 Loss: 0.0581\n",
      "Epoch 005 Batch 012 Loss: 0.0793\n",
      "Epoch 005 Batch 013 Loss: 0.0512\n",
      "Epoch 005 Batch 014 Loss: 0.0102\n",
      "Epoch 005 Batch 015 Loss: 0.0804\n",
      "Epoch 005 Batch 016 Loss: 0.0048\n",
      "Epoch 005 Batch 017 Loss: 0.0051\n",
      "Epoch 005 Batch 018 Loss: 0.0096\n",
      "Epoch 005 Batch 019 Loss: 0.0364\n",
      "Epoch 005 Batch 020 Loss: 0.0351\n",
      "Epoch 005 Batch 021 Loss: 0.0543\n",
      "Epoch 005 Batch 022 Loss: 0.0172\n",
      "Epoch 005 Batch 023 Loss: 0.1503\n",
      "Epoch 005 Batch 024 Loss: 0.0181\n",
      "Epoch 005 Batch 025 Loss: 0.0064\n",
      "Epoch 005 Batch 026 Loss: 0.0104\n",
      "Epoch 005 Batch 027 Loss: 0.0251\n",
      "Epoch 005 Batch 028 Loss: 0.0759\n",
      "Epoch 005 Batch 029 Loss: 0.0171\n",
      "Epoch 005 Batch 030 Loss: 0.0503\n",
      "Epoch 005 Batch 031 Loss: 0.0271\n",
      "Epoch 005 Batch 032 Loss: 0.0287\n",
      "Epoch 005 Batch 033 Loss: 0.0212\n",
      "Epoch 005 Batch 034 Loss: 0.0481\n",
      "Epoch 005 Batch 035 Loss: 0.0186\n",
      "Epoch 005 Batch 036 Loss: 0.0095\n",
      "Epoch 005 Batch 037 Loss: 0.0476\n",
      "Epoch 005 Batch 038 Loss: 0.0071\n",
      "Epoch 005 Batch 039 Loss: 0.0139\n",
      "Epoch 005 Batch 040 Loss: 0.0576\n",
      "Epoch 005 Batch 041 Loss: 0.1212\n",
      "Epoch 005 Batch 042 Loss: 0.0022\n",
      "Epoch 005 Batch 043 Loss: 0.0526\n",
      "Epoch 005 Batch 044 Loss: 0.0324\n",
      "Epoch 005 Batch 045 Loss: 0.0118\n",
      "Epoch 005 Batch 046 Loss: 0.0874\n",
      "Epoch 005 Batch 047 Loss: 0.0091\n",
      "Epoch 005 Batch 048 Loss: 0.0271\n",
      "Epoch 005 Batch 049 Loss: 0.0127\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8642857074737549\n",
      "-----------------------*-----------------------\n",
      "Epoch 006 Batch 000 Loss: 0.0491\n",
      "Epoch 006 Batch 001 Loss: 0.0724\n",
      "Epoch 006 Batch 002 Loss: 0.1124\n",
      "Epoch 006 Batch 003 Loss: 0.0746\n",
      "Epoch 006 Batch 004 Loss: 0.0133\n",
      "Epoch 006 Batch 005 Loss: 0.0320\n",
      "Epoch 006 Batch 006 Loss: 0.0124\n",
      "Epoch 006 Batch 007 Loss: 0.0098\n",
      "Epoch 006 Batch 008 Loss: 0.0508\n",
      "Epoch 006 Batch 009 Loss: 0.0338\n",
      "Epoch 006 Batch 010 Loss: 0.0053\n",
      "Epoch 006 Batch 011 Loss: 0.0018\n",
      "Epoch 006 Batch 012 Loss: 0.0064\n",
      "Epoch 006 Batch 013 Loss: 0.0223\n",
      "Epoch 006 Batch 014 Loss: 0.0944\n",
      "Epoch 006 Batch 015 Loss: 0.0810\n",
      "Epoch 006 Batch 016 Loss: 0.0198\n",
      "Epoch 006 Batch 017 Loss: 0.0094\n",
      "Epoch 006 Batch 018 Loss: 0.0371\n",
      "Epoch 006 Batch 019 Loss: 0.0230\n",
      "Epoch 006 Batch 020 Loss: 0.0118\n",
      "Epoch 006 Batch 021 Loss: 0.0247\n",
      "Epoch 006 Batch 022 Loss: 0.1431\n",
      "Epoch 006 Batch 023 Loss: 0.0398\n",
      "Epoch 006 Batch 024 Loss: 0.1911\n",
      "Epoch 006 Batch 025 Loss: 0.0152\n",
      "Epoch 006 Batch 026 Loss: 0.0116\n",
      "Epoch 006 Batch 027 Loss: 0.0735\n",
      "Epoch 006 Batch 028 Loss: 0.0545\n",
      "Epoch 006 Batch 029 Loss: 0.0672\n",
      "Epoch 006 Batch 030 Loss: 0.0351\n",
      "Epoch 006 Batch 031 Loss: 0.0246\n",
      "Epoch 006 Batch 032 Loss: 0.0296\n",
      "Epoch 006 Batch 033 Loss: 0.0110\n",
      "Epoch 006 Batch 034 Loss: 0.0135\n",
      "Epoch 006 Batch 035 Loss: 0.0090\n",
      "Epoch 006 Batch 036 Loss: 0.0301\n",
      "Epoch 006 Batch 037 Loss: 0.0147\n",
      "Epoch 006 Batch 038 Loss: 0.0490\n",
      "Epoch 006 Batch 039 Loss: 0.0327\n",
      "Epoch 006 Batch 040 Loss: 0.0117\n",
      "Epoch 006 Batch 041 Loss: 0.0543\n",
      "Epoch 006 Batch 042 Loss: 0.0234\n",
      "Epoch 006 Batch 043 Loss: 0.0093\n",
      "Epoch 006 Batch 044 Loss: 0.0150\n",
      "Epoch 006 Batch 045 Loss: 0.0107\n",
      "Epoch 006 Batch 046 Loss: 0.0238\n",
      "Epoch 006 Batch 047 Loss: 0.0679\n",
      "Epoch 006 Batch 048 Loss: 0.0190\n",
      "Epoch 006 Batch 049 Loss: 0.0424\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8357142806053162\n",
      "-----------------------*-----------------------\n",
      "Epoch 007 Batch 000 Loss: 0.0194\n",
      "Epoch 007 Batch 001 Loss: 0.0079\n",
      "Epoch 007 Batch 002 Loss: 0.1137\n",
      "Epoch 007 Batch 003 Loss: 0.1619\n",
      "Epoch 007 Batch 004 Loss: 0.0058\n",
      "Epoch 007 Batch 005 Loss: 0.0133\n",
      "Epoch 007 Batch 006 Loss: 0.0085\n",
      "Epoch 007 Batch 007 Loss: 0.0397\n",
      "Epoch 007 Batch 008 Loss: 0.0593\n",
      "Epoch 007 Batch 009 Loss: 0.1059\n",
      "Epoch 007 Batch 010 Loss: 0.0053\n",
      "Epoch 007 Batch 011 Loss: 0.0269\n",
      "Epoch 007 Batch 012 Loss: 0.1031\n",
      "Epoch 007 Batch 013 Loss: 0.0207\n",
      "Epoch 007 Batch 014 Loss: 0.0333\n",
      "Epoch 007 Batch 015 Loss: 0.0899\n",
      "Epoch 007 Batch 016 Loss: 0.0227\n",
      "Epoch 007 Batch 017 Loss: 0.0133\n",
      "Epoch 007 Batch 018 Loss: 0.0069\n",
      "Epoch 007 Batch 019 Loss: 0.0226\n",
      "Epoch 007 Batch 020 Loss: 0.0034\n",
      "Epoch 007 Batch 021 Loss: 0.0149\n",
      "Epoch 007 Batch 022 Loss: 0.0538\n",
      "Epoch 007 Batch 023 Loss: 0.0377\n",
      "Epoch 007 Batch 024 Loss: 0.0340\n",
      "Epoch 007 Batch 025 Loss: 0.0248\n",
      "Epoch 007 Batch 026 Loss: 0.1232\n",
      "Epoch 007 Batch 027 Loss: 0.0033\n",
      "Epoch 007 Batch 028 Loss: 0.0382\n",
      "Epoch 007 Batch 029 Loss: 0.0116\n",
      "Epoch 007 Batch 030 Loss: 0.0062\n",
      "Epoch 007 Batch 031 Loss: 0.0471\n",
      "Epoch 007 Batch 032 Loss: 0.0083\n",
      "Epoch 007 Batch 033 Loss: 0.0023\n",
      "Epoch 007 Batch 034 Loss: 0.0633\n",
      "Epoch 007 Batch 035 Loss: 0.0124\n",
      "Epoch 007 Batch 036 Loss: 0.0132\n",
      "Epoch 007 Batch 037 Loss: 0.0091\n",
      "Epoch 007 Batch 038 Loss: 0.0111\n",
      "Epoch 007 Batch 039 Loss: 0.0063\n",
      "Epoch 007 Batch 040 Loss: 0.0040\n",
      "Epoch 007 Batch 041 Loss: 0.0119\n",
      "Epoch 007 Batch 042 Loss: 0.1587\n",
      "Epoch 007 Batch 043 Loss: 0.0566\n",
      "Epoch 007 Batch 044 Loss: 0.0538\n",
      "Epoch 007 Batch 045 Loss: 0.0226\n",
      "Epoch 007 Batch 046 Loss: 0.1824\n",
      "Epoch 007 Batch 047 Loss: 0.1060\n",
      "Epoch 007 Batch 048 Loss: 0.0270\n",
      "Epoch 007 Batch 049 Loss: 0.0596\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 008 Batch 000 Loss: 0.1008\n",
      "Epoch 008 Batch 001 Loss: 0.1338\n",
      "Epoch 008 Batch 002 Loss: 0.0947\n",
      "Epoch 008 Batch 003 Loss: 0.0641\n",
      "Epoch 008 Batch 004 Loss: 0.0765\n",
      "Epoch 008 Batch 005 Loss: 0.0323\n",
      "Epoch 008 Batch 006 Loss: 0.0643\n",
      "Epoch 008 Batch 007 Loss: 0.1349\n",
      "Epoch 008 Batch 008 Loss: 0.0217\n",
      "Epoch 008 Batch 009 Loss: 0.3598\n",
      "Epoch 008 Batch 010 Loss: 0.1031\n",
      "Epoch 008 Batch 011 Loss: 0.0820\n",
      "Epoch 008 Batch 012 Loss: 0.0534\n",
      "Epoch 008 Batch 013 Loss: 0.2201\n",
      "Epoch 008 Batch 014 Loss: 0.0066\n",
      "Epoch 008 Batch 015 Loss: 0.0452\n",
      "Epoch 008 Batch 016 Loss: 0.1311\n",
      "Epoch 008 Batch 017 Loss: 0.0824\n",
      "Epoch 008 Batch 018 Loss: 0.0130\n",
      "Epoch 008 Batch 019 Loss: 0.0297\n",
      "Epoch 008 Batch 020 Loss: 0.0197\n",
      "Epoch 008 Batch 021 Loss: 0.2028\n",
      "Epoch 008 Batch 022 Loss: 0.0843\n",
      "Epoch 008 Batch 023 Loss: 0.0378\n",
      "Epoch 008 Batch 024 Loss: 0.1440\n",
      "Epoch 008 Batch 025 Loss: 0.0300\n",
      "Epoch 008 Batch 026 Loss: 0.0632\n",
      "Epoch 008 Batch 027 Loss: 0.0280\n",
      "Epoch 008 Batch 028 Loss: 0.0081\n",
      "Epoch 008 Batch 029 Loss: 0.0422\n",
      "Epoch 008 Batch 030 Loss: 0.0593\n",
      "Epoch 008 Batch 031 Loss: 0.0457\n",
      "Epoch 008 Batch 032 Loss: 0.0170\n",
      "Epoch 008 Batch 033 Loss: 0.0985\n",
      "Epoch 008 Batch 034 Loss: 0.1726\n",
      "Epoch 008 Batch 035 Loss: 0.0163\n",
      "Epoch 008 Batch 036 Loss: 0.0932\n",
      "Epoch 008 Batch 037 Loss: 0.0339\n",
      "Epoch 008 Batch 038 Loss: 0.0117\n",
      "Epoch 008 Batch 039 Loss: 0.0445\n",
      "Epoch 008 Batch 040 Loss: 0.0471\n",
      "Epoch 008 Batch 041 Loss: 0.0583\n",
      "Epoch 008 Batch 042 Loss: 0.0912\n",
      "Epoch 008 Batch 043 Loss: 0.1214\n",
      "Epoch 008 Batch 044 Loss: 0.0847\n",
      "Epoch 008 Batch 045 Loss: 0.0557\n",
      "Epoch 008 Batch 046 Loss: 0.0612\n",
      "Epoch 008 Batch 047 Loss: 0.0336\n",
      "Epoch 008 Batch 048 Loss: 0.2097\n",
      "Epoch 008 Batch 049 Loss: 0.0068\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8214285969734192\n",
      "-----------------------*-----------------------\n",
      "Epoch 009 Batch 000 Loss: 0.0174\n",
      "Epoch 009 Batch 001 Loss: 0.1480\n",
      "Epoch 009 Batch 002 Loss: 0.0378\n",
      "Epoch 009 Batch 003 Loss: 0.0522\n",
      "Epoch 009 Batch 004 Loss: 0.0380\n",
      "Epoch 009 Batch 005 Loss: 0.0417\n",
      "Epoch 009 Batch 006 Loss: 0.0477\n",
      "Epoch 009 Batch 007 Loss: 0.0300\n",
      "Epoch 009 Batch 008 Loss: 0.1167\n",
      "Epoch 009 Batch 009 Loss: 0.0487\n",
      "Epoch 009 Batch 010 Loss: 0.0398\n",
      "Epoch 009 Batch 011 Loss: 0.0290\n",
      "Epoch 009 Batch 012 Loss: 0.1008\n",
      "Epoch 009 Batch 013 Loss: 0.0153\n",
      "Epoch 009 Batch 014 Loss: 0.0153\n",
      "Epoch 009 Batch 015 Loss: 0.0538\n",
      "Epoch 009 Batch 016 Loss: 0.0213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 Batch 017 Loss: 0.0198\n",
      "Epoch 009 Batch 018 Loss: 0.0129\n",
      "Epoch 009 Batch 019 Loss: 0.0103\n",
      "Epoch 009 Batch 020 Loss: 0.0142\n",
      "Epoch 009 Batch 021 Loss: 0.1549\n",
      "Epoch 009 Batch 022 Loss: 0.0209\n",
      "Epoch 009 Batch 023 Loss: 0.0110\n",
      "Epoch 009 Batch 024 Loss: 0.0245\n",
      "Epoch 009 Batch 025 Loss: 0.0495\n",
      "Epoch 009 Batch 026 Loss: 0.0106\n",
      "Epoch 009 Batch 027 Loss: 0.0069\n",
      "Epoch 009 Batch 028 Loss: 0.0104\n",
      "Epoch 009 Batch 029 Loss: 0.0113\n",
      "Epoch 009 Batch 030 Loss: 0.0663\n",
      "Epoch 009 Batch 031 Loss: 0.0050\n",
      "Epoch 009 Batch 032 Loss: 0.0344\n",
      "Epoch 009 Batch 033 Loss: 0.0375\n",
      "Epoch 009 Batch 034 Loss: 0.0139\n",
      "Epoch 009 Batch 035 Loss: 0.0100\n",
      "Epoch 009 Batch 036 Loss: 0.0058\n",
      "Epoch 009 Batch 037 Loss: 0.0126\n",
      "Epoch 009 Batch 038 Loss: 0.0116\n",
      "Epoch 009 Batch 039 Loss: 0.0034\n",
      "Epoch 009 Batch 040 Loss: 0.0923\n",
      "Epoch 009 Batch 041 Loss: 0.0424\n",
      "Epoch 009 Batch 042 Loss: 0.0032\n",
      "Epoch 009 Batch 043 Loss: 0.0039\n",
      "Epoch 009 Batch 044 Loss: 0.1162\n",
      "Epoch 009 Batch 045 Loss: 0.0104\n",
      "Epoch 009 Batch 046 Loss: 0.0318\n",
      "Epoch 009 Batch 047 Loss: 0.0387\n",
      "Epoch 009 Batch 048 Loss: 0.0873\n",
      "Epoch 009 Batch 049 Loss: 0.0213\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 010 Batch 000 Loss: 0.0254\n",
      "Epoch 010 Batch 001 Loss: 0.0348\n",
      "Epoch 010 Batch 002 Loss: 0.0659\n",
      "Epoch 010 Batch 003 Loss: 0.0760\n",
      "Epoch 010 Batch 004 Loss: 0.0806\n",
      "Epoch 010 Batch 005 Loss: 0.0335\n",
      "Epoch 010 Batch 006 Loss: 0.0036\n",
      "Epoch 010 Batch 007 Loss: 0.0481\n",
      "Epoch 010 Batch 008 Loss: 0.0251\n",
      "Epoch 010 Batch 009 Loss: 0.0452\n",
      "Epoch 010 Batch 010 Loss: 0.0674\n",
      "Epoch 010 Batch 011 Loss: 0.0206\n",
      "Epoch 010 Batch 012 Loss: 0.0399\n",
      "Epoch 010 Batch 013 Loss: 0.0255\n",
      "Epoch 010 Batch 014 Loss: 0.0063\n",
      "Epoch 010 Batch 015 Loss: 0.0274\n",
      "Epoch 010 Batch 016 Loss: 0.0378\n",
      "Epoch 010 Batch 017 Loss: 0.0850\n",
      "Epoch 010 Batch 018 Loss: 0.0091\n",
      "Epoch 010 Batch 019 Loss: 0.1026\n",
      "Epoch 010 Batch 020 Loss: 0.1034\n",
      "Epoch 010 Batch 021 Loss: 0.0690\n",
      "Epoch 010 Batch 022 Loss: 0.0491\n",
      "Epoch 010 Batch 023 Loss: 0.0116\n",
      "Epoch 010 Batch 024 Loss: 0.0182\n",
      "Epoch 010 Batch 025 Loss: 0.0173\n",
      "Epoch 010 Batch 026 Loss: 0.0515\n",
      "Epoch 010 Batch 027 Loss: 0.0345\n",
      "Epoch 010 Batch 028 Loss: 0.0176\n",
      "Epoch 010 Batch 029 Loss: 0.0592\n",
      "Epoch 010 Batch 030 Loss: 0.0509\n",
      "Epoch 010 Batch 031 Loss: 0.0507\n",
      "Epoch 010 Batch 032 Loss: 0.0109\n",
      "Epoch 010 Batch 033 Loss: 0.0741\n",
      "Epoch 010 Batch 034 Loss: 0.0080\n",
      "Epoch 010 Batch 035 Loss: 0.0073\n",
      "Epoch 010 Batch 036 Loss: 0.1955\n",
      "Epoch 010 Batch 037 Loss: 0.0512\n",
      "Epoch 010 Batch 038 Loss: 0.0146\n",
      "Epoch 010 Batch 039 Loss: 0.0134\n",
      "Epoch 010 Batch 040 Loss: 0.0189\n",
      "Epoch 010 Batch 041 Loss: 0.0656\n",
      "Epoch 010 Batch 042 Loss: 0.0175\n",
      "Epoch 010 Batch 043 Loss: 0.0277\n",
      "Epoch 010 Batch 044 Loss: 0.0451\n",
      "Epoch 010 Batch 045 Loss: 0.0066\n",
      "Epoch 010 Batch 046 Loss: 0.0300\n",
      "Epoch 010 Batch 047 Loss: 0.0106\n",
      "Epoch 010 Batch 048 Loss: 0.0050\n",
      "Epoch 010 Batch 049 Loss: 0.0212\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8571428656578064\n",
      "-----------------------*-----------------------\n",
      "Epoch 011 Batch 000 Loss: 0.0063\n",
      "Epoch 011 Batch 001 Loss: 0.0752\n",
      "Epoch 011 Batch 002 Loss: 0.0072\n",
      "Epoch 011 Batch 003 Loss: 0.0196\n",
      "Epoch 011 Batch 004 Loss: 0.0122\n",
      "Epoch 011 Batch 005 Loss: 0.0179\n",
      "Epoch 011 Batch 006 Loss: 0.0452\n",
      "Epoch 011 Batch 007 Loss: 0.0154\n",
      "Epoch 011 Batch 008 Loss: 0.0965\n",
      "Epoch 011 Batch 009 Loss: 0.0505\n",
      "Epoch 011 Batch 010 Loss: 0.0306\n",
      "Epoch 011 Batch 011 Loss: 0.0047\n",
      "Epoch 011 Batch 012 Loss: 0.0266\n",
      "Epoch 011 Batch 013 Loss: 0.0749\n",
      "Epoch 011 Batch 014 Loss: 0.1828\n",
      "Epoch 011 Batch 015 Loss: 0.0139\n",
      "Epoch 011 Batch 016 Loss: 0.0047\n",
      "Epoch 011 Batch 017 Loss: 0.0275\n",
      "Epoch 011 Batch 018 Loss: 0.0201\n",
      "Epoch 011 Batch 019 Loss: 0.0152\n",
      "Epoch 011 Batch 020 Loss: 0.0554\n",
      "Epoch 011 Batch 021 Loss: 0.1293\n",
      "Epoch 011 Batch 022 Loss: 0.0172\n",
      "Epoch 011 Batch 023 Loss: 0.0149\n",
      "Epoch 011 Batch 024 Loss: 0.0176\n",
      "Epoch 011 Batch 025 Loss: 0.0065\n",
      "Epoch 011 Batch 026 Loss: 0.0327\n",
      "Epoch 011 Batch 027 Loss: 0.0614\n",
      "Epoch 011 Batch 028 Loss: 0.0577\n",
      "Epoch 011 Batch 029 Loss: 0.0036\n",
      "Epoch 011 Batch 030 Loss: 0.0408\n",
      "Epoch 011 Batch 031 Loss: 0.0830\n",
      "Epoch 011 Batch 032 Loss: 0.0502\n",
      "Epoch 011 Batch 033 Loss: 0.0049\n",
      "Epoch 011 Batch 034 Loss: 0.0849\n",
      "Epoch 011 Batch 035 Loss: 0.0309\n",
      "Epoch 011 Batch 036 Loss: 0.1889\n",
      "Epoch 011 Batch 037 Loss: 0.0278\n",
      "Epoch 011 Batch 038 Loss: 0.0128\n",
      "Epoch 011 Batch 039 Loss: 0.0094\n",
      "Epoch 011 Batch 040 Loss: 0.3064\n",
      "Epoch 011 Batch 041 Loss: 0.0408\n",
      "Epoch 011 Batch 042 Loss: 0.1725\n",
      "Epoch 011 Batch 043 Loss: 0.0863\n",
      "Epoch 011 Batch 044 Loss: 0.0127\n",
      "Epoch 011 Batch 045 Loss: 0.0124\n",
      "Epoch 011 Batch 046 Loss: 0.0194\n",
      "Epoch 011 Batch 047 Loss: 0.0309\n",
      "Epoch 011 Batch 048 Loss: 0.0562\n",
      "Epoch 011 Batch 049 Loss: 0.0263\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 012 Batch 000 Loss: 0.0135\n",
      "Epoch 012 Batch 001 Loss: 0.0151\n",
      "Epoch 012 Batch 002 Loss: 0.0086\n",
      "Epoch 012 Batch 003 Loss: 0.0394\n",
      "Epoch 012 Batch 004 Loss: 0.0329\n",
      "Epoch 012 Batch 005 Loss: 0.0357\n",
      "Epoch 012 Batch 006 Loss: 0.0196\n",
      "Epoch 012 Batch 007 Loss: 0.0107\n",
      "Epoch 012 Batch 008 Loss: 0.0237\n",
      "Epoch 012 Batch 009 Loss: 0.0269\n",
      "Epoch 012 Batch 010 Loss: 0.0079\n",
      "Epoch 012 Batch 011 Loss: 0.1246\n",
      "Epoch 012 Batch 012 Loss: 0.0230\n",
      "Epoch 012 Batch 013 Loss: 0.0165\n",
      "Epoch 012 Batch 014 Loss: 0.1409\n",
      "Epoch 012 Batch 015 Loss: 0.0057\n",
      "Epoch 012 Batch 016 Loss: 0.0552\n",
      "Epoch 012 Batch 017 Loss: 0.0894\n",
      "Epoch 012 Batch 018 Loss: 0.0406\n",
      "Epoch 012 Batch 019 Loss: 0.0860\n",
      "Epoch 012 Batch 020 Loss: 0.0090\n",
      "Epoch 012 Batch 021 Loss: 0.0137\n",
      "Epoch 012 Batch 022 Loss: 0.0609\n",
      "Epoch 012 Batch 023 Loss: 0.0773\n",
      "Epoch 012 Batch 024 Loss: 0.0407\n",
      "Epoch 012 Batch 025 Loss: 0.0083\n",
      "Epoch 012 Batch 026 Loss: 0.1490\n",
      "Epoch 012 Batch 027 Loss: 0.0243\n",
      "Epoch 012 Batch 028 Loss: 0.0635\n",
      "Epoch 012 Batch 029 Loss: 0.0277\n",
      "Epoch 012 Batch 030 Loss: 0.0392\n",
      "Epoch 012 Batch 031 Loss: 0.0369\n",
      "Epoch 012 Batch 032 Loss: 0.1340\n",
      "Epoch 012 Batch 033 Loss: 0.0481\n",
      "Epoch 012 Batch 034 Loss: 0.0436\n",
      "Epoch 012 Batch 035 Loss: 0.0116\n",
      "Epoch 012 Batch 036 Loss: 0.0158\n",
      "Epoch 012 Batch 037 Loss: 0.0080\n",
      "Epoch 012 Batch 038 Loss: 0.0676\n",
      "Epoch 012 Batch 039 Loss: 0.0336\n",
      "Epoch 012 Batch 040 Loss: 0.0290\n",
      "Epoch 012 Batch 041 Loss: 0.0062\n",
      "Epoch 012 Batch 042 Loss: 0.0196\n",
      "Epoch 012 Batch 043 Loss: 0.0610\n",
      "Epoch 012 Batch 044 Loss: 0.0113\n",
      "Epoch 012 Batch 045 Loss: 0.0989\n",
      "Epoch 012 Batch 046 Loss: 0.0662\n",
      "Epoch 012 Batch 047 Loss: 0.0763\n",
      "Epoch 012 Batch 048 Loss: 0.0345\n",
      "Epoch 012 Batch 049 Loss: 0.0724\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8214285969734192\n",
      "-----------------------*-----------------------\n",
      "Epoch 013 Batch 000 Loss: 0.0196\n",
      "Epoch 013 Batch 001 Loss: 0.0727\n",
      "Epoch 013 Batch 002 Loss: 0.0226\n",
      "Epoch 013 Batch 003 Loss: 0.0180\n",
      "Epoch 013 Batch 004 Loss: 0.0142\n",
      "Epoch 013 Batch 005 Loss: 0.0496\n",
      "Epoch 013 Batch 006 Loss: 0.0084\n",
      "Epoch 013 Batch 007 Loss: 0.0050\n",
      "Epoch 013 Batch 008 Loss: 0.0465\n",
      "Epoch 013 Batch 009 Loss: 0.0819\n",
      "Epoch 013 Batch 010 Loss: 0.0142\n",
      "Epoch 013 Batch 011 Loss: 0.0213\n",
      "Epoch 013 Batch 012 Loss: 0.0396\n",
      "Epoch 013 Batch 013 Loss: 0.0497\n",
      "Epoch 013 Batch 014 Loss: 0.0330\n",
      "Epoch 013 Batch 015 Loss: 0.0105\n",
      "Epoch 013 Batch 016 Loss: 0.0046\n",
      "Epoch 013 Batch 017 Loss: 0.0056\n",
      "Epoch 013 Batch 018 Loss: 0.0514\n",
      "Epoch 013 Batch 019 Loss: 0.0176\n",
      "Epoch 013 Batch 020 Loss: 0.1343\n",
      "Epoch 013 Batch 021 Loss: 0.0064\n",
      "Epoch 013 Batch 022 Loss: 0.0246\n",
      "Epoch 013 Batch 023 Loss: 0.0081\n",
      "Epoch 013 Batch 024 Loss: 0.0139\n",
      "Epoch 013 Batch 025 Loss: 0.0755\n",
      "Epoch 013 Batch 026 Loss: 0.0188\n",
      "Epoch 013 Batch 027 Loss: 0.0080\n",
      "Epoch 013 Batch 028 Loss: 0.0066\n",
      "Epoch 013 Batch 029 Loss: 0.0212\n",
      "Epoch 013 Batch 030 Loss: 0.0477\n",
      "Epoch 013 Batch 031 Loss: 0.0325\n",
      "Epoch 013 Batch 032 Loss: 0.0101\n",
      "Epoch 013 Batch 033 Loss: 0.0119\n",
      "Epoch 013 Batch 034 Loss: 0.0080\n",
      "Epoch 013 Batch 035 Loss: 0.0061\n",
      "Epoch 013 Batch 036 Loss: 0.0287\n",
      "Epoch 013 Batch 037 Loss: 0.0158\n",
      "Epoch 013 Batch 038 Loss: 0.0077\n",
      "Epoch 013 Batch 039 Loss: 0.0326\n",
      "Epoch 013 Batch 040 Loss: 0.0041\n",
      "Epoch 013 Batch 041 Loss: 0.0197\n",
      "Epoch 013 Batch 042 Loss: 0.0938\n",
      "Epoch 013 Batch 043 Loss: 0.0028\n",
      "Epoch 013 Batch 044 Loss: 0.0070\n",
      "Epoch 013 Batch 045 Loss: 0.0267\n",
      "Epoch 013 Batch 046 Loss: 0.0449\n",
      "Epoch 013 Batch 047 Loss: 0.0117\n",
      "Epoch 013 Batch 048 Loss: 0.0026\n",
      "Epoch 013 Batch 049 Loss: 0.0371\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 014 Batch 000 Loss: 0.1526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 Batch 001 Loss: 0.0107\n",
      "Epoch 014 Batch 002 Loss: 0.0069\n",
      "Epoch 014 Batch 003 Loss: 0.0104\n",
      "Epoch 014 Batch 004 Loss: 0.0092\n",
      "Epoch 014 Batch 005 Loss: 0.0159\n",
      "Epoch 014 Batch 006 Loss: 0.0253\n",
      "Epoch 014 Batch 007 Loss: 0.0305\n",
      "Epoch 014 Batch 008 Loss: 0.0090\n",
      "Epoch 014 Batch 009 Loss: 0.0156\n",
      "Epoch 014 Batch 010 Loss: 0.0044\n",
      "Epoch 014 Batch 011 Loss: 0.0163\n",
      "Epoch 014 Batch 012 Loss: 0.0140\n",
      "Epoch 014 Batch 013 Loss: 0.0396\n",
      "Epoch 014 Batch 014 Loss: 0.0382\n",
      "Epoch 014 Batch 015 Loss: 0.0065\n",
      "Epoch 014 Batch 016 Loss: 0.0300\n",
      "Epoch 014 Batch 017 Loss: 0.0326\n",
      "Epoch 014 Batch 018 Loss: 0.0319\n",
      "Epoch 014 Batch 019 Loss: 0.1148\n",
      "Epoch 014 Batch 020 Loss: 0.0092\n",
      "Epoch 014 Batch 021 Loss: 0.0146\n",
      "Epoch 014 Batch 022 Loss: 0.0567\n",
      "Epoch 014 Batch 023 Loss: 0.0156\n",
      "Epoch 014 Batch 024 Loss: 0.0089\n",
      "Epoch 014 Batch 025 Loss: 0.0466\n",
      "Epoch 014 Batch 026 Loss: 0.1384\n",
      "Epoch 014 Batch 027 Loss: 0.0483\n",
      "Epoch 014 Batch 028 Loss: 0.0346\n",
      "Epoch 014 Batch 029 Loss: 0.0553\n",
      "Epoch 014 Batch 030 Loss: 0.0157\n",
      "Epoch 014 Batch 031 Loss: 0.0333\n",
      "Epoch 014 Batch 032 Loss: 0.0347\n",
      "Epoch 014 Batch 033 Loss: 0.0138\n",
      "Epoch 014 Batch 034 Loss: 0.0325\n",
      "Epoch 014 Batch 035 Loss: 0.0108\n",
      "Epoch 014 Batch 036 Loss: 0.0068\n",
      "Epoch 014 Batch 037 Loss: 0.0351\n",
      "Epoch 014 Batch 038 Loss: 0.0060\n",
      "Epoch 014 Batch 039 Loss: 0.0253\n",
      "Epoch 014 Batch 040 Loss: 0.0104\n",
      "Epoch 014 Batch 041 Loss: 0.0141\n",
      "Epoch 014 Batch 042 Loss: 0.0826\n",
      "Epoch 014 Batch 043 Loss: 0.0327\n",
      "Epoch 014 Batch 044 Loss: 0.0077\n",
      "Epoch 014 Batch 045 Loss: 0.0779\n",
      "Epoch 014 Batch 046 Loss: 0.0025\n",
      "Epoch 014 Batch 047 Loss: 0.0306\n",
      "Epoch 014 Batch 048 Loss: 0.0234\n",
      "Epoch 014 Batch 049 Loss: 0.0923\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8357142806053162\n",
      "-----------------------*-----------------------\n",
      "Epoch 015 Batch 000 Loss: 0.1167\n",
      "Epoch 015 Batch 001 Loss: 0.0142\n",
      "Epoch 015 Batch 002 Loss: 0.0176\n",
      "Epoch 015 Batch 003 Loss: 0.1093\n",
      "Epoch 015 Batch 004 Loss: 0.0226\n",
      "Epoch 015 Batch 005 Loss: 0.0070\n",
      "Epoch 015 Batch 006 Loss: 0.0080\n",
      "Epoch 015 Batch 007 Loss: 0.0206\n",
      "Epoch 015 Batch 008 Loss: 0.0516\n",
      "Epoch 015 Batch 009 Loss: 0.0199\n",
      "Epoch 015 Batch 010 Loss: 0.0713\n",
      "Epoch 015 Batch 011 Loss: 0.0350\n",
      "Epoch 015 Batch 012 Loss: 0.0059\n",
      "Epoch 015 Batch 013 Loss: 0.0577\n",
      "Epoch 015 Batch 014 Loss: 0.0453\n",
      "Epoch 015 Batch 015 Loss: 0.0637\n",
      "Epoch 015 Batch 016 Loss: 0.0645\n",
      "Epoch 015 Batch 017 Loss: 0.0137\n",
      "Epoch 015 Batch 018 Loss: 0.0062\n",
      "Epoch 015 Batch 019 Loss: 0.0078\n",
      "Epoch 015 Batch 020 Loss: 0.0544\n",
      "Epoch 015 Batch 021 Loss: 0.0591\n",
      "Epoch 015 Batch 022 Loss: 0.0036\n",
      "Epoch 015 Batch 023 Loss: 0.1508\n",
      "Epoch 015 Batch 024 Loss: 0.0218\n",
      "Epoch 015 Batch 025 Loss: 0.0073\n",
      "Epoch 015 Batch 026 Loss: 0.0664\n",
      "Epoch 015 Batch 027 Loss: 0.0037\n",
      "Epoch 015 Batch 028 Loss: 0.0122\n",
      "Epoch 015 Batch 029 Loss: 0.0759\n",
      "Epoch 015 Batch 030 Loss: 0.0351\n",
      "Epoch 015 Batch 031 Loss: 0.0444\n",
      "Epoch 015 Batch 032 Loss: 0.0039\n",
      "Epoch 015 Batch 033 Loss: 0.0443\n",
      "Epoch 015 Batch 034 Loss: 0.0560\n",
      "Epoch 015 Batch 035 Loss: 0.0143\n",
      "Epoch 015 Batch 036 Loss: 0.0100\n",
      "Epoch 015 Batch 037 Loss: 0.0567\n",
      "Epoch 015 Batch 038 Loss: 0.0895\n",
      "Epoch 015 Batch 039 Loss: 0.0111\n",
      "Epoch 015 Batch 040 Loss: 0.1420\n",
      "Epoch 015 Batch 041 Loss: 0.0183\n",
      "Epoch 015 Batch 042 Loss: 0.0205\n",
      "Epoch 015 Batch 043 Loss: 0.0096\n",
      "Epoch 015 Batch 044 Loss: 0.0527\n",
      "Epoch 015 Batch 045 Loss: 0.0942\n",
      "Epoch 015 Batch 046 Loss: 0.0350\n",
      "Epoch 015 Batch 047 Loss: 0.1075\n",
      "Epoch 015 Batch 048 Loss: 0.0439\n",
      "Epoch 015 Batch 049 Loss: 0.0654\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 016 Batch 000 Loss: 0.0251\n",
      "Epoch 016 Batch 001 Loss: 0.0249\n",
      "Epoch 016 Batch 002 Loss: 0.0267\n",
      "Epoch 016 Batch 003 Loss: 0.0449\n",
      "Epoch 016 Batch 004 Loss: 0.1213\n",
      "Epoch 016 Batch 005 Loss: 0.0087\n",
      "Epoch 016 Batch 006 Loss: 0.1412\n",
      "Epoch 016 Batch 007 Loss: 0.0339\n",
      "Epoch 016 Batch 008 Loss: 0.0685\n",
      "Epoch 016 Batch 009 Loss: 0.0362\n",
      "Epoch 016 Batch 010 Loss: 0.0208\n",
      "Epoch 016 Batch 011 Loss: 0.0076\n",
      "Epoch 016 Batch 012 Loss: 0.0210\n",
      "Epoch 016 Batch 013 Loss: 0.0936\n",
      "Epoch 016 Batch 014 Loss: 0.2081\n",
      "Epoch 016 Batch 015 Loss: 0.0335\n",
      "Epoch 016 Batch 016 Loss: 0.3575\n",
      "Epoch 016 Batch 017 Loss: 0.2121\n",
      "Epoch 016 Batch 018 Loss: 0.1246\n",
      "Epoch 016 Batch 019 Loss: 0.0485\n",
      "Epoch 016 Batch 020 Loss: 0.0626\n",
      "Epoch 016 Batch 021 Loss: 0.1077\n",
      "Epoch 016 Batch 022 Loss: 0.1145\n",
      "Epoch 016 Batch 023 Loss: 0.0449\n",
      "Epoch 016 Batch 024 Loss: 0.1994\n",
      "Epoch 016 Batch 025 Loss: 0.0788\n",
      "Epoch 016 Batch 026 Loss: 0.1544\n",
      "Epoch 016 Batch 027 Loss: 0.0425\n",
      "Epoch 016 Batch 028 Loss: 0.0791\n",
      "Epoch 016 Batch 029 Loss: 0.0833\n",
      "Epoch 016 Batch 030 Loss: 0.1520\n",
      "Epoch 016 Batch 031 Loss: 0.0197\n",
      "Epoch 016 Batch 032 Loss: 0.0491\n",
      "Epoch 016 Batch 033 Loss: 0.0293\n",
      "Epoch 016 Batch 034 Loss: 0.0503\n",
      "Epoch 016 Batch 035 Loss: 0.0217\n",
      "Epoch 016 Batch 036 Loss: 0.0330\n",
      "Epoch 016 Batch 037 Loss: 0.0926\n",
      "Epoch 016 Batch 038 Loss: 0.0107\n",
      "Epoch 016 Batch 039 Loss: 0.0184\n",
      "Epoch 016 Batch 040 Loss: 0.0382\n",
      "Epoch 016 Batch 041 Loss: 0.1987\n",
      "Epoch 016 Batch 042 Loss: 0.1423\n",
      "Epoch 016 Batch 043 Loss: 0.2142\n",
      "Epoch 016 Batch 044 Loss: 0.1092\n",
      "Epoch 016 Batch 045 Loss: 0.0788\n",
      "Epoch 016 Batch 046 Loss: 0.0753\n",
      "Epoch 016 Batch 047 Loss: 0.0999\n",
      "Epoch 016 Batch 048 Loss: 0.1061\n",
      "Epoch 016 Batch 049 Loss: 0.0994\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 017 Batch 000 Loss: 0.0868\n",
      "Epoch 017 Batch 001 Loss: 0.1289\n",
      "Epoch 017 Batch 002 Loss: 0.0168\n",
      "Epoch 017 Batch 003 Loss: 0.0317\n",
      "Epoch 017 Batch 004 Loss: 0.1069\n",
      "Epoch 017 Batch 005 Loss: 0.0679\n",
      "Epoch 017 Batch 006 Loss: 0.0892\n",
      "Epoch 017 Batch 007 Loss: 0.0228\n",
      "Epoch 017 Batch 008 Loss: 0.0544\n",
      "Epoch 017 Batch 009 Loss: 0.1073\n",
      "Epoch 017 Batch 010 Loss: 0.0317\n",
      "Epoch 017 Batch 011 Loss: 0.0213\n",
      "Epoch 017 Batch 012 Loss: 0.0875\n",
      "Epoch 017 Batch 013 Loss: 0.2632\n",
      "Epoch 017 Batch 014 Loss: 0.0130\n",
      "Epoch 017 Batch 015 Loss: 0.0442\n",
      "Epoch 017 Batch 016 Loss: 0.0362\n",
      "Epoch 017 Batch 017 Loss: 0.0806\n",
      "Epoch 017 Batch 018 Loss: 0.1740\n",
      "Epoch 017 Batch 019 Loss: 0.0762\n",
      "Epoch 017 Batch 020 Loss: 0.0140\n",
      "Epoch 017 Batch 021 Loss: 0.0227\n",
      "Epoch 017 Batch 022 Loss: 0.0209\n",
      "Epoch 017 Batch 023 Loss: 0.1261\n",
      "Epoch 017 Batch 024 Loss: 0.1302\n",
      "Epoch 017 Batch 025 Loss: 0.0611\n",
      "Epoch 017 Batch 026 Loss: 0.0079\n",
      "Epoch 017 Batch 027 Loss: 0.0731\n",
      "Epoch 017 Batch 028 Loss: 0.0100\n",
      "Epoch 017 Batch 029 Loss: 0.0175\n",
      "Epoch 017 Batch 030 Loss: 0.0607\n",
      "Epoch 017 Batch 031 Loss: 0.2439\n",
      "Epoch 017 Batch 032 Loss: 0.0059\n",
      "Epoch 017 Batch 033 Loss: 0.1732\n",
      "Epoch 017 Batch 034 Loss: 0.0544\n",
      "Epoch 017 Batch 035 Loss: 0.0841\n",
      "Epoch 017 Batch 036 Loss: 0.1432\n",
      "Epoch 017 Batch 037 Loss: 0.0682\n",
      "Epoch 017 Batch 038 Loss: 0.0580\n",
      "Epoch 017 Batch 039 Loss: 0.0678\n",
      "Epoch 017 Batch 040 Loss: 0.0330\n",
      "Epoch 017 Batch 041 Loss: 0.2564\n",
      "Epoch 017 Batch 042 Loss: 0.6698\n",
      "Epoch 017 Batch 043 Loss: 0.1277\n",
      "Epoch 017 Batch 044 Loss: 0.1252\n",
      "Epoch 017 Batch 045 Loss: 0.0944\n",
      "Epoch 017 Batch 046 Loss: 0.0472\n",
      "Epoch 017 Batch 047 Loss: 0.0367\n",
      "Epoch 017 Batch 048 Loss: 0.0429\n",
      "Epoch 017 Batch 049 Loss: 0.0845\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 018 Batch 000 Loss: 0.0155\n",
      "Epoch 018 Batch 001 Loss: 0.2388\n",
      "Epoch 018 Batch 002 Loss: 0.0096\n",
      "Epoch 018 Batch 003 Loss: 0.2040\n",
      "Epoch 018 Batch 004 Loss: 0.0222\n",
      "Epoch 018 Batch 005 Loss: 0.0414\n",
      "Epoch 018 Batch 006 Loss: 0.0318\n",
      "Epoch 018 Batch 007 Loss: 0.0682\n",
      "Epoch 018 Batch 008 Loss: 0.0960\n",
      "Epoch 018 Batch 009 Loss: 0.0724\n",
      "Epoch 018 Batch 010 Loss: 0.0303\n",
      "Epoch 018 Batch 011 Loss: 0.0860\n",
      "Epoch 018 Batch 012 Loss: 0.0069\n",
      "Epoch 018 Batch 013 Loss: 0.0885\n",
      "Epoch 018 Batch 014 Loss: 0.0192\n",
      "Epoch 018 Batch 015 Loss: 0.0256\n",
      "Epoch 018 Batch 016 Loss: 0.0140\n",
      "Epoch 018 Batch 017 Loss: 0.0230\n",
      "Epoch 018 Batch 018 Loss: 0.0244\n",
      "Epoch 018 Batch 019 Loss: 0.0226\n",
      "Epoch 018 Batch 020 Loss: 0.0185\n",
      "Epoch 018 Batch 021 Loss: 0.0115\n",
      "Epoch 018 Batch 022 Loss: 0.0224\n",
      "Epoch 018 Batch 023 Loss: 0.0078\n",
      "Epoch 018 Batch 024 Loss: 0.0236\n",
      "Epoch 018 Batch 025 Loss: 0.0281\n",
      "Epoch 018 Batch 026 Loss: 0.1528\n",
      "Epoch 018 Batch 027 Loss: 0.0217\n",
      "Epoch 018 Batch 028 Loss: 0.1230\n",
      "Epoch 018 Batch 029 Loss: 0.0435\n",
      "Epoch 018 Batch 030 Loss: 0.0205\n",
      "Epoch 018 Batch 031 Loss: 0.0038\n",
      "Epoch 018 Batch 032 Loss: 0.0118\n",
      "Epoch 018 Batch 033 Loss: 0.0085\n",
      "Epoch 018 Batch 034 Loss: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 Batch 035 Loss: 0.0108\n",
      "Epoch 018 Batch 036 Loss: 0.0354\n",
      "Epoch 018 Batch 037 Loss: 0.0443\n",
      "Epoch 018 Batch 038 Loss: 0.0088\n",
      "Epoch 018 Batch 039 Loss: 0.0119\n",
      "Epoch 018 Batch 040 Loss: 0.0032\n",
      "Epoch 018 Batch 041 Loss: 0.0437\n",
      "Epoch 018 Batch 042 Loss: 0.0128\n",
      "Epoch 018 Batch 043 Loss: 0.0558\n",
      "Epoch 018 Batch 044 Loss: 0.0664\n",
      "Epoch 018 Batch 045 Loss: 0.0124\n",
      "Epoch 018 Batch 046 Loss: 0.0067\n",
      "Epoch 018 Batch 047 Loss: 0.0793\n",
      "Epoch 018 Batch 048 Loss: 0.0146\n",
      "Epoch 018 Batch 049 Loss: 0.0081\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8214285969734192\n",
      "-----------------------*-----------------------\n",
      "Epoch 019 Batch 000 Loss: 0.0485\n",
      "Epoch 019 Batch 001 Loss: 0.0247\n",
      "Epoch 019 Batch 002 Loss: 0.0391\n",
      "Epoch 019 Batch 003 Loss: 0.0300\n",
      "Epoch 019 Batch 004 Loss: 0.0107\n",
      "Epoch 019 Batch 005 Loss: 0.0306\n",
      "Epoch 019 Batch 006 Loss: 0.1032\n",
      "Epoch 019 Batch 007 Loss: 0.0077\n",
      "Epoch 019 Batch 008 Loss: 0.0126\n",
      "Epoch 019 Batch 009 Loss: 0.0352\n",
      "Epoch 019 Batch 010 Loss: 0.0115\n",
      "Epoch 019 Batch 011 Loss: 0.0297\n",
      "Epoch 019 Batch 012 Loss: 0.1078\n",
      "Epoch 019 Batch 013 Loss: 0.0148\n",
      "Epoch 019 Batch 014 Loss: 0.0101\n",
      "Epoch 019 Batch 015 Loss: 0.0349\n",
      "Epoch 019 Batch 016 Loss: 0.0552\n",
      "Epoch 019 Batch 017 Loss: 0.0770\n",
      "Epoch 019 Batch 018 Loss: 0.0310\n",
      "Epoch 019 Batch 019 Loss: 0.0111\n",
      "Epoch 019 Batch 020 Loss: 0.0445\n",
      "Epoch 019 Batch 021 Loss: 0.0194\n",
      "Epoch 019 Batch 022 Loss: 0.0376\n",
      "Epoch 019 Batch 023 Loss: 0.0263\n",
      "Epoch 019 Batch 024 Loss: 0.0238\n",
      "Epoch 019 Batch 025 Loss: 0.0095\n",
      "Epoch 019 Batch 026 Loss: 0.0674\n",
      "Epoch 019 Batch 027 Loss: 0.0421\n",
      "Epoch 019 Batch 028 Loss: 0.0783\n",
      "Epoch 019 Batch 029 Loss: 0.0535\n",
      "Epoch 019 Batch 030 Loss: 0.0338\n",
      "Epoch 019 Batch 031 Loss: 0.0309\n",
      "Epoch 019 Batch 032 Loss: 0.0094\n",
      "Epoch 019 Batch 033 Loss: 0.0632\n",
      "Epoch 019 Batch 034 Loss: 0.0205\n",
      "Epoch 019 Batch 035 Loss: 0.0094\n",
      "Epoch 019 Batch 036 Loss: 0.1126\n",
      "Epoch 019 Batch 037 Loss: 0.0102\n",
      "Epoch 019 Batch 038 Loss: 0.0119\n",
      "Epoch 019 Batch 039 Loss: 0.0895\n",
      "Epoch 019 Batch 040 Loss: 0.0427\n",
      "Epoch 019 Batch 041 Loss: 0.1019\n",
      "Epoch 019 Batch 042 Loss: 0.0160\n",
      "Epoch 019 Batch 043 Loss: 0.0916\n",
      "Epoch 019 Batch 044 Loss: 0.0026\n",
      "Epoch 019 Batch 045 Loss: 0.1115\n",
      "Epoch 019 Batch 046 Loss: 0.1298\n",
      "Epoch 019 Batch 047 Loss: 0.0629\n",
      "Epoch 019 Batch 048 Loss: 0.0501\n",
      "Epoch 019 Batch 049 Loss: 0.0117\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8357142806053162\n",
      "-----------------------*-----------------------\n",
      "Epoch 020 Batch 000 Loss: 0.0339\n",
      "Epoch 020 Batch 001 Loss: 0.0296\n",
      "Epoch 020 Batch 002 Loss: 0.0485\n",
      "Epoch 020 Batch 003 Loss: 0.0194\n",
      "Epoch 020 Batch 004 Loss: 0.0223\n",
      "Epoch 020 Batch 005 Loss: 0.0168\n",
      "Epoch 020 Batch 006 Loss: 0.0305\n",
      "Epoch 020 Batch 007 Loss: 0.0159\n",
      "Epoch 020 Batch 008 Loss: 0.0167\n",
      "Epoch 020 Batch 009 Loss: 0.0471\n",
      "Epoch 020 Batch 010 Loss: 0.0260\n",
      "Epoch 020 Batch 011 Loss: 0.0178\n",
      "Epoch 020 Batch 012 Loss: 0.0269\n",
      "Epoch 020 Batch 013 Loss: 0.0039\n",
      "Epoch 020 Batch 014 Loss: 0.0174\n",
      "Epoch 020 Batch 015 Loss: 0.0837\n",
      "Epoch 020 Batch 016 Loss: 0.0053\n",
      "Epoch 020 Batch 017 Loss: 0.0788\n",
      "Epoch 020 Batch 018 Loss: 0.0926\n",
      "Epoch 020 Batch 019 Loss: 0.0431\n",
      "Epoch 020 Batch 020 Loss: 0.0148\n",
      "Epoch 020 Batch 021 Loss: 0.0104\n",
      "Epoch 020 Batch 022 Loss: 0.0039\n",
      "Epoch 020 Batch 023 Loss: 0.0104\n",
      "Epoch 020 Batch 024 Loss: 0.0419\n",
      "Epoch 020 Batch 025 Loss: 0.0123\n",
      "Epoch 020 Batch 026 Loss: 0.0156\n",
      "Epoch 020 Batch 027 Loss: 0.0059\n",
      "Epoch 020 Batch 028 Loss: 0.0268\n",
      "Epoch 020 Batch 029 Loss: 0.0744\n",
      "Epoch 020 Batch 030 Loss: 0.0281\n",
      "Epoch 020 Batch 031 Loss: 0.1069\n",
      "Epoch 020 Batch 032 Loss: 0.0691\n",
      "Epoch 020 Batch 033 Loss: 0.0813\n",
      "Epoch 020 Batch 034 Loss: 0.0651\n",
      "Epoch 020 Batch 035 Loss: 0.0467\n",
      "Epoch 020 Batch 036 Loss: 0.0226\n",
      "Epoch 020 Batch 037 Loss: 0.0440\n",
      "Epoch 020 Batch 038 Loss: 0.0284\n",
      "Epoch 020 Batch 039 Loss: 0.0543\n",
      "Epoch 020 Batch 040 Loss: 0.2200\n",
      "Epoch 020 Batch 041 Loss: 0.2285\n",
      "Epoch 020 Batch 042 Loss: 0.0474\n",
      "Epoch 020 Batch 043 Loss: 0.1905\n",
      "Epoch 020 Batch 044 Loss: 0.0130\n",
      "Epoch 020 Batch 045 Loss: 0.0425\n",
      "Epoch 020 Batch 046 Loss: 0.0108\n",
      "Epoch 020 Batch 047 Loss: 0.1686\n",
      "Epoch 020 Batch 048 Loss: 0.0650\n",
      "Epoch 020 Batch 049 Loss: 0.0731\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8642857074737549\n",
      "-----------------------*-----------------------\n",
      "Epoch 021 Batch 000 Loss: 0.0690\n",
      "Epoch 021 Batch 001 Loss: 0.1404\n",
      "Epoch 021 Batch 002 Loss: 0.0164\n",
      "Epoch 021 Batch 003 Loss: 0.0228\n",
      "Epoch 021 Batch 004 Loss: 0.0114\n",
      "Epoch 021 Batch 005 Loss: 0.1097\n",
      "Epoch 021 Batch 006 Loss: 0.0402\n",
      "Epoch 021 Batch 007 Loss: 0.1145\n",
      "Epoch 021 Batch 008 Loss: 0.1374\n",
      "Epoch 021 Batch 009 Loss: 0.0214\n",
      "Epoch 021 Batch 010 Loss: 0.0464\n",
      "Epoch 021 Batch 011 Loss: 0.0561\n",
      "Epoch 021 Batch 012 Loss: 0.0446\n",
      "Epoch 021 Batch 013 Loss: 0.0444\n",
      "Epoch 021 Batch 014 Loss: 0.1438\n",
      "Epoch 021 Batch 015 Loss: 0.0396\n",
      "Epoch 021 Batch 016 Loss: 0.0569\n",
      "Epoch 021 Batch 017 Loss: 0.1404\n",
      "Epoch 021 Batch 018 Loss: 0.0158\n",
      "Epoch 021 Batch 019 Loss: 0.0891\n",
      "Epoch 021 Batch 020 Loss: 0.1484\n",
      "Epoch 021 Batch 021 Loss: 0.0902\n",
      "Epoch 021 Batch 022 Loss: 0.0073\n",
      "Epoch 021 Batch 023 Loss: 0.0175\n",
      "Epoch 021 Batch 024 Loss: 0.1043\n",
      "Epoch 021 Batch 025 Loss: 0.1023\n",
      "Epoch 021 Batch 026 Loss: 0.1979\n",
      "Epoch 021 Batch 027 Loss: 0.1399\n",
      "Epoch 021 Batch 028 Loss: 0.0214\n",
      "Epoch 021 Batch 029 Loss: 0.0231\n",
      "Epoch 021 Batch 030 Loss: 0.1672\n",
      "Epoch 021 Batch 031 Loss: 0.0613\n",
      "Epoch 021 Batch 032 Loss: 0.1432\n",
      "Epoch 021 Batch 033 Loss: 0.1042\n",
      "Epoch 021 Batch 034 Loss: 0.0868\n",
      "Epoch 021 Batch 035 Loss: 0.1179\n",
      "Epoch 021 Batch 036 Loss: 0.0827\n",
      "Epoch 021 Batch 037 Loss: 0.0269\n",
      "Epoch 021 Batch 038 Loss: 0.0882\n",
      "Epoch 021 Batch 039 Loss: 0.0404\n",
      "Epoch 021 Batch 040 Loss: 0.0497\n",
      "Epoch 021 Batch 041 Loss: 0.0683\n",
      "Epoch 021 Batch 042 Loss: 0.0352\n",
      "Epoch 021 Batch 043 Loss: 0.0372\n",
      "Epoch 021 Batch 044 Loss: 0.0444\n",
      "Epoch 021 Batch 045 Loss: 0.0895\n",
      "Epoch 021 Batch 046 Loss: 0.0379\n",
      "Epoch 021 Batch 047 Loss: 0.0151\n",
      "Epoch 021 Batch 048 Loss: 0.0479\n",
      "Epoch 021 Batch 049 Loss: 0.0075\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 022 Batch 000 Loss: 0.1072\n",
      "Epoch 022 Batch 001 Loss: 0.1628\n",
      "Epoch 022 Batch 002 Loss: 0.0027\n",
      "Epoch 022 Batch 003 Loss: 0.0294\n",
      "Epoch 022 Batch 004 Loss: 0.0062\n",
      "Epoch 022 Batch 005 Loss: 0.0145\n",
      "Epoch 022 Batch 006 Loss: 0.0208\n",
      "Epoch 022 Batch 007 Loss: 0.1672\n",
      "Epoch 022 Batch 008 Loss: 0.0161\n",
      "Epoch 022 Batch 009 Loss: 0.0262\n",
      "Epoch 022 Batch 010 Loss: 0.0424\n",
      "Epoch 022 Batch 011 Loss: 0.0336\n",
      "Epoch 022 Batch 012 Loss: 0.0162\n",
      "Epoch 022 Batch 013 Loss: 0.1896\n",
      "Epoch 022 Batch 014 Loss: 0.0644\n",
      "Epoch 022 Batch 015 Loss: 0.0034\n",
      "Epoch 022 Batch 016 Loss: 0.0176\n",
      "Epoch 022 Batch 017 Loss: 0.0043\n",
      "Epoch 022 Batch 018 Loss: 0.0090\n",
      "Epoch 022 Batch 019 Loss: 0.0907\n",
      "Epoch 022 Batch 020 Loss: 0.0126\n",
      "Epoch 022 Batch 021 Loss: 0.0437\n",
      "Epoch 022 Batch 022 Loss: 0.0413\n",
      "Epoch 022 Batch 023 Loss: 0.1350\n",
      "Epoch 022 Batch 024 Loss: 0.0117\n",
      "Epoch 022 Batch 025 Loss: 0.0585\n",
      "Epoch 022 Batch 026 Loss: 0.0494\n",
      "Epoch 022 Batch 027 Loss: 0.0514\n",
      "Epoch 022 Batch 028 Loss: 0.0430\n",
      "Epoch 022 Batch 029 Loss: 0.1287\n",
      "Epoch 022 Batch 030 Loss: 0.0656\n",
      "Epoch 022 Batch 031 Loss: 0.0280\n",
      "Epoch 022 Batch 032 Loss: 0.0306\n",
      "Epoch 022 Batch 033 Loss: 0.1535\n",
      "Epoch 022 Batch 034 Loss: 0.0558\n",
      "Epoch 022 Batch 035 Loss: 0.0092\n",
      "Epoch 022 Batch 036 Loss: 0.0219\n",
      "Epoch 022 Batch 037 Loss: 0.0173\n",
      "Epoch 022 Batch 038 Loss: 0.0359\n",
      "Epoch 022 Batch 039 Loss: 0.2163\n",
      "Epoch 022 Batch 040 Loss: 0.0786\n",
      "Epoch 022 Batch 041 Loss: 0.0336\n",
      "Epoch 022 Batch 042 Loss: 0.0410\n",
      "Epoch 022 Batch 043 Loss: 0.0710\n",
      "Epoch 022 Batch 044 Loss: 0.0231\n",
      "Epoch 022 Batch 045 Loss: 0.0297\n",
      "Epoch 022 Batch 046 Loss: 0.0466\n",
      "Epoch 022 Batch 047 Loss: 0.0281\n",
      "Epoch 022 Batch 048 Loss: 0.1008\n",
      "Epoch 022 Batch 049 Loss: 0.1585\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 023 Batch 000 Loss: 0.0495\n",
      "Epoch 023 Batch 001 Loss: 0.0344\n",
      "Epoch 023 Batch 002 Loss: 0.1878\n",
      "Epoch 023 Batch 003 Loss: 0.0066\n",
      "Epoch 023 Batch 004 Loss: 0.1822\n",
      "Epoch 023 Batch 005 Loss: 0.0810\n",
      "Epoch 023 Batch 006 Loss: 0.1073\n",
      "Epoch 023 Batch 007 Loss: 0.0327\n",
      "Epoch 023 Batch 008 Loss: 0.1367\n",
      "Epoch 023 Batch 009 Loss: 0.0264\n",
      "Epoch 023 Batch 010 Loss: 0.0151\n",
      "Epoch 023 Batch 011 Loss: 0.1382\n",
      "Epoch 023 Batch 012 Loss: 0.0731\n",
      "Epoch 023 Batch 013 Loss: 0.0155\n",
      "Epoch 023 Batch 014 Loss: 0.0360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 Batch 015 Loss: 0.0352\n",
      "Epoch 023 Batch 016 Loss: 0.0312\n",
      "Epoch 023 Batch 017 Loss: 0.0634\n",
      "Epoch 023 Batch 018 Loss: 0.0723\n",
      "Epoch 023 Batch 019 Loss: 0.1722\n",
      "Epoch 023 Batch 020 Loss: 0.0209\n",
      "Epoch 023 Batch 021 Loss: 0.0252\n",
      "Epoch 023 Batch 022 Loss: 0.0208\n",
      "Epoch 023 Batch 023 Loss: 0.0291\n",
      "Epoch 023 Batch 024 Loss: 0.0217\n",
      "Epoch 023 Batch 025 Loss: 0.0888\n",
      "Epoch 023 Batch 026 Loss: 0.0084\n",
      "Epoch 023 Batch 027 Loss: 0.0436\n",
      "Epoch 023 Batch 028 Loss: 0.0627\n",
      "Epoch 023 Batch 029 Loss: 0.1612\n",
      "Epoch 023 Batch 030 Loss: 0.1800\n",
      "Epoch 023 Batch 031 Loss: 0.0633\n",
      "Epoch 023 Batch 032 Loss: 0.0420\n",
      "Epoch 023 Batch 033 Loss: 0.0525\n",
      "Epoch 023 Batch 034 Loss: 0.0493\n",
      "Epoch 023 Batch 035 Loss: 0.0380\n",
      "Epoch 023 Batch 036 Loss: 0.0494\n",
      "Epoch 023 Batch 037 Loss: 0.0141\n",
      "Epoch 023 Batch 038 Loss: 0.0844\n",
      "Epoch 023 Batch 039 Loss: 0.0227\n",
      "Epoch 023 Batch 040 Loss: 0.1236\n",
      "Epoch 023 Batch 041 Loss: 0.0119\n",
      "Epoch 023 Batch 042 Loss: 0.0543\n",
      "Epoch 023 Batch 043 Loss: 0.0834\n",
      "Epoch 023 Batch 044 Loss: 0.0240\n",
      "Epoch 023 Batch 045 Loss: 0.1298\n",
      "Epoch 023 Batch 046 Loss: 0.0055\n",
      "Epoch 023 Batch 047 Loss: 0.0179\n",
      "Epoch 023 Batch 048 Loss: 0.0199\n",
      "Epoch 023 Batch 049 Loss: 0.0043\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8214285969734192\n",
      "-----------------------*-----------------------\n",
      "Epoch 024 Batch 000 Loss: 0.0135\n",
      "Epoch 024 Batch 001 Loss: 0.0328\n",
      "Epoch 024 Batch 002 Loss: 0.0097\n",
      "Epoch 024 Batch 003 Loss: 0.0112\n",
      "Epoch 024 Batch 004 Loss: 0.1078\n",
      "Epoch 024 Batch 005 Loss: 0.0096\n",
      "Epoch 024 Batch 006 Loss: 0.0036\n",
      "Epoch 024 Batch 007 Loss: 0.0694\n",
      "Epoch 024 Batch 008 Loss: 0.0081\n",
      "Epoch 024 Batch 009 Loss: 0.0360\n",
      "Epoch 024 Batch 010 Loss: 0.0495\n",
      "Epoch 024 Batch 011 Loss: 0.1158\n",
      "Epoch 024 Batch 012 Loss: 0.0178\n",
      "Epoch 024 Batch 013 Loss: 0.0076\n",
      "Epoch 024 Batch 014 Loss: 0.1928\n",
      "Epoch 024 Batch 015 Loss: 0.0556\n",
      "Epoch 024 Batch 016 Loss: 0.0559\n",
      "Epoch 024 Batch 017 Loss: 0.0524\n",
      "Epoch 024 Batch 018 Loss: 0.0394\n",
      "Epoch 024 Batch 019 Loss: 0.0123\n",
      "Epoch 024 Batch 020 Loss: 0.0293\n",
      "Epoch 024 Batch 021 Loss: 0.0247\n",
      "Epoch 024 Batch 022 Loss: 0.0184\n",
      "Epoch 024 Batch 023 Loss: 0.0216\n",
      "Epoch 024 Batch 024 Loss: 0.0220\n",
      "Epoch 024 Batch 025 Loss: 0.0593\n",
      "Epoch 024 Batch 026 Loss: 0.0102\n",
      "Epoch 024 Batch 027 Loss: 0.0072\n",
      "Epoch 024 Batch 028 Loss: 0.0068\n",
      "Epoch 024 Batch 029 Loss: 0.0230\n",
      "Epoch 024 Batch 030 Loss: 0.0754\n",
      "Epoch 024 Batch 031 Loss: 0.0181\n",
      "Epoch 024 Batch 032 Loss: 0.0043\n",
      "Epoch 024 Batch 033 Loss: 0.0071\n",
      "Epoch 024 Batch 034 Loss: 0.0431\n",
      "Epoch 024 Batch 035 Loss: 0.0153\n",
      "Epoch 024 Batch 036 Loss: 0.0554\n",
      "Epoch 024 Batch 037 Loss: 0.0511\n",
      "Epoch 024 Batch 038 Loss: 0.0064\n",
      "Epoch 024 Batch 039 Loss: 0.0196\n",
      "Epoch 024 Batch 040 Loss: 0.0609\n",
      "Epoch 024 Batch 041 Loss: 0.0050\n",
      "Epoch 024 Batch 042 Loss: 0.0049\n",
      "Epoch 024 Batch 043 Loss: 0.0046\n",
      "Epoch 024 Batch 044 Loss: 0.0036\n",
      "Epoch 024 Batch 045 Loss: 0.0125\n",
      "Epoch 024 Batch 046 Loss: 0.0464\n",
      "Epoch 024 Batch 047 Loss: 0.0081\n",
      "Epoch 024 Batch 048 Loss: 0.0387\n",
      "Epoch 024 Batch 049 Loss: 0.0270\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.7928571701049805\n",
      "-----------------------*-----------------------\n",
      "Epoch 025 Batch 000 Loss: 0.0219\n",
      "Epoch 025 Batch 001 Loss: 0.0433\n",
      "Epoch 025 Batch 002 Loss: 0.0069\n",
      "Epoch 025 Batch 003 Loss: 0.0195\n",
      "Epoch 025 Batch 004 Loss: 0.0126\n",
      "Epoch 025 Batch 005 Loss: 0.0450\n",
      "Epoch 025 Batch 006 Loss: 0.0916\n",
      "Epoch 025 Batch 007 Loss: 0.0869\n",
      "Epoch 025 Batch 008 Loss: 0.0700\n",
      "Epoch 025 Batch 009 Loss: 0.0334\n",
      "Epoch 025 Batch 010 Loss: 0.1333\n",
      "Epoch 025 Batch 011 Loss: 0.2634\n",
      "Epoch 025 Batch 012 Loss: 0.0124\n",
      "Epoch 025 Batch 013 Loss: 0.0992\n",
      "Epoch 025 Batch 014 Loss: 0.0276\n",
      "Epoch 025 Batch 015 Loss: 0.3038\n",
      "Epoch 025 Batch 016 Loss: 0.0951\n",
      "Epoch 025 Batch 017 Loss: 0.0332\n",
      "Epoch 025 Batch 018 Loss: 0.0456\n",
      "Epoch 025 Batch 019 Loss: 0.1272\n",
      "Epoch 025 Batch 020 Loss: 0.1569\n",
      "Epoch 025 Batch 021 Loss: 0.0144\n",
      "Epoch 025 Batch 022 Loss: 0.0405\n",
      "Epoch 025 Batch 023 Loss: 0.0576\n",
      "Epoch 025 Batch 024 Loss: 0.0115\n",
      "Epoch 025 Batch 025 Loss: 0.0528\n",
      "Epoch 025 Batch 026 Loss: 0.3945\n",
      "Epoch 025 Batch 027 Loss: 0.0294\n",
      "Epoch 025 Batch 028 Loss: 0.0445\n",
      "Epoch 025 Batch 029 Loss: 0.3015\n",
      "Epoch 025 Batch 030 Loss: 0.0238\n",
      "Epoch 025 Batch 031 Loss: 0.0969\n",
      "Epoch 025 Batch 032 Loss: 0.0438\n",
      "Epoch 025 Batch 033 Loss: 0.1143\n",
      "Epoch 025 Batch 034 Loss: 0.2265\n",
      "Epoch 025 Batch 035 Loss: 0.0332\n",
      "Epoch 025 Batch 036 Loss: 0.0387\n",
      "Epoch 025 Batch 037 Loss: 0.0724\n",
      "Epoch 025 Batch 038 Loss: 0.1358\n",
      "Epoch 025 Batch 039 Loss: 0.1344\n",
      "Epoch 025 Batch 040 Loss: 0.3888\n",
      "Epoch 025 Batch 041 Loss: 0.0672\n",
      "Epoch 025 Batch 042 Loss: 0.0551\n",
      "Epoch 025 Batch 043 Loss: 0.0116\n",
      "Epoch 025 Batch 044 Loss: 0.0465\n",
      "Epoch 025 Batch 045 Loss: 0.0335\n",
      "Epoch 025 Batch 046 Loss: 0.0816\n",
      "Epoch 025 Batch 047 Loss: 0.1850\n",
      "Epoch 025 Batch 048 Loss: 0.0483\n",
      "Epoch 025 Batch 049 Loss: 0.1539\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8357142806053162\n",
      "-----------------------*-----------------------\n",
      "Epoch 026 Batch 000 Loss: 0.0447\n",
      "Epoch 026 Batch 001 Loss: 0.1752\n",
      "Epoch 026 Batch 002 Loss: 0.0918\n",
      "Epoch 026 Batch 003 Loss: 0.0617\n",
      "Epoch 026 Batch 004 Loss: 0.2159\n",
      "Epoch 026 Batch 005 Loss: 0.0327\n",
      "Epoch 026 Batch 006 Loss: 0.0555\n",
      "Epoch 026 Batch 007 Loss: 0.0305\n",
      "Epoch 026 Batch 008 Loss: 0.0378\n",
      "Epoch 026 Batch 009 Loss: 0.1278\n",
      "Epoch 026 Batch 010 Loss: 0.1703\n",
      "Epoch 026 Batch 011 Loss: 0.0971\n",
      "Epoch 026 Batch 012 Loss: 0.0200\n",
      "Epoch 026 Batch 013 Loss: 0.0481\n",
      "Epoch 026 Batch 014 Loss: 0.0491\n",
      "Epoch 026 Batch 015 Loss: 0.1169\n",
      "Epoch 026 Batch 016 Loss: 0.1745\n",
      "Epoch 026 Batch 017 Loss: 0.1159\n",
      "Epoch 026 Batch 018 Loss: 0.0526\n",
      "Epoch 026 Batch 019 Loss: 0.0178\n",
      "Epoch 026 Batch 020 Loss: 0.0880\n",
      "Epoch 026 Batch 021 Loss: 0.0078\n",
      "Epoch 026 Batch 022 Loss: 0.0377\n",
      "Epoch 026 Batch 023 Loss: 0.1029\n",
      "Epoch 026 Batch 024 Loss: 0.0769\n",
      "Epoch 026 Batch 025 Loss: 0.0177\n",
      "Epoch 026 Batch 026 Loss: 0.0324\n",
      "Epoch 026 Batch 027 Loss: 0.0202\n",
      "Epoch 026 Batch 028 Loss: 0.1110\n",
      "Epoch 026 Batch 029 Loss: 0.0336\n",
      "Epoch 026 Batch 030 Loss: 0.0432\n",
      "Epoch 026 Batch 031 Loss: 0.1839\n",
      "Epoch 026 Batch 032 Loss: 0.0134\n",
      "Epoch 026 Batch 033 Loss: 0.2125\n",
      "Epoch 026 Batch 034 Loss: 0.1335\n",
      "Epoch 026 Batch 035 Loss: 0.0423\n",
      "Epoch 026 Batch 036 Loss: 0.1581\n",
      "Epoch 026 Batch 037 Loss: 0.0298\n",
      "Epoch 026 Batch 038 Loss: 0.1240\n",
      "Epoch 026 Batch 039 Loss: 0.0556\n",
      "Epoch 026 Batch 040 Loss: 0.0264\n",
      "Epoch 026 Batch 041 Loss: 0.0536\n",
      "Epoch 026 Batch 042 Loss: 0.0495\n",
      "Epoch 026 Batch 043 Loss: 0.0176\n",
      "Epoch 026 Batch 044 Loss: 0.1006\n",
      "Epoch 026 Batch 045 Loss: 0.1653\n",
      "Epoch 026 Batch 046 Loss: 0.0851\n",
      "Epoch 026 Batch 047 Loss: 0.0141\n",
      "Epoch 026 Batch 048 Loss: 0.0275\n",
      "Epoch 026 Batch 049 Loss: 0.0318\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 027 Batch 000 Loss: 0.0287\n",
      "Epoch 027 Batch 001 Loss: 0.1719\n",
      "Epoch 027 Batch 002 Loss: 0.0355\n",
      "Epoch 027 Batch 003 Loss: 0.0226\n",
      "Epoch 027 Batch 004 Loss: 0.0902\n",
      "Epoch 027 Batch 005 Loss: 0.0485\n",
      "Epoch 027 Batch 006 Loss: 0.0193\n",
      "Epoch 027 Batch 007 Loss: 0.0115\n",
      "Epoch 027 Batch 008 Loss: 0.0119\n",
      "Epoch 027 Batch 009 Loss: 0.0921\n",
      "Epoch 027 Batch 010 Loss: 0.0713\n",
      "Epoch 027 Batch 011 Loss: 0.0520\n",
      "Epoch 027 Batch 012 Loss: 0.0624\n",
      "Epoch 027 Batch 013 Loss: 0.0493\n",
      "Epoch 027 Batch 014 Loss: 0.0281\n",
      "Epoch 027 Batch 015 Loss: 0.0566\n",
      "Epoch 027 Batch 016 Loss: 0.0113\n",
      "Epoch 027 Batch 017 Loss: 0.0111\n",
      "Epoch 027 Batch 018 Loss: 0.0080\n",
      "Epoch 027 Batch 019 Loss: 0.0108\n",
      "Epoch 027 Batch 020 Loss: 0.0082\n",
      "Epoch 027 Batch 021 Loss: 0.0187\n",
      "Epoch 027 Batch 022 Loss: 0.0114\n",
      "Epoch 027 Batch 023 Loss: 0.0151\n",
      "Epoch 027 Batch 024 Loss: 0.0776\n",
      "Epoch 027 Batch 025 Loss: 0.0227\n",
      "Epoch 027 Batch 026 Loss: 0.0112\n",
      "Epoch 027 Batch 027 Loss: 0.0237\n",
      "Epoch 027 Batch 028 Loss: 0.0421\n",
      "Epoch 027 Batch 029 Loss: 0.0186\n",
      "Epoch 027 Batch 030 Loss: 0.0127\n",
      "Epoch 027 Batch 031 Loss: 0.1204\n",
      "Epoch 027 Batch 032 Loss: 0.0079\n",
      "Epoch 027 Batch 033 Loss: 0.0117\n",
      "Epoch 027 Batch 034 Loss: 0.0298\n",
      "Epoch 027 Batch 035 Loss: 0.0131\n",
      "Epoch 027 Batch 036 Loss: 0.0424\n",
      "Epoch 027 Batch 037 Loss: 0.0198\n",
      "Epoch 027 Batch 038 Loss: 0.0233\n",
      "Epoch 027 Batch 039 Loss: 0.0391\n",
      "Epoch 027 Batch 040 Loss: 0.0030\n",
      "Epoch 027 Batch 041 Loss: 0.0513\n",
      "Epoch 027 Batch 042 Loss: 0.1196\n",
      "Epoch 027 Batch 043 Loss: 0.0167\n",
      "Epoch 027 Batch 044 Loss: 0.0180\n",
      "Epoch 027 Batch 045 Loss: 0.0319\n",
      "Epoch 027 Batch 046 Loss: 0.0275\n",
      "Epoch 027 Batch 047 Loss: 0.1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 Batch 048 Loss: 0.0607\n",
      "Epoch 027 Batch 049 Loss: 0.0061\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 028 Batch 000 Loss: 0.0181\n",
      "Epoch 028 Batch 001 Loss: 0.1414\n",
      "Epoch 028 Batch 002 Loss: 0.0046\n",
      "Epoch 028 Batch 003 Loss: 0.2297\n",
      "Epoch 028 Batch 004 Loss: 0.1184\n",
      "Epoch 028 Batch 005 Loss: 0.0796\n",
      "Epoch 028 Batch 006 Loss: 0.1256\n",
      "Epoch 028 Batch 007 Loss: 0.0676\n",
      "Epoch 028 Batch 008 Loss: 0.0344\n",
      "Epoch 028 Batch 009 Loss: 0.2153\n",
      "Epoch 028 Batch 010 Loss: 0.0685\n",
      "Epoch 028 Batch 011 Loss: 0.0260\n",
      "Epoch 028 Batch 012 Loss: 0.0370\n",
      "Epoch 028 Batch 013 Loss: 0.0788\n",
      "Epoch 028 Batch 014 Loss: 0.0179\n",
      "Epoch 028 Batch 015 Loss: 0.0147\n",
      "Epoch 028 Batch 016 Loss: 0.0644\n",
      "Epoch 028 Batch 017 Loss: 0.1144\n",
      "Epoch 028 Batch 018 Loss: 0.0402\n",
      "Epoch 028 Batch 019 Loss: 0.0261\n",
      "Epoch 028 Batch 020 Loss: 0.0677\n",
      "Epoch 028 Batch 021 Loss: 0.0260\n",
      "Epoch 028 Batch 022 Loss: 0.0618\n",
      "Epoch 028 Batch 023 Loss: 0.0619\n",
      "Epoch 028 Batch 024 Loss: 0.0194\n",
      "Epoch 028 Batch 025 Loss: 0.0099\n",
      "Epoch 028 Batch 026 Loss: 0.0140\n",
      "Epoch 028 Batch 027 Loss: 0.0617\n",
      "Epoch 028 Batch 028 Loss: 0.0084\n",
      "Epoch 028 Batch 029 Loss: 0.3250\n",
      "Epoch 028 Batch 030 Loss: 0.0249\n",
      "Epoch 028 Batch 031 Loss: 0.0047\n",
      "Epoch 028 Batch 032 Loss: 0.0365\n",
      "Epoch 028 Batch 033 Loss: 0.0331\n",
      "Epoch 028 Batch 034 Loss: 0.0359\n",
      "Epoch 028 Batch 035 Loss: 0.0095\n",
      "Epoch 028 Batch 036 Loss: 0.0730\n",
      "Epoch 028 Batch 037 Loss: 0.0235\n",
      "Epoch 028 Batch 038 Loss: 0.0116\n",
      "Epoch 028 Batch 039 Loss: 0.1181\n",
      "Epoch 028 Batch 040 Loss: 0.1472\n",
      "Epoch 028 Batch 041 Loss: 0.0500\n",
      "Epoch 028 Batch 042 Loss: 0.0972\n",
      "Epoch 028 Batch 043 Loss: 0.1380\n",
      "Epoch 028 Batch 044 Loss: 0.0683\n",
      "Epoch 028 Batch 045 Loss: 0.0062\n",
      "Epoch 028 Batch 046 Loss: 0.0935\n",
      "Epoch 028 Batch 047 Loss: 0.0416\n",
      "Epoch 028 Batch 048 Loss: 0.1040\n",
      "Epoch 028 Batch 049 Loss: 0.0532\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 029 Batch 000 Loss: 0.0134\n",
      "Epoch 029 Batch 001 Loss: 0.0502\n",
      "Epoch 029 Batch 002 Loss: 0.0635\n",
      "Epoch 029 Batch 003 Loss: 0.0320\n",
      "Epoch 029 Batch 004 Loss: 0.0350\n",
      "Epoch 029 Batch 005 Loss: 0.0232\n",
      "Epoch 029 Batch 006 Loss: 0.1165\n",
      "Epoch 029 Batch 007 Loss: 0.1899\n",
      "Epoch 029 Batch 008 Loss: 0.0231\n",
      "Epoch 029 Batch 009 Loss: 0.0942\n",
      "Epoch 029 Batch 010 Loss: 0.1290\n",
      "Epoch 029 Batch 011 Loss: 0.0218\n",
      "Epoch 029 Batch 012 Loss: 0.1645\n",
      "Epoch 029 Batch 013 Loss: 0.1134\n",
      "Epoch 029 Batch 014 Loss: 0.0669\n",
      "Epoch 029 Batch 015 Loss: 0.0350\n",
      "Epoch 029 Batch 016 Loss: 0.0276\n",
      "Epoch 029 Batch 017 Loss: 0.0710\n",
      "Epoch 029 Batch 018 Loss: 0.0615\n",
      "Epoch 029 Batch 019 Loss: 0.0842\n",
      "Epoch 029 Batch 020 Loss: 0.1189\n",
      "Epoch 029 Batch 021 Loss: 0.1154\n",
      "Epoch 029 Batch 022 Loss: 0.1023\n",
      "Epoch 029 Batch 023 Loss: 0.0137\n",
      "Epoch 029 Batch 024 Loss: 0.0309\n",
      "Epoch 029 Batch 025 Loss: 0.2897\n",
      "Epoch 029 Batch 026 Loss: 0.1265\n",
      "Epoch 029 Batch 027 Loss: 0.0143\n",
      "Epoch 029 Batch 028 Loss: 0.0469\n",
      "Epoch 029 Batch 029 Loss: 0.0767\n",
      "Epoch 029 Batch 030 Loss: 0.0132\n",
      "Epoch 029 Batch 031 Loss: 0.0401\n",
      "Epoch 029 Batch 032 Loss: 0.0431\n",
      "Epoch 029 Batch 033 Loss: 0.0229\n",
      "Epoch 029 Batch 034 Loss: 0.0502\n",
      "Epoch 029 Batch 035 Loss: 0.0601\n",
      "Epoch 029 Batch 036 Loss: 0.1282\n",
      "Epoch 029 Batch 037 Loss: 0.2533\n",
      "Epoch 029 Batch 038 Loss: 0.0344\n",
      "Epoch 029 Batch 039 Loss: 0.1219\n",
      "Epoch 029 Batch 040 Loss: 0.0647\n",
      "Epoch 029 Batch 041 Loss: 0.0700\n",
      "Epoch 029 Batch 042 Loss: 0.0829\n",
      "Epoch 029 Batch 043 Loss: 0.0459\n",
      "Epoch 029 Batch 044 Loss: 0.0510\n",
      "Epoch 029 Batch 045 Loss: 0.0245\n",
      "Epoch 029 Batch 046 Loss: 0.0697\n",
      "Epoch 029 Batch 047 Loss: 0.2805\n",
      "Epoch 029 Batch 048 Loss: 0.0710\n",
      "Epoch 029 Batch 049 Loss: 0.0123\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 030 Batch 000 Loss: 0.0766\n",
      "Epoch 030 Batch 001 Loss: 0.1037\n",
      "Epoch 030 Batch 002 Loss: 0.0170\n",
      "Epoch 030 Batch 003 Loss: 0.0097\n",
      "Epoch 030 Batch 004 Loss: 0.0626\n",
      "Epoch 030 Batch 005 Loss: 0.1548\n",
      "Epoch 030 Batch 006 Loss: 0.0270\n",
      "Epoch 030 Batch 007 Loss: 0.0236\n",
      "Epoch 030 Batch 008 Loss: 0.0208\n",
      "Epoch 030 Batch 009 Loss: 0.0236\n",
      "Epoch 030 Batch 010 Loss: 0.0073\n",
      "Epoch 030 Batch 011 Loss: 0.0260\n",
      "Epoch 030 Batch 012 Loss: 0.0179\n",
      "Epoch 030 Batch 013 Loss: 0.1521\n",
      "Epoch 030 Batch 014 Loss: 0.0113\n",
      "Epoch 030 Batch 015 Loss: 0.1018\n",
      "Epoch 030 Batch 016 Loss: 0.0314\n",
      "Epoch 030 Batch 017 Loss: 0.0628\n",
      "Epoch 030 Batch 018 Loss: 0.0548\n",
      "Epoch 030 Batch 019 Loss: 0.1523\n",
      "Epoch 030 Batch 020 Loss: 0.1541\n",
      "Epoch 030 Batch 021 Loss: 0.0100\n",
      "Epoch 030 Batch 022 Loss: 0.0510\n",
      "Epoch 030 Batch 023 Loss: 0.0396\n",
      "Epoch 030 Batch 024 Loss: 0.0190\n",
      "Epoch 030 Batch 025 Loss: 0.0743\n",
      "Epoch 030 Batch 026 Loss: 0.0774\n",
      "Epoch 030 Batch 027 Loss: 0.0672\n",
      "Epoch 030 Batch 028 Loss: 0.0360\n",
      "Epoch 030 Batch 029 Loss: 0.0384\n",
      "Epoch 030 Batch 030 Loss: 0.1065\n",
      "Epoch 030 Batch 031 Loss: 0.0702\n",
      "Epoch 030 Batch 032 Loss: 0.0264\n",
      "Epoch 030 Batch 033 Loss: 0.0174\n",
      "Epoch 030 Batch 034 Loss: 0.0595\n",
      "Epoch 030 Batch 035 Loss: 0.1893\n",
      "Epoch 030 Batch 036 Loss: 0.0045\n",
      "Epoch 030 Batch 037 Loss: 0.0623\n",
      "Epoch 030 Batch 038 Loss: 0.0487\n",
      "Epoch 030 Batch 039 Loss: 0.2043\n",
      "Epoch 030 Batch 040 Loss: 0.0395\n",
      "Epoch 030 Batch 041 Loss: 0.0918\n",
      "Epoch 030 Batch 042 Loss: 0.1057\n",
      "Epoch 030 Batch 043 Loss: 0.0422\n",
      "Epoch 030 Batch 044 Loss: 0.1280\n",
      "Epoch 030 Batch 045 Loss: 0.0157\n",
      "Epoch 030 Batch 046 Loss: 0.2075\n",
      "Epoch 030 Batch 047 Loss: 0.0176\n",
      "Epoch 030 Batch 048 Loss: 0.0270\n",
      "Epoch 030 Batch 049 Loss: 0.1525\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 031 Batch 000 Loss: 0.0296\n",
      "Epoch 031 Batch 001 Loss: 0.0609\n",
      "Epoch 031 Batch 002 Loss: 0.0895\n",
      "Epoch 031 Batch 003 Loss: 0.0661\n",
      "Epoch 031 Batch 004 Loss: 0.0238\n",
      "Epoch 031 Batch 005 Loss: 0.0643\n",
      "Epoch 031 Batch 006 Loss: 0.0750\n",
      "Epoch 031 Batch 007 Loss: 0.1936\n",
      "Epoch 031 Batch 008 Loss: 0.0629\n",
      "Epoch 031 Batch 009 Loss: 0.0400\n",
      "Epoch 031 Batch 010 Loss: 0.0151\n",
      "Epoch 031 Batch 011 Loss: 0.2168\n",
      "Epoch 031 Batch 012 Loss: 0.1195\n",
      "Epoch 031 Batch 013 Loss: 0.1499\n",
      "Epoch 031 Batch 014 Loss: 0.0447\n",
      "Epoch 031 Batch 015 Loss: 0.0555\n",
      "Epoch 031 Batch 016 Loss: 0.0842\n",
      "Epoch 031 Batch 017 Loss: 0.0490\n",
      "Epoch 031 Batch 018 Loss: 0.0435\n",
      "Epoch 031 Batch 019 Loss: 0.2345\n",
      "Epoch 031 Batch 020 Loss: 0.0869\n",
      "Epoch 031 Batch 021 Loss: 0.0855\n",
      "Epoch 031 Batch 022 Loss: 0.0478\n",
      "Epoch 031 Batch 023 Loss: 0.0868\n",
      "Epoch 031 Batch 024 Loss: 0.1258\n",
      "Epoch 031 Batch 025 Loss: 0.0189\n",
      "Epoch 031 Batch 026 Loss: 0.0297\n",
      "Epoch 031 Batch 027 Loss: 0.0068\n",
      "Epoch 031 Batch 028 Loss: 0.0602\n",
      "Epoch 031 Batch 029 Loss: 0.0290\n",
      "Epoch 031 Batch 030 Loss: 0.0894\n",
      "Epoch 031 Batch 031 Loss: 0.0307\n",
      "Epoch 031 Batch 032 Loss: 0.0254\n",
      "Epoch 031 Batch 033 Loss: 0.0096\n",
      "Epoch 031 Batch 034 Loss: 0.0477\n",
      "Epoch 031 Batch 035 Loss: 0.0287\n",
      "Epoch 031 Batch 036 Loss: 0.0104\n",
      "Epoch 031 Batch 037 Loss: 0.0125\n",
      "Epoch 031 Batch 038 Loss: 0.0023\n",
      "Epoch 031 Batch 039 Loss: 0.1164\n",
      "Epoch 031 Batch 040 Loss: 0.0153\n",
      "Epoch 031 Batch 041 Loss: 0.0059\n",
      "Epoch 031 Batch 042 Loss: 0.1812\n",
      "Epoch 031 Batch 043 Loss: 0.0877\n",
      "Epoch 031 Batch 044 Loss: 0.0117\n",
      "Epoch 031 Batch 045 Loss: 0.0392\n",
      "Epoch 031 Batch 046 Loss: 0.0296\n",
      "Epoch 031 Batch 047 Loss: 0.0321\n",
      "Epoch 031 Batch 048 Loss: 0.0964\n",
      "Epoch 031 Batch 049 Loss: 0.0096\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.7928571701049805\n",
      "-----------------------*-----------------------\n",
      "Epoch 032 Batch 000 Loss: 0.1047\n",
      "Epoch 032 Batch 001 Loss: 0.0662\n",
      "Epoch 032 Batch 002 Loss: 0.0287\n",
      "Epoch 032 Batch 003 Loss: 0.0766\n",
      "Epoch 032 Batch 004 Loss: 0.0262\n",
      "Epoch 032 Batch 005 Loss: 0.0100\n",
      "Epoch 032 Batch 006 Loss: 0.0098\n",
      "Epoch 032 Batch 007 Loss: 0.0138\n",
      "Epoch 032 Batch 008 Loss: 0.0519\n",
      "Epoch 032 Batch 009 Loss: 0.0149\n",
      "Epoch 032 Batch 010 Loss: 0.0483\n",
      "Epoch 032 Batch 011 Loss: 0.0838\n",
      "Epoch 032 Batch 012 Loss: 0.0271\n",
      "Epoch 032 Batch 013 Loss: 0.0766\n",
      "Epoch 032 Batch 014 Loss: 0.0352\n",
      "Epoch 032 Batch 015 Loss: 0.0702\n",
      "Epoch 032 Batch 016 Loss: 0.0301\n",
      "Epoch 032 Batch 017 Loss: 0.0101\n",
      "Epoch 032 Batch 018 Loss: 0.0205\n",
      "Epoch 032 Batch 019 Loss: 0.0112\n",
      "Epoch 032 Batch 020 Loss: 0.0073\n",
      "Epoch 032 Batch 021 Loss: 0.1052\n",
      "Epoch 032 Batch 022 Loss: 0.1634\n",
      "Epoch 032 Batch 023 Loss: 0.0133\n",
      "Epoch 032 Batch 024 Loss: 0.0757\n",
      "Epoch 032 Batch 025 Loss: 0.0825\n",
      "Epoch 032 Batch 026 Loss: 0.0876\n",
      "Epoch 032 Batch 027 Loss: 0.0107\n",
      "Epoch 032 Batch 028 Loss: 0.0646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 Batch 029 Loss: 0.0173\n",
      "Epoch 032 Batch 030 Loss: 0.0214\n",
      "Epoch 032 Batch 031 Loss: 0.0713\n",
      "Epoch 032 Batch 032 Loss: 0.0569\n",
      "Epoch 032 Batch 033 Loss: 0.0171\n",
      "Epoch 032 Batch 034 Loss: 0.3462\n",
      "Epoch 032 Batch 035 Loss: 0.1059\n",
      "Epoch 032 Batch 036 Loss: 0.0546\n",
      "Epoch 032 Batch 037 Loss: 0.0139\n",
      "Epoch 032 Batch 038 Loss: 0.0763\n",
      "Epoch 032 Batch 039 Loss: 0.0578\n",
      "Epoch 032 Batch 040 Loss: 0.0698\n",
      "Epoch 032 Batch 041 Loss: 0.0097\n",
      "Epoch 032 Batch 042 Loss: 0.1255\n",
      "Epoch 032 Batch 043 Loss: 0.0174\n",
      "Epoch 032 Batch 044 Loss: 0.0399\n",
      "Epoch 032 Batch 045 Loss: 0.0192\n",
      "Epoch 032 Batch 046 Loss: 0.0392\n",
      "Epoch 032 Batch 047 Loss: 0.0773\n",
      "Epoch 032 Batch 048 Loss: 0.0288\n",
      "Epoch 032 Batch 049 Loss: 0.0435\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8500000238418579\n",
      "-----------------------*-----------------------\n",
      "Epoch 033 Batch 000 Loss: 0.1281\n",
      "Epoch 033 Batch 001 Loss: 0.1283\n",
      "Epoch 033 Batch 002 Loss: 0.0313\n",
      "Epoch 033 Batch 003 Loss: 0.0860\n",
      "Epoch 033 Batch 004 Loss: 0.0648\n",
      "Epoch 033 Batch 005 Loss: 0.1037\n",
      "Epoch 033 Batch 006 Loss: 0.0126\n",
      "Epoch 033 Batch 007 Loss: 0.0415\n",
      "Epoch 033 Batch 008 Loss: 0.0420\n",
      "Epoch 033 Batch 009 Loss: 0.0859\n",
      "Epoch 033 Batch 010 Loss: 0.0441\n",
      "Epoch 033 Batch 011 Loss: 0.0482\n",
      "Epoch 033 Batch 012 Loss: 0.0524\n",
      "Epoch 033 Batch 013 Loss: 0.0795\n",
      "Epoch 033 Batch 014 Loss: 0.1190\n",
      "Epoch 033 Batch 015 Loss: 0.0651\n",
      "Epoch 033 Batch 016 Loss: 0.0402\n",
      "Epoch 033 Batch 017 Loss: 0.1383\n",
      "Epoch 033 Batch 018 Loss: 0.0088\n",
      "Epoch 033 Batch 019 Loss: 0.0390\n",
      "Epoch 033 Batch 020 Loss: 0.0311\n",
      "Epoch 033 Batch 021 Loss: 0.0191\n",
      "Epoch 033 Batch 022 Loss: 0.0495\n",
      "Epoch 033 Batch 023 Loss: 0.0178\n",
      "Epoch 033 Batch 024 Loss: 0.0381\n",
      "Epoch 033 Batch 025 Loss: 0.0356\n",
      "Epoch 033 Batch 026 Loss: 0.0450\n",
      "Epoch 033 Batch 027 Loss: 0.0186\n",
      "Epoch 033 Batch 028 Loss: 0.0611\n",
      "Epoch 033 Batch 029 Loss: 0.0313\n",
      "Epoch 033 Batch 030 Loss: 0.1068\n",
      "Epoch 033 Batch 031 Loss: 0.0939\n",
      "Epoch 033 Batch 032 Loss: 0.2713\n",
      "Epoch 033 Batch 033 Loss: 0.0119\n",
      "Epoch 033 Batch 034 Loss: 0.1101\n",
      "Epoch 033 Batch 035 Loss: 0.2118\n",
      "Epoch 033 Batch 036 Loss: 0.0181\n",
      "Epoch 033 Batch 037 Loss: 0.0275\n",
      "Epoch 033 Batch 038 Loss: 0.3335\n",
      "Epoch 033 Batch 039 Loss: 0.0800\n",
      "Epoch 033 Batch 040 Loss: 0.0273\n",
      "Epoch 033 Batch 041 Loss: 0.0163\n",
      "Epoch 033 Batch 042 Loss: 0.0460\n",
      "Epoch 033 Batch 043 Loss: 0.1240\n",
      "Epoch 033 Batch 044 Loss: 0.0095\n",
      "Epoch 033 Batch 045 Loss: 0.0516\n",
      "Epoch 033 Batch 046 Loss: 0.0408\n",
      "Epoch 033 Batch 047 Loss: 0.0184\n",
      "Epoch 033 Batch 048 Loss: 0.0597\n",
      "Epoch 033 Batch 049 Loss: 0.0267\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 034 Batch 000 Loss: 0.0330\n",
      "Epoch 034 Batch 001 Loss: 0.0277\n",
      "Epoch 034 Batch 002 Loss: 0.0190\n",
      "Epoch 034 Batch 003 Loss: 0.0150\n",
      "Epoch 034 Batch 004 Loss: 0.1165\n",
      "Epoch 034 Batch 005 Loss: 0.0643\n",
      "Epoch 034 Batch 006 Loss: 0.0188\n",
      "Epoch 034 Batch 007 Loss: 0.1256\n",
      "Epoch 034 Batch 008 Loss: 0.0832\n",
      "Epoch 034 Batch 009 Loss: 0.0182\n",
      "Epoch 034 Batch 010 Loss: 0.0433\n",
      "Epoch 034 Batch 011 Loss: 0.0066\n",
      "Epoch 034 Batch 012 Loss: 0.0911\n",
      "Epoch 034 Batch 013 Loss: 0.0096\n",
      "Epoch 034 Batch 014 Loss: 0.0439\n",
      "Epoch 034 Batch 015 Loss: 0.0393\n",
      "Epoch 034 Batch 016 Loss: 0.0088\n",
      "Epoch 034 Batch 017 Loss: 0.0327\n",
      "Epoch 034 Batch 018 Loss: 0.2118\n",
      "Epoch 034 Batch 019 Loss: 0.1971\n",
      "Epoch 034 Batch 020 Loss: 0.2948\n",
      "Epoch 034 Batch 021 Loss: 0.0180\n",
      "Epoch 034 Batch 022 Loss: 0.7068\n",
      "Epoch 034 Batch 023 Loss: 0.1711\n",
      "Epoch 034 Batch 024 Loss: 0.0659\n",
      "Epoch 034 Batch 025 Loss: 0.1036\n",
      "Epoch 034 Batch 026 Loss: 0.1241\n",
      "Epoch 034 Batch 027 Loss: 0.3610\n",
      "Epoch 034 Batch 028 Loss: 0.1830\n",
      "Epoch 034 Batch 029 Loss: 0.2127\n",
      "Epoch 034 Batch 030 Loss: 0.0328\n",
      "Epoch 034 Batch 031 Loss: 0.1309\n",
      "Epoch 034 Batch 032 Loss: 0.0868\n",
      "Epoch 034 Batch 033 Loss: 0.0442\n",
      "Epoch 034 Batch 034 Loss: 0.0322\n",
      "Epoch 034 Batch 035 Loss: 0.0305\n",
      "Epoch 034 Batch 036 Loss: 0.0661\n",
      "Epoch 034 Batch 037 Loss: 0.4270\n",
      "Epoch 034 Batch 038 Loss: 0.0599\n",
      "Epoch 034 Batch 039 Loss: 0.0616\n",
      "Epoch 034 Batch 040 Loss: 0.1327\n",
      "Epoch 034 Batch 041 Loss: 0.0525\n",
      "Epoch 034 Batch 042 Loss: 0.0352\n",
      "Epoch 034 Batch 043 Loss: 0.1017\n",
      "Epoch 034 Batch 044 Loss: 0.1608\n",
      "Epoch 034 Batch 045 Loss: 0.1870\n",
      "Epoch 034 Batch 046 Loss: 0.0455\n",
      "Epoch 034 Batch 047 Loss: 0.0766\n",
      "Epoch 034 Batch 048 Loss: 0.0437\n",
      "Epoch 034 Batch 049 Loss: 0.0213\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8500000238418579\n",
      "-----------------------*-----------------------\n",
      "training complete!\n",
      "best accuracy:  0.8785714507102966\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8785714507102966"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
