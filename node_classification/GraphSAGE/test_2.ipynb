{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e33946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3695810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random_graph\n",
    "node_df, edge_df = random_graph.random_graph_gcn(1000,3000,nums_features=120,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc7658cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_format_process(node_df,edge_df):\n",
    "    \"\"\"\n",
    "    output\n",
    "    x: node features (array: float)\n",
    "    y: label         (array: float)\n",
    "    adjacency_dict: a dict store neighbor node's info  {node_index:[neighbor1,neighbor2..]}\n",
    "    train_mask: mask for providing training dataset (list: bool)\n",
    "    test_mask: mask for providing testing dataset (list: bool)\n",
    "    \"\"\"\n",
    "    # node_lookup: store node index\n",
    "    node_lookup = pd.DataFrame({'node': node_df.index,}, index=node_df.cust_id)\n",
    "    \n",
    "    # delete no-edge-node \n",
    "    diff_node = list(set(node_df['cust_id'])-(set(node_df['cust_id']) - set(edge_df['cust_id']) - set(edge_df['opp_id'])))\n",
    "    \n",
    "    node_df = node_df.iloc[node_lookup.iloc[diff_node]['node']].reset_index(drop=True)\n",
    "    \n",
    "    # build neighbor dictionary\n",
    "    node_lookup = pd.DataFrame({'node': node_df.index,}, index=node_df.cust_id)\n",
    "    adjacency_dict = defaultdict(list)\n",
    "    for cust,opp in zip(edge_df['cust_id'],edge_df['opp_id']):\n",
    "        adjacency_dict[node_lookup.loc[cust]['node']].append(node_lookup.loc[opp]['node'])\n",
    "    \n",
    "    # convert to Array\n",
    "    x = node_df[set(node_df) - {'cust_id', 'is_driver', 'is_reported'}].to_numpy()\n",
    "    y = node_df.is_reported.to_numpy() * 1\n",
    "    \n",
    "    # mask conf\n",
    "    train_mask = node_df.is_driver.to_numpy()\n",
    "    test_mask = ~train_mask\n",
    "    \n",
    "    return x, y, adjacency_dict, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98687c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, adjacency_dict, train_mask, test_mask  = data_format_process(node_df,edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f6e818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5575387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency(adj_dict):\n",
    "        \"\"\"根据邻接表创建邻接矩阵\"\"\"\n",
    "        edge_index = []\n",
    "        num_nodes = len(adj_dict)\n",
    "        for src, dst in adj_dict.items():\n",
    "            edge_index.extend([src, v] for v in dst)\n",
    "            edge_index.extend([v, src] for v in dst)\n",
    "        # 去除重复的边\n",
    "        edge_index = list(k for k, _ in itertools.groupby(sorted(edge_index)))\n",
    "        edge_index = np.asarray(edge_index)\n",
    "        adjacency = sp.coo_matrix((np.ones(len(edge_index)), \n",
    "                                   (edge_index[:, 0], edge_index[:, 1])),\n",
    "                    shape=(num_nodes, num_nodes), dtype=\"float32\")\n",
    "        return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f792556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = build_adjacency(adjacency_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "824d9523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 998)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0d4c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalization(adjacency):\n",
    "    \"\"\"计算 L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
    "    adjacency += sp.eye(adjacency.shape[0])    # 增加自连接\n",
    "    degree = np.array(adjacency.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    return d_hat.dot(adjacency).dot(d_hat).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde7d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = normalization(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e1a8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes, input_dim = x.shape\n",
    "indices = torch.from_numpy(np.asarray([adjacency.row, \n",
    "                                       adjacency.col]).astype('int64')).long()\n",
    "values = torch.from_numpy(adjacency.data.astype(np.float32))\n",
    "tensor_adjacency = torch.sparse.FloatTensor(indices, values, \n",
    "                                            (num_nodes, num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c0fe45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[  0,   0,   0,  ..., 997, 997, 997],\n",
       "                       [  0,  72, 260,  ..., 644, 743, 997]]),\n",
       "       values=tensor([0.1250, 0.1250, 0.1021,  ..., 0.1667, 0.1768, 0.2500]),\n",
       "       size=(998, 998), nnz=6968, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e4b1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.Tensor(np.random.rand(120,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d045478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35526368",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = torch.mm(x,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f7a2788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([998, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adfcad79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1788,  0.0176,  0.0294,  ..., -0.2162, -1.3712, -0.2204],\n",
       "        [ 3.2295,  2.4297,  5.7494,  ...,  5.0903,  6.1482,  4.7603],\n",
       "        [-1.2034, -1.3787, -0.4547,  ..., -1.7536,  0.5767, -0.1689],\n",
       "        ...,\n",
       "        [-0.6672, -2.6697, -1.7699,  ..., -1.9914, -2.2019, -0.4455],\n",
       "        [ 0.8897,  2.3477,  4.0818,  ...,  1.1187,  3.0042,  2.0533],\n",
       "        [-0.2056, -2.8479, -1.3615,  ..., -0.9712, -0.4147,  1.1956]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse.mm(tensor_adjacency, support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee250e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage_v2\n",
    "import random_graph\n",
    "node_df, edge_df = random_graph.random_graph_gcn(1000,3000,nums_features=120,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026a07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing..\n",
      "data preprocessing complete!\n",
      "-----------------------*-----------------------\n",
      "after filtering single nodes\n",
      "num of train instances: 554\n",
      "num of test instances: 444\n",
      "-----------------------*-----------------------\n",
      "model structure\n",
      "GraphSage(\n",
      "  in_features=120, num_neighbors_list=[10, 10]\n",
      "  (gcn): ModuleList(\n",
      "    (0): SageGCN(\n",
      "      in_features=120, out_features=128, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=120, out_features=128, aggr_method=mean)\n",
      "      (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (linear_2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): SageGCN(\n",
      "      in_features=128, out_features=2, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=128, out_features=2, aggr_method=mean)\n",
      "      (linear_1): Linear(in_features=2, out_features=4, bias=True)\n",
      "      (linear_2): Linear(in_features=4, out_features=2, bias=True)\n",
      "      (bn_1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a = sage_v2.run_model(node_df,edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc54e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training..\n",
      "training through cpu\n",
      "Epoch 000 Batch 000 Loss: 5.6416\n",
      "Epoch 000 Batch 001 Loss: 1.1078\n",
      "Epoch 000 Batch 002 Loss: 10.3123\n",
      "Epoch 000 Batch 003 Loss: 10.4840\n",
      "Epoch 000 Batch 004 Loss: 6.2598\n",
      "Epoch 000 Batch 005 Loss: 7.8102\n",
      "Epoch 000 Batch 006 Loss: 7.3508\n",
      "Epoch 000 Batch 007 Loss: 10.1797\n",
      "Epoch 000 Batch 008 Loss: 10.5731\n",
      "Epoch 000 Batch 009 Loss: 0.3671\n",
      "Epoch 000 Batch 010 Loss: 8.9212\n",
      "Epoch 000 Batch 011 Loss: 5.5620\n",
      "Epoch 000 Batch 012 Loss: 13.9099\n",
      "Epoch 000 Batch 013 Loss: 16.8648\n",
      "Epoch 000 Batch 014 Loss: 6.6336\n",
      "Epoch 000 Batch 015 Loss: 6.3487\n",
      "Epoch 000 Batch 016 Loss: 7.3861\n",
      "Epoch 000 Batch 017 Loss: 15.7320\n",
      "Epoch 000 Batch 018 Loss: 5.5056\n",
      "Epoch 000 Batch 019 Loss: 4.6191\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5585585832595825\n",
      "-----------------------*-----------------------\n",
      "Epoch 001 Batch 000 Loss: 7.7747\n",
      "Epoch 001 Batch 001 Loss: 9.6212\n",
      "Epoch 001 Batch 002 Loss: 1.7640\n",
      "Epoch 001 Batch 003 Loss: 4.1173\n",
      "Epoch 001 Batch 004 Loss: 7.2695\n",
      "Epoch 001 Batch 005 Loss: 4.0446\n",
      "Epoch 001 Batch 006 Loss: 14.6319\n",
      "Epoch 001 Batch 007 Loss: 5.0608\n",
      "Epoch 001 Batch 008 Loss: 4.9615\n",
      "Epoch 001 Batch 009 Loss: 5.0418\n",
      "Epoch 001 Batch 010 Loss: 5.9652\n",
      "Epoch 001 Batch 011 Loss: 5.2041\n",
      "Epoch 001 Batch 012 Loss: 1.8951\n",
      "Epoch 001 Batch 013 Loss: 2.0551\n",
      "Epoch 001 Batch 014 Loss: 2.3009\n",
      "Epoch 001 Batch 015 Loss: 10.5453\n",
      "Epoch 001 Batch 016 Loss: 4.8407\n",
      "Epoch 001 Batch 017 Loss: 7.0704\n",
      "Epoch 001 Batch 018 Loss: 4.4087\n",
      "Epoch 001 Batch 019 Loss: 0.9038\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5405405163764954\n",
      "-----------------------*-----------------------\n",
      "Epoch 002 Batch 000 Loss: 1.8882\n",
      "Epoch 002 Batch 001 Loss: 0.7223\n",
      "Epoch 002 Batch 002 Loss: 2.5062\n",
      "Epoch 002 Batch 003 Loss: 0.8989\n",
      "Epoch 002 Batch 004 Loss: 9.3528\n",
      "Epoch 002 Batch 005 Loss: 0.7927\n",
      "Epoch 002 Batch 006 Loss: 2.0183\n",
      "Epoch 002 Batch 007 Loss: 2.1704\n",
      "Epoch 002 Batch 008 Loss: 4.2648\n",
      "Epoch 002 Batch 009 Loss: 7.1707\n",
      "Epoch 002 Batch 010 Loss: 3.3708\n",
      "Epoch 002 Batch 011 Loss: 4.7954\n",
      "Epoch 002 Batch 012 Loss: 3.3704\n",
      "Epoch 002 Batch 013 Loss: 4.3635\n",
      "Epoch 002 Batch 014 Loss: 2.4879\n",
      "Epoch 002 Batch 015 Loss: 6.7264\n",
      "Epoch 002 Batch 016 Loss: 2.5973\n",
      "Epoch 002 Batch 017 Loss: 1.5004\n",
      "Epoch 002 Batch 018 Loss: 2.9126\n",
      "Epoch 002 Batch 019 Loss: 2.2744\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.4954954981803894\n",
      "-----------------------*-----------------------\n",
      "Epoch 003 Batch 000 Loss: 0.7272\n",
      "Epoch 003 Batch 001 Loss: 2.4438\n",
      "Epoch 003 Batch 002 Loss: 0.5122\n",
      "Epoch 003 Batch 003 Loss: 2.1238\n",
      "Epoch 003 Batch 004 Loss: 1.2991\n",
      "Epoch 003 Batch 005 Loss: 2.3384\n",
      "Epoch 003 Batch 006 Loss: 0.3057\n",
      "Epoch 003 Batch 007 Loss: 5.2934\n",
      "Epoch 003 Batch 008 Loss: 0.7426\n",
      "Epoch 003 Batch 009 Loss: 2.4783\n",
      "Epoch 003 Batch 010 Loss: 2.3062\n",
      "Epoch 003 Batch 011 Loss: 3.9245\n",
      "Epoch 003 Batch 012 Loss: 0.5317\n",
      "Epoch 003 Batch 013 Loss: 3.9953\n",
      "Epoch 003 Batch 014 Loss: 3.1475\n",
      "Epoch 003 Batch 015 Loss: 1.3342\n",
      "Epoch 003 Batch 016 Loss: 1.2817\n",
      "Epoch 003 Batch 017 Loss: 1.0936\n",
      "Epoch 003 Batch 018 Loss: 0.9380\n",
      "Epoch 003 Batch 019 Loss: 1.4474\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6036036014556885\n",
      "-----------------------*-----------------------\n",
      "Epoch 004 Batch 000 Loss: 0.0120\n",
      "Epoch 004 Batch 001 Loss: 1.1086\n",
      "Epoch 004 Batch 002 Loss: 0.7976\n",
      "Epoch 004 Batch 003 Loss: 2.2565\n",
      "Epoch 004 Batch 004 Loss: 0.3802\n",
      "Epoch 004 Batch 005 Loss: 0.1281\n",
      "Epoch 004 Batch 006 Loss: 0.3974\n",
      "Epoch 004 Batch 007 Loss: 0.2355\n",
      "Epoch 004 Batch 008 Loss: 1.0265\n",
      "Epoch 004 Batch 009 Loss: 0.8773\n",
      "Epoch 004 Batch 010 Loss: 0.8523\n",
      "Epoch 004 Batch 011 Loss: 0.5302\n",
      "Epoch 004 Batch 012 Loss: 0.2335\n",
      "Epoch 004 Batch 013 Loss: 3.4653\n",
      "Epoch 004 Batch 014 Loss: 1.5715\n",
      "Epoch 004 Batch 015 Loss: 1.4671\n",
      "Epoch 004 Batch 016 Loss: 0.3535\n",
      "Epoch 004 Batch 017 Loss: 1.8852\n",
      "Epoch 004 Batch 018 Loss: 0.0419\n",
      "Epoch 004 Batch 019 Loss: 0.0085\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5743243098258972\n",
      "-----------------------*-----------------------\n",
      "Epoch 005 Batch 000 Loss: 1.1563\n",
      "Epoch 005 Batch 001 Loss: 0.6101\n",
      "Epoch 005 Batch 002 Loss: 0.4146\n",
      "Epoch 005 Batch 003 Loss: 0.4346\n",
      "Epoch 005 Batch 004 Loss: 0.3863\n",
      "Epoch 005 Batch 005 Loss: 1.2015\n",
      "Epoch 005 Batch 006 Loss: 0.4047\n",
      "Epoch 005 Batch 007 Loss: 0.3163\n",
      "Epoch 005 Batch 008 Loss: 0.3741\n",
      "Epoch 005 Batch 009 Loss: 0.1561\n",
      "Epoch 005 Batch 010 Loss: 0.9934\n",
      "Epoch 005 Batch 011 Loss: 0.2048\n",
      "Epoch 005 Batch 012 Loss: 1.4968\n",
      "Epoch 005 Batch 013 Loss: 0.4714\n",
      "Epoch 005 Batch 014 Loss: 0.2312\n",
      "Epoch 005 Batch 015 Loss: 1.4397\n",
      "Epoch 005 Batch 016 Loss: 0.6288\n",
      "Epoch 005 Batch 017 Loss: 1.5947\n",
      "Epoch 005 Batch 018 Loss: 0.6066\n",
      "Epoch 005 Batch 019 Loss: 0.4085\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5788288116455078\n",
      "-----------------------*-----------------------\n",
      "Epoch 006 Batch 000 Loss: 1.4247\n",
      "Epoch 006 Batch 001 Loss: 1.1250\n",
      "Epoch 006 Batch 002 Loss: 1.6446\n",
      "Epoch 006 Batch 003 Loss: 0.1700\n",
      "Epoch 006 Batch 004 Loss: 0.7053\n",
      "Epoch 006 Batch 005 Loss: 0.1417\n",
      "Epoch 006 Batch 006 Loss: 0.1659\n",
      "Epoch 006 Batch 007 Loss: 0.8088\n",
      "Epoch 006 Batch 008 Loss: 0.2867\n",
      "Epoch 006 Batch 009 Loss: 0.3799\n",
      "Epoch 006 Batch 010 Loss: 0.8573\n",
      "Epoch 006 Batch 011 Loss: 0.2923\n",
      "Epoch 006 Batch 012 Loss: 1.4182\n",
      "Epoch 006 Batch 013 Loss: 0.2428\n",
      "Epoch 006 Batch 014 Loss: 0.4492\n",
      "Epoch 006 Batch 015 Loss: 0.2471\n",
      "Epoch 006 Batch 016 Loss: 0.3686\n",
      "Epoch 006 Batch 017 Loss: 0.2297\n",
      "Epoch 006 Batch 018 Loss: 0.1554\n",
      "Epoch 006 Batch 019 Loss: 0.1640\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6126126050949097\n",
      "-----------------------*-----------------------\n",
      "Epoch 007 Batch 000 Loss: 0.6703\n",
      "Epoch 007 Batch 001 Loss: 0.3973\n",
      "Epoch 007 Batch 002 Loss: 0.2419\n",
      "Epoch 007 Batch 003 Loss: 0.2552\n",
      "Epoch 007 Batch 004 Loss: 1.3143\n",
      "Epoch 007 Batch 005 Loss: 0.7543\n",
      "Epoch 007 Batch 006 Loss: 0.1972\n",
      "Epoch 007 Batch 007 Loss: 0.1507\n",
      "Epoch 007 Batch 008 Loss: 0.1969\n",
      "Epoch 007 Batch 009 Loss: 0.4738\n",
      "Epoch 007 Batch 010 Loss: 0.2722\n",
      "Epoch 007 Batch 011 Loss: 0.1921\n",
      "Epoch 007 Batch 012 Loss: 0.1590\n",
      "Epoch 007 Batch 013 Loss: 0.6435\n",
      "Epoch 007 Batch 014 Loss: 0.4652\n",
      "Epoch 007 Batch 015 Loss: 0.1285\n",
      "Epoch 007 Batch 016 Loss: 0.2836\n",
      "Epoch 007 Batch 017 Loss: 0.0528\n",
      "Epoch 007 Batch 018 Loss: 0.1538\n",
      "Epoch 007 Batch 019 Loss: 0.0639\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6036036014556885\n",
      "-----------------------*-----------------------\n",
      "Epoch 008 Batch 000 Loss: 0.1567\n",
      "Epoch 008 Batch 001 Loss: 0.1322\n",
      "Epoch 008 Batch 002 Loss: 0.2827\n",
      "Epoch 008 Batch 003 Loss: 0.1449\n",
      "Epoch 008 Batch 004 Loss: 0.1719\n",
      "Epoch 008 Batch 005 Loss: 0.1970\n",
      "Epoch 008 Batch 006 Loss: 0.3267\n",
      "Epoch 008 Batch 007 Loss: 0.1519\n",
      "Epoch 008 Batch 008 Loss: 0.2025\n",
      "Epoch 008 Batch 009 Loss: 0.0555\n",
      "Epoch 008 Batch 010 Loss: 0.4516\n",
      "Epoch 008 Batch 011 Loss: 0.1460\n",
      "Epoch 008 Batch 012 Loss: 0.2462\n",
      "Epoch 008 Batch 013 Loss: 0.0375\n",
      "Epoch 008 Batch 014 Loss: 0.4232\n",
      "Epoch 008 Batch 015 Loss: 2.3469\n",
      "Epoch 008 Batch 016 Loss: 0.0878\n",
      "Epoch 008 Batch 017 Loss: 0.1979\n",
      "Epoch 008 Batch 018 Loss: 0.1184\n",
      "Epoch 008 Batch 019 Loss: 0.1573\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5405405163764954\n",
      "-----------------------*-----------------------\n",
      "Epoch 009 Batch 000 Loss: 0.1453\n",
      "Epoch 009 Batch 001 Loss: 0.1204\n",
      "Epoch 009 Batch 002 Loss: 0.3699\n",
      "Epoch 009 Batch 003 Loss: 0.1115\n",
      "Epoch 009 Batch 004 Loss: 0.2542\n",
      "Epoch 009 Batch 005 Loss: 0.1536\n",
      "Epoch 009 Batch 006 Loss: 0.0994\n",
      "Epoch 009 Batch 007 Loss: 0.2205\n",
      "Epoch 009 Batch 008 Loss: 0.3186\n",
      "Epoch 009 Batch 009 Loss: 0.1482\n",
      "Epoch 009 Batch 010 Loss: 0.0878\n",
      "Epoch 009 Batch 011 Loss: 0.0784\n",
      "Epoch 009 Batch 012 Loss: 0.0949\n",
      "Epoch 009 Batch 013 Loss: 0.2308\n",
      "Epoch 009 Batch 014 Loss: 0.1169\n",
      "Epoch 009 Batch 015 Loss: 0.1275\n",
      "Epoch 009 Batch 016 Loss: 0.0517\n",
      "Epoch 009 Batch 017 Loss: 0.0297\n",
      "Epoch 009 Batch 018 Loss: 0.1805\n",
      "Epoch 009 Batch 019 Loss: 0.0657\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5698198080062866\n",
      "-----------------------*-----------------------\n",
      "Epoch 010 Batch 000 Loss: 0.0873\n",
      "Epoch 010 Batch 001 Loss: 0.0204\n",
      "Epoch 010 Batch 002 Loss: 0.1331\n",
      "Epoch 010 Batch 003 Loss: 0.0985\n",
      "Epoch 010 Batch 004 Loss: 0.0352\n",
      "Epoch 010 Batch 005 Loss: 0.2438\n",
      "Epoch 010 Batch 006 Loss: 0.0493\n",
      "Epoch 010 Batch 007 Loss: 0.0441\n",
      "Epoch 010 Batch 008 Loss: 0.2602\n",
      "Epoch 010 Batch 009 Loss: 0.0414\n",
      "Epoch 010 Batch 010 Loss: 0.2796\n",
      "Epoch 010 Batch 011 Loss: 0.0587\n",
      "Epoch 010 Batch 012 Loss: 0.0994\n",
      "Epoch 010 Batch 013 Loss: 0.0045\n",
      "Epoch 010 Batch 014 Loss: 0.6756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 Batch 015 Loss: 0.2363\n",
      "Epoch 010 Batch 016 Loss: 0.0563\n",
      "Epoch 010 Batch 017 Loss: 0.0241\n",
      "Epoch 010 Batch 018 Loss: 0.0394\n",
      "Epoch 010 Batch 019 Loss: 0.0473\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5855855941772461\n",
      "-----------------------*-----------------------\n",
      "Epoch 011 Batch 000 Loss: 0.0614\n",
      "Epoch 011 Batch 001 Loss: 0.0214\n",
      "Epoch 011 Batch 002 Loss: 0.1221\n",
      "Epoch 011 Batch 003 Loss: 0.0376\n",
      "Epoch 011 Batch 004 Loss: 0.3028\n",
      "Epoch 011 Batch 005 Loss: 0.0288\n",
      "Epoch 011 Batch 006 Loss: 0.0332\n",
      "Epoch 011 Batch 007 Loss: 0.0786\n",
      "Epoch 011 Batch 008 Loss: 0.1162\n",
      "Epoch 011 Batch 009 Loss: 0.1305\n",
      "Epoch 011 Batch 010 Loss: 0.1637\n",
      "Epoch 011 Batch 011 Loss: 0.0249\n",
      "Epoch 011 Batch 012 Loss: 0.2756\n",
      "Epoch 011 Batch 013 Loss: 0.0527\n",
      "Epoch 011 Batch 014 Loss: 0.0351\n",
      "Epoch 011 Batch 015 Loss: 0.0299\n",
      "Epoch 011 Batch 016 Loss: 0.1167\n",
      "Epoch 011 Batch 017 Loss: 0.0092\n",
      "Epoch 011 Batch 018 Loss: 0.0226\n",
      "Epoch 011 Batch 019 Loss: 0.1197\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5382882952690125\n",
      "-----------------------*-----------------------\n",
      "Epoch 012 Batch 000 Loss: 0.1317\n",
      "Epoch 012 Batch 001 Loss: 0.0275\n",
      "Epoch 012 Batch 002 Loss: 0.0716\n",
      "Epoch 012 Batch 003 Loss: 0.1380\n",
      "Epoch 012 Batch 004 Loss: 0.0225\n",
      "Epoch 012 Batch 005 Loss: 0.1432\n",
      "Epoch 012 Batch 006 Loss: 0.0705\n",
      "Epoch 012 Batch 007 Loss: 0.0150\n",
      "Epoch 012 Batch 008 Loss: 0.0381\n",
      "Epoch 012 Batch 009 Loss: 0.0189\n",
      "Epoch 012 Batch 010 Loss: 0.0556\n",
      "Epoch 012 Batch 011 Loss: 0.0518\n",
      "Epoch 012 Batch 012 Loss: 0.0065\n",
      "Epoch 012 Batch 013 Loss: 0.0327\n",
      "Epoch 012 Batch 014 Loss: 0.6338\n",
      "Epoch 012 Batch 015 Loss: 0.0217\n",
      "Epoch 012 Batch 016 Loss: 0.2782\n",
      "Epoch 012 Batch 017 Loss: 0.1769\n",
      "Epoch 012 Batch 018 Loss: 0.0705\n",
      "Epoch 012 Batch 019 Loss: 0.1032\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5810810923576355\n",
      "-----------------------*-----------------------\n",
      "Epoch 013 Batch 000 Loss: 0.0196\n",
      "Epoch 013 Batch 001 Loss: 0.0091\n",
      "Epoch 013 Batch 002 Loss: 0.1920\n",
      "Epoch 013 Batch 003 Loss: 0.0467\n",
      "Epoch 013 Batch 004 Loss: 0.0066\n",
      "Epoch 013 Batch 005 Loss: 0.0285\n",
      "Epoch 013 Batch 006 Loss: 0.0260\n",
      "Epoch 013 Batch 007 Loss: 0.1151\n",
      "Epoch 013 Batch 008 Loss: 0.0685\n",
      "Epoch 013 Batch 009 Loss: 0.9318\n",
      "Epoch 013 Batch 010 Loss: 0.1938\n",
      "Epoch 013 Batch 011 Loss: 0.1414\n",
      "Epoch 013 Batch 012 Loss: 0.0710\n",
      "Epoch 013 Batch 013 Loss: 0.0513\n",
      "Epoch 013 Batch 014 Loss: 0.0216\n",
      "Epoch 013 Batch 015 Loss: 0.0112\n",
      "Epoch 013 Batch 016 Loss: 0.0148\n",
      "Epoch 013 Batch 017 Loss: 0.0240\n",
      "Epoch 013 Batch 018 Loss: 0.4528\n",
      "Epoch 013 Batch 019 Loss: 0.1037\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5675675868988037\n",
      "-----------------------*-----------------------\n",
      "Epoch 014 Batch 000 Loss: 0.0232\n",
      "Epoch 014 Batch 001 Loss: 0.0634\n",
      "Epoch 014 Batch 002 Loss: 0.0087\n",
      "Epoch 014 Batch 003 Loss: 0.0231\n",
      "Epoch 014 Batch 004 Loss: 0.0022\n",
      "Epoch 014 Batch 005 Loss: 0.0769\n",
      "Epoch 014 Batch 006 Loss: 0.0176\n",
      "Epoch 014 Batch 007 Loss: 0.0027\n",
      "Epoch 014 Batch 008 Loss: 0.1219\n",
      "Epoch 014 Batch 009 Loss: 0.0183\n",
      "Epoch 014 Batch 010 Loss: 0.0122\n",
      "Epoch 014 Batch 011 Loss: 0.0298\n",
      "Epoch 014 Batch 012 Loss: 0.0551\n",
      "Epoch 014 Batch 013 Loss: 0.2291\n",
      "Epoch 014 Batch 014 Loss: 0.0126\n",
      "Epoch 014 Batch 015 Loss: 0.0523\n",
      "Epoch 014 Batch 016 Loss: 0.5394\n",
      "Epoch 014 Batch 017 Loss: 0.0271\n",
      "Epoch 014 Batch 018 Loss: 0.0995\n",
      "Epoch 014 Batch 019 Loss: 0.0029\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.565315306186676\n",
      "-----------------------*-----------------------\n",
      "Epoch 015 Batch 000 Loss: 0.0512\n",
      "Epoch 015 Batch 001 Loss: 0.0342\n",
      "Epoch 015 Batch 002 Loss: 0.0116\n",
      "Epoch 015 Batch 003 Loss: 0.3659\n",
      "Epoch 015 Batch 004 Loss: 0.0551\n",
      "Epoch 015 Batch 005 Loss: 0.0187\n",
      "Epoch 015 Batch 006 Loss: 0.0308\n",
      "Epoch 015 Batch 007 Loss: 0.0197\n",
      "Epoch 015 Batch 008 Loss: 0.0608\n",
      "Epoch 015 Batch 009 Loss: 0.0025\n",
      "Epoch 015 Batch 010 Loss: 0.0270\n",
      "Epoch 015 Batch 011 Loss: 0.0076\n",
      "Epoch 015 Batch 012 Loss: 0.0833\n",
      "Epoch 015 Batch 013 Loss: 0.0131\n",
      "Epoch 015 Batch 014 Loss: 0.0057\n",
      "Epoch 015 Batch 015 Loss: 0.0173\n",
      "Epoch 015 Batch 016 Loss: 0.0121\n",
      "Epoch 015 Batch 017 Loss: 0.0173\n",
      "Epoch 015 Batch 018 Loss: 0.0178\n",
      "Epoch 015 Batch 019 Loss: 0.0057\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5630630850791931\n",
      "-----------------------*-----------------------\n",
      "Epoch 016 Batch 000 Loss: 0.0533\n",
      "Epoch 016 Batch 001 Loss: 0.0549\n",
      "Epoch 016 Batch 002 Loss: 0.2034\n",
      "Epoch 016 Batch 003 Loss: 0.0674\n",
      "Epoch 016 Batch 004 Loss: 0.0037\n",
      "Epoch 016 Batch 005 Loss: 0.0383\n",
      "Epoch 016 Batch 006 Loss: 0.0056\n",
      "Epoch 016 Batch 007 Loss: 0.0119\n",
      "Epoch 016 Batch 008 Loss: 0.0026\n",
      "Epoch 016 Batch 009 Loss: 0.0096\n",
      "Epoch 016 Batch 010 Loss: 0.0045\n",
      "Epoch 016 Batch 011 Loss: 0.0031\n",
      "Epoch 016 Batch 012 Loss: 0.0279\n",
      "Epoch 016 Batch 013 Loss: 0.0206\n",
      "Epoch 016 Batch 014 Loss: 0.0346\n",
      "Epoch 016 Batch 015 Loss: 0.0025\n",
      "Epoch 016 Batch 016 Loss: 0.0046\n",
      "Epoch 016 Batch 017 Loss: 0.1012\n",
      "Epoch 016 Batch 018 Loss: 0.0291\n",
      "Epoch 016 Batch 019 Loss: 0.0261\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5518018007278442\n",
      "-----------------------*-----------------------\n",
      "Epoch 017 Batch 000 Loss: 0.0313\n",
      "Epoch 017 Batch 001 Loss: 0.0052\n",
      "Epoch 017 Batch 002 Loss: 0.2508\n",
      "Epoch 017 Batch 003 Loss: 0.0647\n",
      "Epoch 017 Batch 004 Loss: 0.0016\n",
      "Epoch 017 Batch 005 Loss: 0.0257\n",
      "Epoch 017 Batch 006 Loss: 0.0054\n",
      "Epoch 017 Batch 007 Loss: 0.1450\n",
      "Epoch 017 Batch 008 Loss: 0.0134\n",
      "Epoch 017 Batch 009 Loss: 0.0122\n",
      "Epoch 017 Batch 010 Loss: 0.0999\n",
      "Epoch 017 Batch 011 Loss: 0.0278\n",
      "Epoch 017 Batch 012 Loss: 0.0075\n",
      "Epoch 017 Batch 013 Loss: 0.0098\n",
      "Epoch 017 Batch 014 Loss: 0.0134\n",
      "Epoch 017 Batch 015 Loss: 0.0190\n",
      "Epoch 017 Batch 016 Loss: 0.0097\n",
      "Epoch 017 Batch 017 Loss: 0.0173\n",
      "Epoch 017 Batch 018 Loss: 0.0113\n",
      "Epoch 017 Batch 019 Loss: 0.0220\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6036036014556885\n",
      "-----------------------*-----------------------\n",
      "Epoch 018 Batch 000 Loss: 0.0034\n",
      "Epoch 018 Batch 001 Loss: 0.2099\n",
      "Epoch 018 Batch 002 Loss: 0.0015\n",
      "Epoch 018 Batch 003 Loss: 0.0125\n",
      "Epoch 018 Batch 004 Loss: 0.0073\n",
      "Epoch 018 Batch 005 Loss: 0.0023\n",
      "Epoch 018 Batch 006 Loss: 0.0078\n",
      "Epoch 018 Batch 007 Loss: 0.0089\n",
      "Epoch 018 Batch 008 Loss: 0.0217\n",
      "Epoch 018 Batch 009 Loss: 0.0033\n",
      "Epoch 018 Batch 010 Loss: 0.0016\n",
      "Epoch 018 Batch 011 Loss: 0.0079\n",
      "Epoch 018 Batch 012 Loss: 0.0023\n",
      "Epoch 018 Batch 013 Loss: 0.0008\n",
      "Epoch 018 Batch 014 Loss: 0.0067\n",
      "Epoch 018 Batch 015 Loss: 0.0565\n",
      "Epoch 018 Batch 016 Loss: 0.0018\n",
      "Epoch 018 Batch 017 Loss: 0.0045\n",
      "Epoch 018 Batch 018 Loss: 0.0041\n",
      "Epoch 018 Batch 019 Loss: 0.0075\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5630630850791931\n",
      "-----------------------*-----------------------\n",
      "Epoch 019 Batch 000 Loss: 0.0016\n",
      "Epoch 019 Batch 001 Loss: 0.0896\n",
      "Epoch 019 Batch 002 Loss: 0.0387\n",
      "Epoch 019 Batch 003 Loss: 0.0052\n",
      "Epoch 019 Batch 004 Loss: 0.0345\n",
      "Epoch 019 Batch 005 Loss: 0.0171\n",
      "Epoch 019 Batch 006 Loss: 0.0006\n",
      "Epoch 019 Batch 007 Loss: 0.0080\n",
      "Epoch 019 Batch 008 Loss: 0.0011\n",
      "Epoch 019 Batch 009 Loss: 0.2129\n",
      "Epoch 019 Batch 010 Loss: 0.0114\n",
      "Epoch 019 Batch 011 Loss: 0.0089\n",
      "Epoch 019 Batch 012 Loss: 1.2485\n",
      "Epoch 019 Batch 013 Loss: 0.0125\n",
      "Epoch 019 Batch 014 Loss: 0.0007\n",
      "Epoch 019 Batch 015 Loss: 0.0187\n",
      "Epoch 019 Batch 016 Loss: 0.0022\n",
      "Epoch 019 Batch 017 Loss: 0.0124\n",
      "Epoch 019 Batch 018 Loss: 0.0075\n",
      "Epoch 019 Batch 019 Loss: 0.0033\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5360360145568848\n",
      "-----------------------*-----------------------\n",
      "training complete!\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826f0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sage_v2_cora\n",
    "from data import CoraData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06aebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cached file: /Users/shuaihengxiao/Desktop/graphSAGE_v0/data/cora/ch7_cached.pkl\n"
     ]
    }
   ],
   "source": [
    "data = CoraData(data_root=\"/Users/shuaihengxiao/Desktop/graphSAGE_v0/data/cora\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fe9443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing..\n",
      "data preprocessing complete!\n",
      "-----------------------*-----------------------\n",
      "after filtering single nodes\n",
      "num of train instances: 1000\n",
      "num of test instances: 140\n",
      "-----------------------*-----------------------\n",
      "model structure\n",
      "GraphSage(\n",
      "  in_features=1433, num_neighbors_list=[20, 10]\n",
      "  (gcn): ModuleList(\n",
      "    (0): SageGCN(\n",
      "      in_features=1433, out_features=128, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=1433, out_features=128, aggr_method=mean)\n",
      "      (dropout1): Dropout(p=0.4, inplace=False)\n",
      "      (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (linear_2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (bn_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): SageGCN(\n",
      "      in_features=128, out_features=7, aggr_hidden_method=sum\n",
      "      (aggregator): NeighborAggregator(in_features=128, out_features=7, aggr_method=mean)\n",
      "      (dropout1): Dropout(p=0.4, inplace=False)\n",
      "      (linear_1): Linear(in_features=7, out_features=14, bias=True)\n",
      "      (linear_2): Linear(in_features=14, out_features=7, bias=True)\n",
      "      (bn_1): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a = sage_v2_cora.run_model(data,\n",
    "                            hidden_dim = [128,7],\n",
    "                            num_neighbors_list=[20, 10],\n",
    "                            aggr_neighbor_method = 'mean',\n",
    "                            aggr_hidden_method = 'sum',\n",
    "                            batch_size = 64,\n",
    "                            epochs = 30,\n",
    "                            num_batch_per_epoch = 32,\n",
    "                            lr=1e-4,\n",
    "                            residual_block=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1fe097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training..\n",
      "training through cpu\n",
      "Epoch 000 Batch 000 Loss: 8.6681\n",
      "Epoch 000 Batch 001 Loss: 7.5369\n",
      "Epoch 000 Batch 002 Loss: 6.8348\n",
      "Epoch 000 Batch 003 Loss: 7.8554\n",
      "Epoch 000 Batch 004 Loss: 7.5405\n",
      "Epoch 000 Batch 005 Loss: 8.5050\n",
      "Epoch 000 Batch 006 Loss: 8.4968\n",
      "Epoch 000 Batch 007 Loss: 8.1036\n",
      "Epoch 000 Batch 008 Loss: 8.8254\n",
      "Epoch 000 Batch 009 Loss: 7.3863\n",
      "Epoch 000 Batch 010 Loss: 9.3827\n",
      "Epoch 000 Batch 011 Loss: 8.4671\n",
      "Epoch 000 Batch 012 Loss: 7.6952\n",
      "Epoch 000 Batch 013 Loss: 7.8918\n",
      "Epoch 000 Batch 014 Loss: 7.7317\n",
      "Epoch 000 Batch 015 Loss: 6.3541\n",
      "Epoch 000 Batch 016 Loss: 7.9998\n",
      "Epoch 000 Batch 017 Loss: 6.8887\n",
      "Epoch 000 Batch 018 Loss: 6.9706\n",
      "Epoch 000 Batch 019 Loss: 6.8206\n",
      "Epoch 000 Batch 020 Loss: 6.8563\n",
      "Epoch 000 Batch 021 Loss: 7.7420\n",
      "Epoch 000 Batch 022 Loss: 6.4108\n",
      "Epoch 000 Batch 023 Loss: 5.3994\n",
      "Epoch 000 Batch 024 Loss: 6.6458\n",
      "Epoch 000 Batch 025 Loss: 7.0993\n",
      "Epoch 000 Batch 026 Loss: 6.7405\n",
      "Epoch 000 Batch 027 Loss: 8.0373\n",
      "Epoch 000 Batch 028 Loss: 5.5541\n",
      "Epoch 000 Batch 029 Loss: 6.1723\n",
      "Epoch 000 Batch 030 Loss: 7.7298\n",
      "Epoch 000 Batch 031 Loss: 4.7335\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.25\n",
      "-----------------------*-----------------------\n",
      "Epoch 001 Batch 000 Loss: 2.3792\n",
      "Epoch 001 Batch 001 Loss: 2.1223\n",
      "Epoch 001 Batch 002 Loss: 2.2147\n",
      "Epoch 001 Batch 003 Loss: 2.4569\n",
      "Epoch 001 Batch 004 Loss: 2.1941\n",
      "Epoch 001 Batch 005 Loss: 1.8836\n",
      "Epoch 001 Batch 006 Loss: 2.5086\n",
      "Epoch 001 Batch 007 Loss: 2.1457\n",
      "Epoch 001 Batch 008 Loss: 2.3160\n",
      "Epoch 001 Batch 009 Loss: 2.1391\n",
      "Epoch 001 Batch 010 Loss: 2.6143\n",
      "Epoch 001 Batch 011 Loss: 2.2508\n",
      "Epoch 001 Batch 012 Loss: 2.4520\n",
      "Epoch 001 Batch 013 Loss: 2.2700\n",
      "Epoch 001 Batch 014 Loss: 1.8424\n",
      "Epoch 001 Batch 015 Loss: 1.9993\n",
      "Epoch 001 Batch 016 Loss: 1.6911\n",
      "Epoch 001 Batch 017 Loss: 1.7776\n",
      "Epoch 001 Batch 018 Loss: 1.8825\n",
      "Epoch 001 Batch 019 Loss: 2.1832\n",
      "Epoch 001 Batch 020 Loss: 1.9681\n",
      "Epoch 001 Batch 021 Loss: 1.9177\n",
      "Epoch 001 Batch 022 Loss: 1.7305\n",
      "Epoch 001 Batch 023 Loss: 1.8981\n",
      "Epoch 001 Batch 024 Loss: 1.5943\n",
      "Epoch 001 Batch 025 Loss: 2.0596\n",
      "Epoch 001 Batch 026 Loss: 1.7205\n",
      "Epoch 001 Batch 027 Loss: 1.8896\n",
      "Epoch 001 Batch 028 Loss: 2.0203\n",
      "Epoch 001 Batch 029 Loss: 1.6599\n",
      "Epoch 001 Batch 030 Loss: 1.7509\n",
      "Epoch 001 Batch 031 Loss: 1.7666\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.3285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 002 Batch 000 Loss: 1.6078\n",
      "Epoch 002 Batch 001 Loss: 1.4686\n",
      "Epoch 002 Batch 002 Loss: 1.5718\n",
      "Epoch 002 Batch 003 Loss: 1.6363\n",
      "Epoch 002 Batch 004 Loss: 1.7091\n",
      "Epoch 002 Batch 005 Loss: 1.5441\n",
      "Epoch 002 Batch 006 Loss: 1.6732\n",
      "Epoch 002 Batch 007 Loss: 1.6210\n",
      "Epoch 002 Batch 008 Loss: 1.4541\n",
      "Epoch 002 Batch 009 Loss: 1.7142\n",
      "Epoch 002 Batch 010 Loss: 1.6287\n",
      "Epoch 002 Batch 011 Loss: 1.5413\n",
      "Epoch 002 Batch 012 Loss: 1.4200\n",
      "Epoch 002 Batch 013 Loss: 1.4389\n",
      "Epoch 002 Batch 014 Loss: 2.0580\n",
      "Epoch 002 Batch 015 Loss: 1.8346\n",
      "Epoch 002 Batch 016 Loss: 1.4001\n",
      "Epoch 002 Batch 017 Loss: 1.6771\n",
      "Epoch 002 Batch 018 Loss: 1.3333\n",
      "Epoch 002 Batch 019 Loss: 1.5436\n",
      "Epoch 002 Batch 020 Loss: 1.3709\n",
      "Epoch 002 Batch 021 Loss: 1.4990\n",
      "Epoch 002 Batch 022 Loss: 1.6662\n",
      "Epoch 002 Batch 023 Loss: 1.1104\n",
      "Epoch 002 Batch 024 Loss: 2.0158\n",
      "Epoch 002 Batch 025 Loss: 1.4989\n",
      "Epoch 002 Batch 026 Loss: 1.4916\n",
      "Epoch 002 Batch 027 Loss: 1.3573\n",
      "Epoch 002 Batch 028 Loss: 1.5762\n",
      "Epoch 002 Batch 029 Loss: 1.3693\n",
      "Epoch 002 Batch 030 Loss: 1.3021\n",
      "Epoch 002 Batch 031 Loss: 1.3199\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.4285714328289032\n",
      "-----------------------*-----------------------\n",
      "Epoch 003 Batch 000 Loss: 1.2708\n",
      "Epoch 003 Batch 001 Loss: 1.5589\n",
      "Epoch 003 Batch 002 Loss: 1.1613\n",
      "Epoch 003 Batch 003 Loss: 1.2244\n",
      "Epoch 003 Batch 004 Loss: 1.3310\n",
      "Epoch 003 Batch 005 Loss: 1.2990\n",
      "Epoch 003 Batch 006 Loss: 1.0755\n",
      "Epoch 003 Batch 007 Loss: 1.3559\n",
      "Epoch 003 Batch 008 Loss: 1.2344\n",
      "Epoch 003 Batch 009 Loss: 1.2218\n",
      "Epoch 003 Batch 010 Loss: 1.1579\n",
      "Epoch 003 Batch 011 Loss: 1.1092\n",
      "Epoch 003 Batch 012 Loss: 1.2427\n",
      "Epoch 003 Batch 013 Loss: 1.2135\n",
      "Epoch 003 Batch 014 Loss: 1.0994\n",
      "Epoch 003 Batch 015 Loss: 1.0781\n",
      "Epoch 003 Batch 016 Loss: 1.2503\n",
      "Epoch 003 Batch 017 Loss: 1.2144\n",
      "Epoch 003 Batch 018 Loss: 0.9063\n",
      "Epoch 003 Batch 019 Loss: 1.3416\n",
      "Epoch 003 Batch 020 Loss: 1.1894\n",
      "Epoch 003 Batch 021 Loss: 1.0534\n",
      "Epoch 003 Batch 022 Loss: 1.1621\n",
      "Epoch 003 Batch 023 Loss: 1.2351\n",
      "Epoch 003 Batch 024 Loss: 1.2470\n",
      "Epoch 003 Batch 025 Loss: 1.1906\n",
      "Epoch 003 Batch 026 Loss: 0.9376\n",
      "Epoch 003 Batch 027 Loss: 1.0782\n",
      "Epoch 003 Batch 028 Loss: 1.2069\n",
      "Epoch 003 Batch 029 Loss: 1.1166\n",
      "Epoch 003 Batch 030 Loss: 0.9537\n",
      "Epoch 003 Batch 031 Loss: 1.1512\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5571428537368774\n",
      "-----------------------*-----------------------\n",
      "Epoch 004 Batch 000 Loss: 1.2415\n",
      "Epoch 004 Batch 001 Loss: 1.0091\n",
      "Epoch 004 Batch 002 Loss: 1.2214\n",
      "Epoch 004 Batch 003 Loss: 1.0892\n",
      "Epoch 004 Batch 004 Loss: 1.1528\n",
      "Epoch 004 Batch 005 Loss: 1.2017\n",
      "Epoch 004 Batch 006 Loss: 1.0120\n",
      "Epoch 004 Batch 007 Loss: 0.9960\n",
      "Epoch 004 Batch 008 Loss: 1.0789\n",
      "Epoch 004 Batch 009 Loss: 1.1414\n",
      "Epoch 004 Batch 010 Loss: 1.0568\n",
      "Epoch 004 Batch 011 Loss: 1.1860\n",
      "Epoch 004 Batch 012 Loss: 1.2577\n",
      "Epoch 004 Batch 013 Loss: 1.1379\n",
      "Epoch 004 Batch 014 Loss: 1.2362\n",
      "Epoch 004 Batch 015 Loss: 1.2387\n",
      "Epoch 004 Batch 016 Loss: 1.1540\n",
      "Epoch 004 Batch 017 Loss: 1.3041\n",
      "Epoch 004 Batch 018 Loss: 0.8797\n",
      "Epoch 004 Batch 019 Loss: 0.8843\n",
      "Epoch 004 Batch 020 Loss: 1.1516\n",
      "Epoch 004 Batch 021 Loss: 1.0578\n",
      "Epoch 004 Batch 022 Loss: 0.9151\n",
      "Epoch 004 Batch 023 Loss: 1.4246\n",
      "Epoch 004 Batch 024 Loss: 0.9815\n",
      "Epoch 004 Batch 025 Loss: 0.8934\n",
      "Epoch 004 Batch 026 Loss: 0.8596\n",
      "Epoch 004 Batch 027 Loss: 0.9573\n",
      "Epoch 004 Batch 028 Loss: 1.2652\n",
      "Epoch 004 Batch 029 Loss: 0.9175\n",
      "Epoch 004 Batch 030 Loss: 0.9206\n",
      "Epoch 004 Batch 031 Loss: 0.8317\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6214285492897034\n",
      "-----------------------*-----------------------\n",
      "Epoch 005 Batch 000 Loss: 1.0990\n",
      "Epoch 005 Batch 001 Loss: 1.0150\n",
      "Epoch 005 Batch 002 Loss: 1.1290\n",
      "Epoch 005 Batch 003 Loss: 0.6870\n",
      "Epoch 005 Batch 004 Loss: 0.9485\n",
      "Epoch 005 Batch 005 Loss: 0.8797\n",
      "Epoch 005 Batch 006 Loss: 1.0109\n",
      "Epoch 005 Batch 007 Loss: 0.7788\n",
      "Epoch 005 Batch 008 Loss: 0.8478\n",
      "Epoch 005 Batch 009 Loss: 0.9677\n",
      "Epoch 005 Batch 010 Loss: 0.7442\n",
      "Epoch 005 Batch 011 Loss: 0.9688\n",
      "Epoch 005 Batch 012 Loss: 0.9474\n",
      "Epoch 005 Batch 013 Loss: 0.6403\n",
      "Epoch 005 Batch 014 Loss: 0.8069\n",
      "Epoch 005 Batch 015 Loss: 0.9704\n",
      "Epoch 005 Batch 016 Loss: 0.7191\n",
      "Epoch 005 Batch 017 Loss: 0.9032\n",
      "Epoch 005 Batch 018 Loss: 1.0998\n",
      "Epoch 005 Batch 019 Loss: 0.8202\n",
      "Epoch 005 Batch 020 Loss: 0.7587\n",
      "Epoch 005 Batch 021 Loss: 1.1119\n",
      "Epoch 005 Batch 022 Loss: 0.5965\n",
      "Epoch 005 Batch 023 Loss: 0.9270\n",
      "Epoch 005 Batch 024 Loss: 0.7654\n",
      "Epoch 005 Batch 025 Loss: 0.8815\n",
      "Epoch 005 Batch 026 Loss: 0.7316\n",
      "Epoch 005 Batch 027 Loss: 1.0514\n",
      "Epoch 005 Batch 028 Loss: 0.6983\n",
      "Epoch 005 Batch 029 Loss: 0.7747\n",
      "Epoch 005 Batch 030 Loss: 0.7886\n",
      "Epoch 005 Batch 031 Loss: 0.9275\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6714285612106323\n",
      "-----------------------*-----------------------\n",
      "Epoch 006 Batch 000 Loss: 0.8602\n",
      "Epoch 006 Batch 001 Loss: 0.5826\n",
      "Epoch 006 Batch 002 Loss: 0.6600\n",
      "Epoch 006 Batch 003 Loss: 0.5886\n",
      "Epoch 006 Batch 004 Loss: 0.8245\n",
      "Epoch 006 Batch 005 Loss: 0.7592\n",
      "Epoch 006 Batch 006 Loss: 0.5970\n",
      "Epoch 006 Batch 007 Loss: 0.9281\n",
      "Epoch 006 Batch 008 Loss: 0.8180\n",
      "Epoch 006 Batch 009 Loss: 0.7279\n",
      "Epoch 006 Batch 010 Loss: 0.6741\n",
      "Epoch 006 Batch 011 Loss: 0.5417\n",
      "Epoch 006 Batch 012 Loss: 0.6515\n",
      "Epoch 006 Batch 013 Loss: 0.6448\n",
      "Epoch 006 Batch 014 Loss: 0.7642\n",
      "Epoch 006 Batch 015 Loss: 0.6500\n",
      "Epoch 006 Batch 016 Loss: 0.8012\n",
      "Epoch 006 Batch 017 Loss: 0.8066\n",
      "Epoch 006 Batch 018 Loss: 0.6140\n",
      "Epoch 006 Batch 019 Loss: 0.6036\n",
      "Epoch 006 Batch 020 Loss: 0.7498\n",
      "Epoch 006 Batch 021 Loss: 0.8350\n",
      "Epoch 006 Batch 022 Loss: 0.7618\n",
      "Epoch 006 Batch 023 Loss: 0.7171\n",
      "Epoch 006 Batch 024 Loss: 0.8090\n",
      "Epoch 006 Batch 025 Loss: 0.6473\n",
      "Epoch 006 Batch 026 Loss: 0.6803\n",
      "Epoch 006 Batch 027 Loss: 0.7320\n",
      "Epoch 006 Batch 028 Loss: 0.6575\n",
      "Epoch 006 Batch 029 Loss: 0.5427\n",
      "Epoch 006 Batch 030 Loss: 0.6168\n",
      "Epoch 006 Batch 031 Loss: 0.8140\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.7142857313156128\n",
      "-----------------------*-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 Batch 000 Loss: 0.7704\n",
      "Epoch 007 Batch 001 Loss: 0.5516\n",
      "Epoch 007 Batch 002 Loss: 0.7092\n",
      "Epoch 007 Batch 003 Loss: 0.7262\n",
      "Epoch 007 Batch 004 Loss: 0.5840\n",
      "Epoch 007 Batch 005 Loss: 0.7649\n",
      "Epoch 007 Batch 006 Loss: 0.7110\n",
      "Epoch 007 Batch 007 Loss: 0.5827\n",
      "Epoch 007 Batch 008 Loss: 0.7422\n",
      "Epoch 007 Batch 009 Loss: 0.4716\n",
      "Epoch 007 Batch 010 Loss: 0.6803\n",
      "Epoch 007 Batch 011 Loss: 0.7611\n",
      "Epoch 007 Batch 012 Loss: 0.4990\n",
      "Epoch 007 Batch 013 Loss: 0.6994\n",
      "Epoch 007 Batch 014 Loss: 0.6018\n",
      "Epoch 007 Batch 015 Loss: 0.7227\n",
      "Epoch 007 Batch 016 Loss: 0.6197\n",
      "Epoch 007 Batch 017 Loss: 0.5511\n",
      "Epoch 007 Batch 018 Loss: 0.6000\n",
      "Epoch 007 Batch 019 Loss: 0.8131\n",
      "Epoch 007 Batch 020 Loss: 0.5545\n",
      "Epoch 007 Batch 021 Loss: 0.7054\n",
      "Epoch 007 Batch 022 Loss: 0.6485\n",
      "Epoch 007 Batch 023 Loss: 0.5040\n",
      "Epoch 007 Batch 024 Loss: 0.7603\n",
      "Epoch 007 Batch 025 Loss: 0.4412\n",
      "Epoch 007 Batch 026 Loss: 0.5798\n",
      "Epoch 007 Batch 027 Loss: 0.5671\n",
      "Epoch 007 Batch 028 Loss: 0.5767\n",
      "Epoch 007 Batch 029 Loss: 0.7498\n",
      "Epoch 007 Batch 030 Loss: 0.4377\n",
      "Epoch 007 Batch 031 Loss: 0.6057\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.7571428418159485\n",
      "-----------------------*-----------------------\n",
      "Epoch 008 Batch 000 Loss: 0.6073\n",
      "Epoch 008 Batch 001 Loss: 0.5439\n",
      "Epoch 008 Batch 002 Loss: 0.5077\n",
      "Epoch 008 Batch 003 Loss: 0.5566\n",
      "Epoch 008 Batch 004 Loss: 0.5885\n",
      "Epoch 008 Batch 005 Loss: 0.4882\n",
      "Epoch 008 Batch 006 Loss: 0.4568\n",
      "Epoch 008 Batch 007 Loss: 0.4975\n",
      "Epoch 008 Batch 008 Loss: 0.3994\n",
      "Epoch 008 Batch 009 Loss: 0.4810\n",
      "Epoch 008 Batch 010 Loss: 0.5380\n",
      "Epoch 008 Batch 011 Loss: 0.4366\n",
      "Epoch 008 Batch 012 Loss: 0.5727\n",
      "Epoch 008 Batch 013 Loss: 0.5787\n",
      "Epoch 008 Batch 014 Loss: 0.3704\n",
      "Epoch 008 Batch 015 Loss: 0.6779\n",
      "Epoch 008 Batch 016 Loss: 0.5424\n",
      "Epoch 008 Batch 017 Loss: 0.5575\n",
      "Epoch 008 Batch 018 Loss: 0.3704\n",
      "Epoch 008 Batch 019 Loss: 0.4030\n",
      "Epoch 008 Batch 020 Loss: 0.5362\n",
      "Epoch 008 Batch 021 Loss: 0.5380\n",
      "Epoch 008 Batch 022 Loss: 0.6003\n",
      "Epoch 008 Batch 023 Loss: 0.3614\n",
      "Epoch 008 Batch 024 Loss: 0.3410\n",
      "Epoch 008 Batch 025 Loss: 0.5808\n",
      "Epoch 008 Batch 026 Loss: 0.5580\n",
      "Epoch 008 Batch 027 Loss: 0.5932\n",
      "Epoch 008 Batch 028 Loss: 0.3917\n",
      "Epoch 008 Batch 029 Loss: 0.5051\n",
      "Epoch 008 Batch 030 Loss: 0.3143\n",
      "Epoch 008 Batch 031 Loss: 0.5075\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.7571428418159485\n",
      "-----------------------*-----------------------\n",
      "Epoch 009 Batch 000 Loss: 0.5399\n",
      "Epoch 009 Batch 001 Loss: 0.6053\n",
      "Epoch 009 Batch 002 Loss: 0.4466\n",
      "Epoch 009 Batch 003 Loss: 0.4974\n",
      "Epoch 009 Batch 004 Loss: 0.3881\n",
      "Epoch 009 Batch 005 Loss: 0.5959\n",
      "Epoch 009 Batch 006 Loss: 0.4532\n",
      "Epoch 009 Batch 007 Loss: 0.3935\n",
      "Epoch 009 Batch 008 Loss: 0.6198\n",
      "Epoch 009 Batch 009 Loss: 0.5335\n",
      "Epoch 009 Batch 010 Loss: 0.4647\n",
      "Epoch 009 Batch 011 Loss: 0.3780\n",
      "Epoch 009 Batch 012 Loss: 0.4458\n",
      "Epoch 009 Batch 013 Loss: 0.2606\n",
      "Epoch 009 Batch 014 Loss: 0.4956\n",
      "Epoch 009 Batch 015 Loss: 0.4756\n",
      "Epoch 009 Batch 016 Loss: 0.6212\n",
      "Epoch 009 Batch 017 Loss: 0.3620\n",
      "Epoch 009 Batch 018 Loss: 0.5024\n",
      "Epoch 009 Batch 019 Loss: 0.4475\n",
      "Epoch 009 Batch 020 Loss: 0.4445\n",
      "Epoch 009 Batch 021 Loss: 0.5043\n",
      "Epoch 009 Batch 022 Loss: 0.4301\n",
      "Epoch 009 Batch 023 Loss: 0.4980\n",
      "Epoch 009 Batch 024 Loss: 0.4043\n",
      "Epoch 009 Batch 025 Loss: 0.3509\n",
      "Epoch 009 Batch 026 Loss: 0.3921\n",
      "Epoch 009 Batch 027 Loss: 0.3905\n",
      "Epoch 009 Batch 028 Loss: 0.3302\n",
      "Epoch 009 Batch 029 Loss: 0.4165\n",
      "Epoch 009 Batch 030 Loss: 0.5849\n",
      "Epoch 009 Batch 031 Loss: 0.4758\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.7571428418159485\n",
      "-----------------------*-----------------------\n",
      "Epoch 010 Batch 000 Loss: 0.4233\n",
      "Epoch 010 Batch 001 Loss: 0.4432\n",
      "Epoch 010 Batch 002 Loss: 0.4971\n",
      "Epoch 010 Batch 003 Loss: 0.3111\n",
      "Epoch 010 Batch 004 Loss: 0.5022\n",
      "Epoch 010 Batch 005 Loss: 0.4907\n",
      "Epoch 010 Batch 006 Loss: 0.3898\n",
      "Epoch 010 Batch 007 Loss: 0.4953\n",
      "Epoch 010 Batch 008 Loss: 0.5154\n",
      "Epoch 010 Batch 009 Loss: 0.4764\n",
      "Epoch 010 Batch 010 Loss: 0.3314\n",
      "Epoch 010 Batch 011 Loss: 0.3968\n",
      "Epoch 010 Batch 012 Loss: 0.3255\n",
      "Epoch 010 Batch 013 Loss: 0.3648\n",
      "Epoch 010 Batch 014 Loss: 0.4147\n",
      "Epoch 010 Batch 015 Loss: 0.3500\n",
      "Epoch 010 Batch 016 Loss: 0.4662\n",
      "Epoch 010 Batch 017 Loss: 0.3962\n",
      "Epoch 010 Batch 018 Loss: 0.3908\n",
      "Epoch 010 Batch 019 Loss: 0.2429\n",
      "Epoch 010 Batch 020 Loss: 0.4008\n",
      "Epoch 010 Batch 021 Loss: 0.4242\n",
      "Epoch 010 Batch 022 Loss: 0.4582\n",
      "Epoch 010 Batch 023 Loss: 0.4482\n",
      "Epoch 010 Batch 024 Loss: 0.4785\n",
      "Epoch 010 Batch 025 Loss: 0.4635\n",
      "Epoch 010 Batch 026 Loss: 0.4103\n",
      "Epoch 010 Batch 027 Loss: 0.2116\n",
      "Epoch 010 Batch 028 Loss: 0.2479\n",
      "Epoch 010 Batch 029 Loss: 0.2950\n",
      "Epoch 010 Batch 030 Loss: 0.4346\n",
      "Epoch 010 Batch 031 Loss: 0.3196\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.7785714268684387\n",
      "-----------------------*-----------------------\n",
      "Epoch 011 Batch 000 Loss: 0.3713\n",
      "Epoch 011 Batch 001 Loss: 0.3296\n",
      "Epoch 011 Batch 002 Loss: 0.4089\n",
      "Epoch 011 Batch 003 Loss: 0.3453\n",
      "Epoch 011 Batch 004 Loss: 0.2507\n",
      "Epoch 011 Batch 005 Loss: 0.3697\n",
      "Epoch 011 Batch 006 Loss: 0.3742\n",
      "Epoch 011 Batch 007 Loss: 0.3326\n",
      "Epoch 011 Batch 008 Loss: 0.3396\n",
      "Epoch 011 Batch 009 Loss: 0.4280\n",
      "Epoch 011 Batch 010 Loss: 0.3875\n",
      "Epoch 011 Batch 011 Loss: 0.3385\n",
      "Epoch 011 Batch 012 Loss: 0.3660\n",
      "Epoch 011 Batch 013 Loss: 0.2784\n",
      "Epoch 011 Batch 014 Loss: 0.3500\n",
      "Epoch 011 Batch 015 Loss: 0.4337\n",
      "Epoch 011 Batch 016 Loss: 0.3418\n",
      "Epoch 011 Batch 017 Loss: 0.2485\n",
      "Epoch 011 Batch 018 Loss: 0.2309\n",
      "Epoch 011 Batch 019 Loss: 0.3317\n",
      "Epoch 011 Batch 020 Loss: 0.2802\n",
      "Epoch 011 Batch 021 Loss: 0.2500\n",
      "Epoch 011 Batch 022 Loss: 0.4271\n",
      "Epoch 011 Batch 023 Loss: 0.2967\n",
      "Epoch 011 Batch 024 Loss: 0.2683\n",
      "Epoch 011 Batch 025 Loss: 0.2209\n",
      "Epoch 011 Batch 026 Loss: 0.4116\n",
      "Epoch 011 Batch 027 Loss: 0.3015\n",
      "Epoch 011 Batch 028 Loss: 0.3599\n",
      "Epoch 011 Batch 029 Loss: 0.3185\n",
      "Epoch 011 Batch 030 Loss: 0.3908\n",
      "Epoch 011 Batch 031 Loss: 0.4043\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.800000011920929\n",
      "-----------------------*-----------------------\n",
      "Epoch 012 Batch 000 Loss: 0.1540\n",
      "Epoch 012 Batch 001 Loss: 0.2777\n",
      "Epoch 012 Batch 002 Loss: 0.2834\n",
      "Epoch 012 Batch 003 Loss: 0.3070\n",
      "Epoch 012 Batch 004 Loss: 0.3606\n",
      "Epoch 012 Batch 005 Loss: 0.3221\n",
      "Epoch 012 Batch 006 Loss: 0.3546\n",
      "Epoch 012 Batch 007 Loss: 0.4513\n",
      "Epoch 012 Batch 008 Loss: 0.3652\n",
      "Epoch 012 Batch 009 Loss: 0.3501\n",
      "Epoch 012 Batch 010 Loss: 0.2661\n",
      "Epoch 012 Batch 011 Loss: 0.5111\n",
      "Epoch 012 Batch 012 Loss: 0.3311\n",
      "Epoch 012 Batch 013 Loss: 0.3566\n",
      "Epoch 012 Batch 014 Loss: 0.3732\n",
      "Epoch 012 Batch 015 Loss: 0.1950\n",
      "Epoch 012 Batch 016 Loss: 0.3139\n",
      "Epoch 012 Batch 017 Loss: 0.2100\n",
      "Epoch 012 Batch 018 Loss: 0.3745\n",
      "Epoch 012 Batch 019 Loss: 0.2775\n",
      "Epoch 012 Batch 020 Loss: 0.3245\n",
      "Epoch 012 Batch 021 Loss: 0.2343\n",
      "Epoch 012 Batch 022 Loss: 0.2200\n",
      "Epoch 012 Batch 023 Loss: 0.2192\n",
      "Epoch 012 Batch 024 Loss: 0.2327\n",
      "Epoch 012 Batch 025 Loss: 0.2548\n",
      "Epoch 012 Batch 026 Loss: 0.4930\n",
      "Epoch 012 Batch 027 Loss: 0.2984\n",
      "Epoch 012 Batch 028 Loss: 0.2650\n",
      "Epoch 012 Batch 029 Loss: 0.2166\n",
      "Epoch 012 Batch 030 Loss: 0.3656\n",
      "Epoch 012 Batch 031 Loss: 0.2876\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 013 Batch 000 Loss: 0.3029\n",
      "Epoch 013 Batch 001 Loss: 0.2769\n",
      "Epoch 013 Batch 002 Loss: 0.1840\n",
      "Epoch 013 Batch 003 Loss: 0.2306\n",
      "Epoch 013 Batch 004 Loss: 0.3527\n",
      "Epoch 013 Batch 005 Loss: 0.2999\n",
      "Epoch 013 Batch 006 Loss: 0.3118\n",
      "Epoch 013 Batch 007 Loss: 0.3399\n",
      "Epoch 013 Batch 008 Loss: 0.2422\n",
      "Epoch 013 Batch 009 Loss: 0.3276\n",
      "Epoch 013 Batch 010 Loss: 0.3006\n",
      "Epoch 013 Batch 011 Loss: 0.3413\n",
      "Epoch 013 Batch 012 Loss: 0.2154\n",
      "Epoch 013 Batch 013 Loss: 0.3447\n",
      "Epoch 013 Batch 014 Loss: 0.3250\n",
      "Epoch 013 Batch 015 Loss: 0.3048\n",
      "Epoch 013 Batch 016 Loss: 0.2181\n",
      "Epoch 013 Batch 017 Loss: 0.3369\n",
      "Epoch 013 Batch 018 Loss: 0.2624\n",
      "Epoch 013 Batch 019 Loss: 0.2365\n",
      "Epoch 013 Batch 020 Loss: 0.3412\n",
      "Epoch 013 Batch 021 Loss: 0.3187\n",
      "Epoch 013 Batch 022 Loss: 0.2489\n",
      "Epoch 013 Batch 023 Loss: 0.2003\n",
      "Epoch 013 Batch 024 Loss: 0.2524\n",
      "Epoch 013 Batch 025 Loss: 0.2759\n",
      "Epoch 013 Batch 026 Loss: 0.2790\n",
      "Epoch 013 Batch 027 Loss: 0.2585\n",
      "Epoch 013 Batch 028 Loss: 0.3323\n",
      "Epoch 013 Batch 029 Loss: 0.3230\n",
      "Epoch 013 Batch 030 Loss: 0.2022\n",
      "Epoch 013 Batch 031 Loss: 0.3327\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 Batch 000 Loss: 0.1723\n",
      "Epoch 014 Batch 001 Loss: 0.2446\n",
      "Epoch 014 Batch 002 Loss: 0.2148\n",
      "Epoch 014 Batch 003 Loss: 0.2099\n",
      "Epoch 014 Batch 004 Loss: 0.2443\n",
      "Epoch 014 Batch 005 Loss: 0.2844\n",
      "Epoch 014 Batch 006 Loss: 0.2938\n",
      "Epoch 014 Batch 007 Loss: 0.2522\n",
      "Epoch 014 Batch 008 Loss: 0.2301\n",
      "Epoch 014 Batch 009 Loss: 0.1906\n",
      "Epoch 014 Batch 010 Loss: 0.2854\n",
      "Epoch 014 Batch 011 Loss: 0.1978\n",
      "Epoch 014 Batch 012 Loss: 0.3398\n",
      "Epoch 014 Batch 013 Loss: 0.2952\n",
      "Epoch 014 Batch 014 Loss: 0.2285\n",
      "Epoch 014 Batch 015 Loss: 0.2153\n",
      "Epoch 014 Batch 016 Loss: 0.3226\n",
      "Epoch 014 Batch 017 Loss: 0.3211\n",
      "Epoch 014 Batch 018 Loss: 0.2460\n",
      "Epoch 014 Batch 019 Loss: 0.2076\n",
      "Epoch 014 Batch 020 Loss: 0.3138\n",
      "Epoch 014 Batch 021 Loss: 0.2266\n",
      "Epoch 014 Batch 022 Loss: 0.1413\n",
      "Epoch 014 Batch 023 Loss: 0.1895\n",
      "Epoch 014 Batch 024 Loss: 0.2068\n",
      "Epoch 014 Batch 025 Loss: 0.2969\n",
      "Epoch 014 Batch 026 Loss: 0.2600\n",
      "Epoch 014 Batch 027 Loss: 0.3257\n",
      "Epoch 014 Batch 028 Loss: 0.2738\n",
      "Epoch 014 Batch 029 Loss: 0.2432\n",
      "Epoch 014 Batch 030 Loss: 0.2305\n",
      "Epoch 014 Batch 031 Loss: 0.2131\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8214285969734192\n",
      "-----------------------*-----------------------\n",
      "Epoch 015 Batch 000 Loss: 0.2396\n",
      "Epoch 015 Batch 001 Loss: 0.1764\n",
      "Epoch 015 Batch 002 Loss: 0.2144\n",
      "Epoch 015 Batch 003 Loss: 0.1814\n",
      "Epoch 015 Batch 004 Loss: 0.2201\n",
      "Epoch 015 Batch 005 Loss: 0.2240\n",
      "Epoch 015 Batch 006 Loss: 0.2602\n",
      "Epoch 015 Batch 007 Loss: 0.1642\n",
      "Epoch 015 Batch 008 Loss: 0.2182\n",
      "Epoch 015 Batch 009 Loss: 0.2068\n",
      "Epoch 015 Batch 010 Loss: 0.2522\n",
      "Epoch 015 Batch 011 Loss: 0.2547\n",
      "Epoch 015 Batch 012 Loss: 0.3310\n",
      "Epoch 015 Batch 013 Loss: 0.2113\n",
      "Epoch 015 Batch 014 Loss: 0.1868\n",
      "Epoch 015 Batch 015 Loss: 0.2763\n",
      "Epoch 015 Batch 016 Loss: 0.1866\n",
      "Epoch 015 Batch 017 Loss: 0.1819\n",
      "Epoch 015 Batch 018 Loss: 0.2455\n",
      "Epoch 015 Batch 019 Loss: 0.2221\n",
      "Epoch 015 Batch 020 Loss: 0.2833\n",
      "Epoch 015 Batch 021 Loss: 0.2112\n",
      "Epoch 015 Batch 022 Loss: 0.1999\n",
      "Epoch 015 Batch 023 Loss: 0.1527\n",
      "Epoch 015 Batch 024 Loss: 0.2409\n",
      "Epoch 015 Batch 025 Loss: 0.3208\n",
      "Epoch 015 Batch 026 Loss: 0.2746\n",
      "Epoch 015 Batch 027 Loss: 0.1942\n",
      "Epoch 015 Batch 028 Loss: 0.2054\n",
      "Epoch 015 Batch 029 Loss: 0.1501\n",
      "Epoch 015 Batch 030 Loss: 0.2582\n",
      "Epoch 015 Batch 031 Loss: 0.3311\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8214285969734192\n",
      "-----------------------*-----------------------\n",
      "Epoch 016 Batch 000 Loss: 0.2005\n",
      "Epoch 016 Batch 001 Loss: 0.1922\n",
      "Epoch 016 Batch 002 Loss: 0.1964\n",
      "Epoch 016 Batch 003 Loss: 0.2153\n",
      "Epoch 016 Batch 004 Loss: 0.1785\n",
      "Epoch 016 Batch 005 Loss: 0.1743\n",
      "Epoch 016 Batch 006 Loss: 0.1848\n",
      "Epoch 016 Batch 007 Loss: 0.2905\n",
      "Epoch 016 Batch 008 Loss: 0.1356\n",
      "Epoch 016 Batch 009 Loss: 0.1827\n",
      "Epoch 016 Batch 010 Loss: 0.2573\n",
      "Epoch 016 Batch 011 Loss: 0.1623\n",
      "Epoch 016 Batch 012 Loss: 0.2369\n",
      "Epoch 016 Batch 013 Loss: 0.1657\n",
      "Epoch 016 Batch 014 Loss: 0.1686\n",
      "Epoch 016 Batch 015 Loss: 0.1400\n",
      "Epoch 016 Batch 016 Loss: 0.1701\n",
      "Epoch 016 Batch 017 Loss: 0.2199\n",
      "Epoch 016 Batch 018 Loss: 0.2261\n",
      "Epoch 016 Batch 019 Loss: 0.2451\n",
      "Epoch 016 Batch 020 Loss: 0.1213\n",
      "Epoch 016 Batch 021 Loss: 0.2212\n",
      "Epoch 016 Batch 022 Loss: 0.1101\n",
      "Epoch 016 Batch 023 Loss: 0.1432\n",
      "Epoch 016 Batch 024 Loss: 0.2188\n",
      "Epoch 016 Batch 025 Loss: 0.1426\n",
      "Epoch 016 Batch 026 Loss: 0.1801\n",
      "Epoch 016 Batch 027 Loss: 0.2055\n",
      "Epoch 016 Batch 028 Loss: 0.2872\n",
      "Epoch 016 Batch 029 Loss: 0.1636\n",
      "Epoch 016 Batch 030 Loss: 0.2244\n",
      "Epoch 016 Batch 031 Loss: 0.1629\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8428571224212646\n",
      "-----------------------*-----------------------\n",
      "Epoch 017 Batch 000 Loss: 0.1326\n",
      "Epoch 017 Batch 001 Loss: 0.1838\n",
      "Epoch 017 Batch 002 Loss: 0.1491\n",
      "Epoch 017 Batch 003 Loss: 0.1700\n",
      "Epoch 017 Batch 004 Loss: 0.2153\n",
      "Epoch 017 Batch 005 Loss: 0.1316\n",
      "Epoch 017 Batch 006 Loss: 0.1927\n",
      "Epoch 017 Batch 007 Loss: 0.2116\n",
      "Epoch 017 Batch 008 Loss: 0.1340\n",
      "Epoch 017 Batch 009 Loss: 0.2428\n",
      "Epoch 017 Batch 010 Loss: 0.2200\n",
      "Epoch 017 Batch 011 Loss: 0.3212\n",
      "Epoch 017 Batch 012 Loss: 0.2146\n",
      "Epoch 017 Batch 013 Loss: 0.1670\n",
      "Epoch 017 Batch 014 Loss: 0.2247\n",
      "Epoch 017 Batch 015 Loss: 0.1854\n",
      "Epoch 017 Batch 016 Loss: 0.1076\n",
      "Epoch 017 Batch 017 Loss: 0.1130\n",
      "Epoch 017 Batch 018 Loss: 0.1881\n",
      "Epoch 017 Batch 019 Loss: 0.3193\n",
      "Epoch 017 Batch 020 Loss: 0.1277\n",
      "Epoch 017 Batch 021 Loss: 0.1477\n",
      "Epoch 017 Batch 022 Loss: 0.1677\n",
      "Epoch 017 Batch 023 Loss: 0.1858\n",
      "Epoch 017 Batch 024 Loss: 0.1710\n",
      "Epoch 017 Batch 025 Loss: 0.1091\n",
      "Epoch 017 Batch 026 Loss: 0.1955\n",
      "Epoch 017 Batch 027 Loss: 0.2140\n",
      "Epoch 017 Batch 028 Loss: 0.2120\n",
      "Epoch 017 Batch 029 Loss: 0.2059\n",
      "Epoch 017 Batch 030 Loss: 0.1250\n",
      "Epoch 017 Batch 031 Loss: 0.1575\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8142856955528259\n",
      "-----------------------*-----------------------\n",
      "Epoch 018 Batch 000 Loss: 0.1185\n",
      "Epoch 018 Batch 001 Loss: 0.2065\n",
      "Epoch 018 Batch 002 Loss: 0.1699\n",
      "Epoch 018 Batch 003 Loss: 0.1846\n",
      "Epoch 018 Batch 004 Loss: 0.1278\n",
      "Epoch 018 Batch 005 Loss: 0.2165\n",
      "Epoch 018 Batch 006 Loss: 0.1217\n",
      "Epoch 018 Batch 007 Loss: 0.1566\n",
      "Epoch 018 Batch 008 Loss: 0.1235\n",
      "Epoch 018 Batch 009 Loss: 0.1519\n",
      "Epoch 018 Batch 010 Loss: 0.1024\n",
      "Epoch 018 Batch 011 Loss: 0.1504\n",
      "Epoch 018 Batch 012 Loss: 0.2109\n",
      "Epoch 018 Batch 013 Loss: 0.1505\n",
      "Epoch 018 Batch 014 Loss: 0.1505\n",
      "Epoch 018 Batch 015 Loss: 0.1175\n",
      "Epoch 018 Batch 016 Loss: 0.2651\n",
      "Epoch 018 Batch 017 Loss: 0.1626\n",
      "Epoch 018 Batch 018 Loss: 0.1387\n",
      "Epoch 018 Batch 019 Loss: 0.1612\n",
      "Epoch 018 Batch 020 Loss: 0.1482\n",
      "Epoch 018 Batch 021 Loss: 0.1479\n",
      "Epoch 018 Batch 022 Loss: 0.1776\n",
      "Epoch 018 Batch 023 Loss: 0.3350\n",
      "Epoch 018 Batch 024 Loss: 0.2325\n",
      "Epoch 018 Batch 025 Loss: 0.1627\n",
      "Epoch 018 Batch 026 Loss: 0.0909\n",
      "Epoch 018 Batch 027 Loss: 0.1356\n",
      "Epoch 018 Batch 028 Loss: 0.1618\n",
      "Epoch 018 Batch 029 Loss: 0.1389\n",
      "Epoch 018 Batch 030 Loss: 0.0595\n",
      "Epoch 018 Batch 031 Loss: 0.1472\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8428571224212646\n",
      "-----------------------*-----------------------\n",
      "Epoch 019 Batch 000 Loss: 0.1289\n",
      "Epoch 019 Batch 001 Loss: 0.1548\n",
      "Epoch 019 Batch 002 Loss: 0.1454\n",
      "Epoch 019 Batch 003 Loss: 0.1521\n",
      "Epoch 019 Batch 004 Loss: 0.2073\n",
      "Epoch 019 Batch 005 Loss: 0.1411\n",
      "Epoch 019 Batch 006 Loss: 0.0828\n",
      "Epoch 019 Batch 007 Loss: 0.1154\n",
      "Epoch 019 Batch 008 Loss: 0.1154\n",
      "Epoch 019 Batch 009 Loss: 0.1041\n",
      "Epoch 019 Batch 010 Loss: 0.2276\n",
      "Epoch 019 Batch 011 Loss: 0.1556\n",
      "Epoch 019 Batch 012 Loss: 0.0971\n",
      "Epoch 019 Batch 013 Loss: 0.0688\n",
      "Epoch 019 Batch 014 Loss: 0.1679\n",
      "Epoch 019 Batch 015 Loss: 0.1805\n",
      "Epoch 019 Batch 016 Loss: 0.1389\n",
      "Epoch 019 Batch 017 Loss: 0.1411\n",
      "Epoch 019 Batch 018 Loss: 0.2125\n",
      "Epoch 019 Batch 019 Loss: 0.2085\n",
      "Epoch 019 Batch 020 Loss: 0.1172\n",
      "Epoch 019 Batch 021 Loss: 0.1614\n",
      "Epoch 019 Batch 022 Loss: 0.0962\n",
      "Epoch 019 Batch 023 Loss: 0.1723\n",
      "Epoch 019 Batch 024 Loss: 0.1709\n",
      "Epoch 019 Batch 025 Loss: 0.1596\n",
      "Epoch 019 Batch 026 Loss: 0.0878\n",
      "Epoch 019 Batch 027 Loss: 0.1060\n",
      "Epoch 019 Batch 028 Loss: 0.0895\n",
      "Epoch 019 Batch 029 Loss: 0.1091\n",
      "Epoch 019 Batch 030 Loss: 0.1879\n",
      "Epoch 019 Batch 031 Loss: 0.1011\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 020 Batch 000 Loss: 0.0774\n",
      "Epoch 020 Batch 001 Loss: 0.0843\n",
      "Epoch 020 Batch 002 Loss: 0.0890\n",
      "Epoch 020 Batch 003 Loss: 0.1500\n",
      "Epoch 020 Batch 004 Loss: 0.1325\n",
      "Epoch 020 Batch 005 Loss: 0.1156\n",
      "Epoch 020 Batch 006 Loss: 0.1151\n",
      "Epoch 020 Batch 007 Loss: 0.1822\n",
      "Epoch 020 Batch 008 Loss: 0.1781\n",
      "Epoch 020 Batch 009 Loss: 0.0836\n",
      "Epoch 020 Batch 010 Loss: 0.1690\n",
      "Epoch 020 Batch 011 Loss: 0.1150\n",
      "Epoch 020 Batch 012 Loss: 0.1073\n",
      "Epoch 020 Batch 013 Loss: 0.2207\n",
      "Epoch 020 Batch 014 Loss: 0.0992\n",
      "Epoch 020 Batch 015 Loss: 0.1207\n",
      "Epoch 020 Batch 016 Loss: 0.1336\n",
      "Epoch 020 Batch 017 Loss: 0.1023\n",
      "Epoch 020 Batch 018 Loss: 0.1673\n",
      "Epoch 020 Batch 019 Loss: 0.1845\n",
      "Epoch 020 Batch 020 Loss: 0.1160\n",
      "Epoch 020 Batch 021 Loss: 0.1113\n",
      "Epoch 020 Batch 022 Loss: 0.0897\n",
      "Epoch 020 Batch 023 Loss: 0.0829\n",
      "Epoch 020 Batch 024 Loss: 0.1165\n",
      "Epoch 020 Batch 025 Loss: 0.1681\n",
      "Epoch 020 Batch 026 Loss: 0.0944\n",
      "Epoch 020 Batch 027 Loss: 0.1099\n",
      "Epoch 020 Batch 028 Loss: 0.0900\n",
      "Epoch 020 Batch 029 Loss: 0.0865\n",
      "Epoch 020 Batch 030 Loss: 0.2065\n",
      "Epoch 020 Batch 031 Loss: 0.1085\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8214285969734192\n",
      "-----------------------*-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 Batch 000 Loss: 0.1655\n",
      "Epoch 021 Batch 001 Loss: 0.1434\n",
      "Epoch 021 Batch 002 Loss: 0.0839\n",
      "Epoch 021 Batch 003 Loss: 0.1347\n",
      "Epoch 021 Batch 004 Loss: 0.0991\n",
      "Epoch 021 Batch 005 Loss: 0.0903\n",
      "Epoch 021 Batch 006 Loss: 0.1000\n",
      "Epoch 021 Batch 007 Loss: 0.0809\n",
      "Epoch 021 Batch 008 Loss: 0.0943\n",
      "Epoch 021 Batch 009 Loss: 0.0917\n",
      "Epoch 021 Batch 010 Loss: 0.1031\n",
      "Epoch 021 Batch 011 Loss: 0.1026\n",
      "Epoch 021 Batch 012 Loss: 0.1891\n",
      "Epoch 021 Batch 013 Loss: 0.1289\n",
      "Epoch 021 Batch 014 Loss: 0.0796\n",
      "Epoch 021 Batch 015 Loss: 0.2134\n",
      "Epoch 021 Batch 016 Loss: 0.1108\n",
      "Epoch 021 Batch 017 Loss: 0.1574\n",
      "Epoch 021 Batch 018 Loss: 0.0994\n",
      "Epoch 021 Batch 019 Loss: 0.1226\n",
      "Epoch 021 Batch 020 Loss: 0.0839\n",
      "Epoch 021 Batch 021 Loss: 0.0912\n",
      "Epoch 021 Batch 022 Loss: 0.1328\n",
      "Epoch 021 Batch 023 Loss: 0.1345\n",
      "Epoch 021 Batch 024 Loss: 0.1057\n",
      "Epoch 021 Batch 025 Loss: 0.1054\n",
      "Epoch 021 Batch 026 Loss: 0.1562\n",
      "Epoch 021 Batch 027 Loss: 0.0826\n",
      "Epoch 021 Batch 028 Loss: 0.0987\n",
      "Epoch 021 Batch 029 Loss: 0.1721\n",
      "Epoch 021 Batch 030 Loss: 0.0710\n",
      "Epoch 021 Batch 031 Loss: 0.1029\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 022 Batch 000 Loss: 0.1017\n",
      "Epoch 022 Batch 001 Loss: 0.1027\n",
      "Epoch 022 Batch 002 Loss: 0.0932\n",
      "Epoch 022 Batch 003 Loss: 0.0843\n",
      "Epoch 022 Batch 004 Loss: 0.1129\n",
      "Epoch 022 Batch 005 Loss: 0.0793\n",
      "Epoch 022 Batch 006 Loss: 0.0828\n",
      "Epoch 022 Batch 007 Loss: 0.1105\n",
      "Epoch 022 Batch 008 Loss: 0.1005\n",
      "Epoch 022 Batch 009 Loss: 0.1882\n",
      "Epoch 022 Batch 010 Loss: 0.1024\n",
      "Epoch 022 Batch 011 Loss: 0.1341\n",
      "Epoch 022 Batch 012 Loss: 0.0949\n",
      "Epoch 022 Batch 013 Loss: 0.1016\n",
      "Epoch 022 Batch 014 Loss: 0.0877\n",
      "Epoch 022 Batch 015 Loss: 0.0954\n",
      "Epoch 022 Batch 016 Loss: 0.0769\n",
      "Epoch 022 Batch 017 Loss: 0.1159\n",
      "Epoch 022 Batch 018 Loss: 0.0851\n",
      "Epoch 022 Batch 019 Loss: 0.1209\n",
      "Epoch 022 Batch 020 Loss: 0.0791\n",
      "Epoch 022 Batch 021 Loss: 0.1348\n",
      "Epoch 022 Batch 022 Loss: 0.1458\n",
      "Epoch 022 Batch 023 Loss: 0.1483\n",
      "Epoch 022 Batch 024 Loss: 0.0633\n",
      "Epoch 022 Batch 025 Loss: 0.1146\n",
      "Epoch 022 Batch 026 Loss: 0.1447\n",
      "Epoch 022 Batch 027 Loss: 0.1158\n",
      "Epoch 022 Batch 028 Loss: 0.0850\n",
      "Epoch 022 Batch 029 Loss: 0.0941\n",
      "Epoch 022 Batch 030 Loss: 0.1003\n",
      "Epoch 022 Batch 031 Loss: 0.0896\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 023 Batch 000 Loss: 0.1651\n",
      "Epoch 023 Batch 001 Loss: 0.1019\n",
      "Epoch 023 Batch 002 Loss: 0.0593\n",
      "Epoch 023 Batch 003 Loss: 0.1143\n",
      "Epoch 023 Batch 004 Loss: 0.1128\n",
      "Epoch 023 Batch 005 Loss: 0.1131\n",
      "Epoch 023 Batch 006 Loss: 0.0695\n",
      "Epoch 023 Batch 007 Loss: 0.1011\n",
      "Epoch 023 Batch 008 Loss: 0.1799\n",
      "Epoch 023 Batch 009 Loss: 0.0900\n",
      "Epoch 023 Batch 010 Loss: 0.1249\n",
      "Epoch 023 Batch 011 Loss: 0.0701\n",
      "Epoch 023 Batch 012 Loss: 0.1749\n",
      "Epoch 023 Batch 013 Loss: 0.1030\n",
      "Epoch 023 Batch 014 Loss: 0.1220\n",
      "Epoch 023 Batch 015 Loss: 0.1557\n",
      "Epoch 023 Batch 016 Loss: 0.1136\n",
      "Epoch 023 Batch 017 Loss: 0.1336\n",
      "Epoch 023 Batch 018 Loss: 0.0727\n",
      "Epoch 023 Batch 019 Loss: 0.0881\n",
      "Epoch 023 Batch 020 Loss: 0.1124\n",
      "Epoch 023 Batch 021 Loss: 0.0942\n",
      "Epoch 023 Batch 022 Loss: 0.0442\n",
      "Epoch 023 Batch 023 Loss: 0.1120\n",
      "Epoch 023 Batch 024 Loss: 0.0854\n",
      "Epoch 023 Batch 025 Loss: 0.1500\n",
      "Epoch 023 Batch 026 Loss: 0.0940\n",
      "Epoch 023 Batch 027 Loss: 0.0597\n",
      "Epoch 023 Batch 028 Loss: 0.0754\n",
      "Epoch 023 Batch 029 Loss: 0.1305\n",
      "Epoch 023 Batch 030 Loss: 0.1042\n",
      "Epoch 023 Batch 031 Loss: 0.0992\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8357142806053162\n",
      "-----------------------*-----------------------\n",
      "Epoch 024 Batch 000 Loss: 0.1122\n",
      "Epoch 024 Batch 001 Loss: 0.1270\n",
      "Epoch 024 Batch 002 Loss: 0.0861\n",
      "Epoch 024 Batch 003 Loss: 0.0581\n",
      "Epoch 024 Batch 004 Loss: 0.0655\n",
      "Epoch 024 Batch 005 Loss: 0.1021\n",
      "Epoch 024 Batch 006 Loss: 0.1262\n",
      "Epoch 024 Batch 007 Loss: 0.1046\n",
      "Epoch 024 Batch 008 Loss: 0.0799\n",
      "Epoch 024 Batch 009 Loss: 0.0346\n",
      "Epoch 024 Batch 010 Loss: 0.0711\n",
      "Epoch 024 Batch 011 Loss: 0.0965\n",
      "Epoch 024 Batch 012 Loss: 0.0695\n",
      "Epoch 024 Batch 013 Loss: 0.1035\n",
      "Epoch 024 Batch 014 Loss: 0.0873\n",
      "Epoch 024 Batch 015 Loss: 0.0833\n",
      "Epoch 024 Batch 016 Loss: 0.1127\n",
      "Epoch 024 Batch 017 Loss: 0.0758\n",
      "Epoch 024 Batch 018 Loss: 0.1291\n",
      "Epoch 024 Batch 019 Loss: 0.0762\n",
      "Epoch 024 Batch 020 Loss: 0.0600\n",
      "Epoch 024 Batch 021 Loss: 0.0888\n",
      "Epoch 024 Batch 022 Loss: 0.0660\n",
      "Epoch 024 Batch 023 Loss: 0.0696\n",
      "Epoch 024 Batch 024 Loss: 0.1196\n",
      "Epoch 024 Batch 025 Loss: 0.0956\n",
      "Epoch 024 Batch 026 Loss: 0.0618\n",
      "Epoch 024 Batch 027 Loss: 0.0790\n",
      "Epoch 024 Batch 028 Loss: 0.1070\n",
      "Epoch 024 Batch 029 Loss: 0.0880\n",
      "Epoch 024 Batch 030 Loss: 0.0612\n",
      "Epoch 024 Batch 031 Loss: 0.0728\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 025 Batch 000 Loss: 0.1247\n",
      "Epoch 025 Batch 001 Loss: 0.0909\n",
      "Epoch 025 Batch 002 Loss: 0.1077\n",
      "Epoch 025 Batch 003 Loss: 0.0817\n",
      "Epoch 025 Batch 004 Loss: 0.0916\n",
      "Epoch 025 Batch 005 Loss: 0.1421\n",
      "Epoch 025 Batch 006 Loss: 0.0510\n",
      "Epoch 025 Batch 007 Loss: 0.0725\n",
      "Epoch 025 Batch 008 Loss: 0.0617\n",
      "Epoch 025 Batch 009 Loss: 0.1130\n",
      "Epoch 025 Batch 010 Loss: 0.0801\n",
      "Epoch 025 Batch 011 Loss: 0.1130\n",
      "Epoch 025 Batch 012 Loss: 0.0904\n",
      "Epoch 025 Batch 013 Loss: 0.1014\n",
      "Epoch 025 Batch 014 Loss: 0.0741\n",
      "Epoch 025 Batch 015 Loss: 0.1087\n",
      "Epoch 025 Batch 016 Loss: 0.0601\n",
      "Epoch 025 Batch 017 Loss: 0.1096\n",
      "Epoch 025 Batch 018 Loss: 0.1050\n",
      "Epoch 025 Batch 019 Loss: 0.1433\n",
      "Epoch 025 Batch 020 Loss: 0.0669\n",
      "Epoch 025 Batch 021 Loss: 0.1107\n",
      "Epoch 025 Batch 022 Loss: 0.1157\n",
      "Epoch 025 Batch 023 Loss: 0.0495\n",
      "Epoch 025 Batch 024 Loss: 0.0861\n",
      "Epoch 025 Batch 025 Loss: 0.1349\n",
      "Epoch 025 Batch 026 Loss: 0.0850\n",
      "Epoch 025 Batch 027 Loss: 0.0777\n",
      "Epoch 025 Batch 028 Loss: 0.0617\n",
      "Epoch 025 Batch 029 Loss: 0.1016\n",
      "Epoch 025 Batch 030 Loss: 0.0650\n",
      "Epoch 025 Batch 031 Loss: 0.1033\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "Epoch 026 Batch 000 Loss: 0.0849\n",
      "Epoch 026 Batch 001 Loss: 0.0634\n",
      "Epoch 026 Batch 002 Loss: 0.0825\n",
      "Epoch 026 Batch 003 Loss: 0.0991\n",
      "Epoch 026 Batch 004 Loss: 0.1007\n",
      "Epoch 026 Batch 005 Loss: 0.0885\n",
      "Epoch 026 Batch 006 Loss: 0.0956\n",
      "Epoch 026 Batch 007 Loss: 0.0686\n",
      "Epoch 026 Batch 008 Loss: 0.0734\n",
      "Epoch 026 Batch 009 Loss: 0.0558\n",
      "Epoch 026 Batch 010 Loss: 0.0634\n",
      "Epoch 026 Batch 011 Loss: 0.0687\n",
      "Epoch 026 Batch 012 Loss: 0.0530\n",
      "Epoch 026 Batch 013 Loss: 0.0475\n",
      "Epoch 026 Batch 014 Loss: 0.0623\n",
      "Epoch 026 Batch 015 Loss: 0.0831\n",
      "Epoch 026 Batch 016 Loss: 0.1064\n",
      "Epoch 026 Batch 017 Loss: 0.1128\n",
      "Epoch 026 Batch 018 Loss: 0.1005\n",
      "Epoch 026 Batch 019 Loss: 0.0547\n",
      "Epoch 026 Batch 020 Loss: 0.0613\n",
      "Epoch 026 Batch 021 Loss: 0.0590\n",
      "Epoch 026 Batch 022 Loss: 0.0633\n",
      "Epoch 026 Batch 023 Loss: 0.0718\n",
      "Epoch 026 Batch 024 Loss: 0.0550\n",
      "Epoch 026 Batch 025 Loss: 0.0547\n",
      "Epoch 026 Batch 026 Loss: 0.0539\n",
      "Epoch 026 Batch 027 Loss: 0.0984\n",
      "Epoch 026 Batch 028 Loss: 0.0732\n",
      "Epoch 026 Batch 029 Loss: 0.0877\n",
      "Epoch 026 Batch 030 Loss: 0.0650\n",
      "Epoch 026 Batch 031 Loss: 0.0708\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8357142806053162\n",
      "-----------------------*-----------------------\n",
      "Epoch 027 Batch 000 Loss: 0.0615\n",
      "Epoch 027 Batch 001 Loss: 0.0937\n",
      "Epoch 027 Batch 002 Loss: 0.0957\n",
      "Epoch 027 Batch 003 Loss: 0.0688\n",
      "Epoch 027 Batch 004 Loss: 0.0570\n",
      "Epoch 027 Batch 005 Loss: 0.0910\n",
      "Epoch 027 Batch 006 Loss: 0.0992\n",
      "Epoch 027 Batch 007 Loss: 0.0818\n",
      "Epoch 027 Batch 008 Loss: 0.0767\n",
      "Epoch 027 Batch 009 Loss: 0.0905\n",
      "Epoch 027 Batch 010 Loss: 0.0536\n",
      "Epoch 027 Batch 011 Loss: 0.0596\n",
      "Epoch 027 Batch 012 Loss: 0.0775\n",
      "Epoch 027 Batch 013 Loss: 0.1551\n",
      "Epoch 027 Batch 014 Loss: 0.0765\n",
      "Epoch 027 Batch 015 Loss: 0.1146\n",
      "Epoch 027 Batch 016 Loss: 0.0596\n",
      "Epoch 027 Batch 017 Loss: 0.0781\n",
      "Epoch 027 Batch 018 Loss: 0.0817\n",
      "Epoch 027 Batch 019 Loss: 0.0969\n",
      "Epoch 027 Batch 020 Loss: 0.0554\n",
      "Epoch 027 Batch 021 Loss: 0.0692\n",
      "Epoch 027 Batch 022 Loss: 0.0713\n",
      "Epoch 027 Batch 023 Loss: 0.0522\n",
      "Epoch 027 Batch 024 Loss: 0.0903\n",
      "Epoch 027 Batch 025 Loss: 0.0798\n",
      "Epoch 027 Batch 026 Loss: 0.0697\n",
      "Epoch 027 Batch 027 Loss: 0.0745\n",
      "Epoch 027 Batch 028 Loss: 0.0687\n",
      "Epoch 027 Batch 029 Loss: 0.0674\n",
      "Epoch 027 Batch 030 Loss: 0.0701\n",
      "Epoch 027 Batch 031 Loss: 0.0619\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 Batch 000 Loss: 0.0705\n",
      "Epoch 028 Batch 001 Loss: 0.0723\n",
      "Epoch 028 Batch 002 Loss: 0.0498\n",
      "Epoch 028 Batch 003 Loss: 0.0616\n",
      "Epoch 028 Batch 004 Loss: 0.0701\n",
      "Epoch 028 Batch 005 Loss: 0.0478\n",
      "Epoch 028 Batch 006 Loss: 0.0441\n",
      "Epoch 028 Batch 007 Loss: 0.0853\n",
      "Epoch 028 Batch 008 Loss: 0.0673\n",
      "Epoch 028 Batch 009 Loss: 0.0676\n",
      "Epoch 028 Batch 010 Loss: 0.0931\n",
      "Epoch 028 Batch 011 Loss: 0.0641\n",
      "Epoch 028 Batch 012 Loss: 0.0488\n",
      "Epoch 028 Batch 013 Loss: 0.0754\n",
      "Epoch 028 Batch 014 Loss: 0.0557\n",
      "Epoch 028 Batch 015 Loss: 0.0463\n",
      "Epoch 028 Batch 016 Loss: 0.0487\n",
      "Epoch 028 Batch 017 Loss: 0.0295\n",
      "Epoch 028 Batch 018 Loss: 0.0425\n",
      "Epoch 028 Batch 019 Loss: 0.0488\n",
      "Epoch 028 Batch 020 Loss: 0.0663\n",
      "Epoch 028 Batch 021 Loss: 0.0607\n",
      "Epoch 028 Batch 022 Loss: 0.0397\n",
      "Epoch 028 Batch 023 Loss: 0.0732\n",
      "Epoch 028 Batch 024 Loss: 0.0808\n",
      "Epoch 028 Batch 025 Loss: 0.1090\n",
      "Epoch 028 Batch 026 Loss: 0.0561\n",
      "Epoch 028 Batch 027 Loss: 0.0421\n",
      "Epoch 028 Batch 028 Loss: 0.0559\n",
      "Epoch 028 Batch 029 Loss: 0.0407\n",
      "Epoch 028 Batch 030 Loss: 0.1039\n",
      "Epoch 028 Batch 031 Loss: 0.0541\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8500000238418579\n",
      "-----------------------*-----------------------\n",
      "Epoch 029 Batch 000 Loss: 0.0935\n",
      "Epoch 029 Batch 001 Loss: 0.0858\n",
      "Epoch 029 Batch 002 Loss: 0.0732\n",
      "Epoch 029 Batch 003 Loss: 0.0478\n",
      "Epoch 029 Batch 004 Loss: 0.0712\n",
      "Epoch 029 Batch 005 Loss: 0.0483\n",
      "Epoch 029 Batch 006 Loss: 0.0387\n",
      "Epoch 029 Batch 007 Loss: 0.0833\n",
      "Epoch 029 Batch 008 Loss: 0.0885\n",
      "Epoch 029 Batch 009 Loss: 0.0498\n",
      "Epoch 029 Batch 010 Loss: 0.0777\n",
      "Epoch 029 Batch 011 Loss: 0.0697\n",
      "Epoch 029 Batch 012 Loss: 0.0923\n",
      "Epoch 029 Batch 013 Loss: 0.0908\n",
      "Epoch 029 Batch 014 Loss: 0.0657\n",
      "Epoch 029 Batch 015 Loss: 0.0627\n",
      "Epoch 029 Batch 016 Loss: 0.0659\n",
      "Epoch 029 Batch 017 Loss: 0.0359\n",
      "Epoch 029 Batch 018 Loss: 0.0521\n",
      "Epoch 029 Batch 019 Loss: 0.0857\n",
      "Epoch 029 Batch 020 Loss: 0.0429\n",
      "Epoch 029 Batch 021 Loss: 0.0768\n",
      "Epoch 029 Batch 022 Loss: 0.0450\n",
      "Epoch 029 Batch 023 Loss: 0.0675\n",
      "Epoch 029 Batch 024 Loss: 0.0471\n",
      "Epoch 029 Batch 025 Loss: 0.0638\n",
      "Epoch 029 Batch 026 Loss: 0.0669\n",
      "Epoch 029 Batch 027 Loss: 0.0844\n",
      "Epoch 029 Batch 028 Loss: 0.0640\n",
      "Epoch 029 Batch 029 Loss: 0.0823\n",
      "Epoch 029 Batch 030 Loss: 0.0571\n",
      "Epoch 029 Batch 031 Loss: 0.0781\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.8285714387893677\n",
      "-----------------------*-----------------------\n",
      "training complete!\n",
      "best accuracy:  0.8500000238418579\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cdf35f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfae8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8928571343421936"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
