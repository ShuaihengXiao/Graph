{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "05191f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "from tqdm import trange, tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d81e0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_graph import random_graph_gcn\n",
    "node_df, edge_df = random_graph_gcn(1000,3000,report_rate=0.5, driver_rate=0.8,nums_features=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7600519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 123)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a086f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据格式处理\n",
    "# 数据集被分成num_clusters个子数据集（子图）->其实相当于，被分成了几个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7a60c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d80a4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# node_lookup: store node index\n",
    "node_lookup = pd.DataFrame({'node': node_df.index,}, index=node_df.cust_id)\n",
    "\n",
    "# delete no-edge-node \n",
    "diff_node = list(set(node_df['cust_id'])-(set(node_df['cust_id']) - set(edge_df['cust_id']) - set(edge_df['opp_id'])))\n",
    "\n",
    "node_df = node_df.iloc[node_lookup.iloc[diff_node]['node']].reset_index(drop=True)\n",
    "\n",
    "#build up graph\n",
    "graph = nx.from_edgelist([(cust,opp) for cust, opp in zip(edge_df['cust_id'],edge_df['opp_id'])])\n",
    "\n",
    "# random_clustering\n",
    "clusters = [cluster for cluster in range(num_clusters)]\n",
    "cluster_membership = {node : random.choice(clusters) for node in graph.nodes()}\n",
    "\n",
    "# build-up membership dict\n",
    "sg_nodes = {}\n",
    "sg_edges = {}\n",
    "sg_train_nodes = {}\n",
    "sg_test_nodes = {}\n",
    "sg_train_features = {}\n",
    "sg_test_features = {}\n",
    "sg_train_targets = {}\n",
    "sg_test_targets = {}\n",
    "\n",
    "for cluster in clusters:\n",
    "    #print(cluster)\n",
    "    subgraph = graph.subgraph([node for node in sorted(graph.nodes()) if cluster_membership[node] == cluster])\n",
    "    sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "\n",
    "    mapper = {node: i for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "    sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "\n",
    "    sg_train_nodes[cluster] = [node for node in node_df[node_df['is_driver'] == True]['cust_id'] if node in sg_nodes[cluster]]\n",
    "    sg_test_nodes[cluster] = [node for node in node_df[node_df['is_driver'] == False]['cust_id'] if node in sg_nodes[cluster]]\n",
    "\n",
    "    sg_test_nodes[cluster] = sorted(sg_test_nodes[cluster])\n",
    "    sg_train_nodes[cluster] = sorted(sg_train_nodes[cluster])\n",
    "    \n",
    "    feats_name = list(set(node_df.columns) - set(['cust_id','is_driver','is_reported']))\n",
    "    sg_train_features[cluster] = pd.concat([node_df[(node_df['cust_id'] == cust)&(node_df['is_driver'] == True)][feats_name] for cust in sg_nodes[cluster]],axis = 0)\n",
    "    sg_test_features[cluster] = pd.concat([node_df[(node_df['cust_id'] == cust)&(node_df['is_driver'] == False)][feats_name] for cust in sg_nodes[cluster]],axis = 0)\n",
    "    sg_train_targets[cluster] = pd.concat([node_df[(node_df['cust_id'] == cust)&(node_df['is_driver'] == True)][['is_reported']] for cust in sg_nodes[cluster]],axis = 0)\n",
    "    sg_test_targets[cluster] = pd.concat([node_df[(node_df['cust_id'] == cust)&(node_df['is_driver'] == False)][['is_reported']] for cust in sg_nodes[cluster]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c01bd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph,sg_nodes,sg_edges,sg_train_nodes,sg_test_nodes,sg_features,sg_targets = data_format_process(node_df,edge_df,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733b0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ClusterGraph_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dee05ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ClusterGraph_v0.preprocessing(node_df,edge_df,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326a30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph,data_dict = a.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde30c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cf28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "795b8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing():\n",
    "    \n",
    "    def __init__(self, node_df, edge_df, num_clusters):\n",
    "        \n",
    "        \n",
    "        assert all(col in node_df.columns for col in ['cust_id','is_driver','is_reported'])\n",
    "        assert all(col in edge_df.columns for col in ['cust_id','opp_id'])\n",
    "        assert type(num_clusters) == int\n",
    "        \n",
    "        self.node_df = node_df\n",
    "        self.edge_df = edge_df\n",
    "        self.num_clusters = num_clusters\n",
    "        self.data_dict = {}\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        self.delete_nodes()\n",
    "        graph = self.build_graph(self.edge_df)\n",
    "        clusters, cluster_membership = self.random_clustering(graph)\n",
    "        self.data_dict['sg_nodes'], self.data_dict['sg_edges'], self.data_dict['sg_train_nodes'], self.data_dict['sg_test_nodes'], self.data_dict['sg_train_features'], self.data_dict['sg_test_features'], self.data_dict['sg_train_targets'], self.data_dict['sg_test_targets'] = self.build_membership_dict(graph,clusters, cluster_membership)\n",
    "        \n",
    "        \n",
    "        return graph, self.data_dict\n",
    "    \n",
    "    \n",
    "    def delete_nodes(self):\n",
    "        \n",
    "        # node_lookup: store node index\n",
    "        node_lookup = pd.DataFrame({'node': self.node_df.index,}, index=self.node_df.cust_id)\n",
    "\n",
    "        # delete no-edge-node \n",
    "        diff_node = list(set(self.node_df['cust_id'])-(set(self.node_df['cust_id']) - set(self.edge_df['cust_id']) - set(self.edge_df['opp_id'])))\n",
    "\n",
    "        self.node_df = self.node_df.iloc[node_lookup.iloc[diff_node]['node']].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "    def build_graph(self, edge_df):\n",
    "        \n",
    "        #build up graph using networkx\n",
    "        graph = nx.from_edgelist([(cust,opp) for cust, opp in zip(edge_df['cust_id'],edge_df['opp_id'])])\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def random_clustering(self,graph):\n",
    "        \n",
    "        # random_clustering\n",
    "        clusters = [cluster for cluster in range(self.num_clusters)]\n",
    "        cluster_membership = {node : random.choice(clusters) for node in graph.nodes()}\n",
    "        \n",
    "        return clusters, cluster_membership\n",
    "    \n",
    "    def build_membership_dict(self,graph,clusters, cluster_membership):\n",
    "        \n",
    "        # build-up membership dict\n",
    "        sg_nodes = {}\n",
    "        sg_edges = {}\n",
    "        sg_train_nodes = {}\n",
    "        sg_test_nodes = {}\n",
    "        sg_train_features = {}\n",
    "        sg_test_features = {}\n",
    "        sg_train_targets = {}\n",
    "        sg_test_targets = {}\n",
    "\n",
    "        for cluster in clusters:\n",
    "\n",
    "            #print(cluster)\n",
    "            subgraph = graph.subgraph([node for node in sorted(graph.nodes()) if cluster_membership[node] == cluster])\n",
    "            sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "\n",
    "            mapper = {node: i for i, node in enumerate(sorted(sg_nodes[cluster]))}\n",
    "            sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "\n",
    "            sg_train_nodes[cluster] = [node for node in self.node_df[self.node_df['is_driver'] == True]['cust_id'] if node in sg_nodes[cluster]]\n",
    "            sg_test_nodes[cluster] = [node for node in self.node_df[self.node_df['is_driver'] == False]['cust_id'] if node in sg_nodes[cluster]]\n",
    "\n",
    "            sg_test_nodes[cluster] = sorted(sg_test_nodes[cluster])\n",
    "            sg_train_nodes[cluster] = sorted(sg_train_nodes[cluster])\n",
    "            \n",
    "            feats_name = list(set(self.node_df.columns) - set(['cust_id','is_driver','is_reported']))\n",
    "            sg_train_features[cluster] = pd.concat([self.node_df[(self.node_df['cust_id'] == cust)&(self.node_df['is_driver'] == True)][feats_name] for cust in sg_nodes[cluster]],axis = 0)\n",
    "            sg_test_features[cluster] = pd.concat([self.node_df[(self.node_df['cust_id'] == cust)&(self.node_df['is_driver'] == False)][feats_name] for cust in sg_nodes[cluster]],axis = 0)\n",
    "            sg_train_targets[cluster] = pd.concat([self.node_df[(self.node_df['cust_id'] == cust)&(self.node_df['is_driver'] == True)][['is_reported']] * 1 for cust in sg_nodes[cluster]],axis = 0)\n",
    "            sg_test_targets[cluster] = pd.concat([self.node_df[(self.node_df['cust_id'] == cust)&(self.node_df['is_driver'] == False)][['is_reported']] * 1 for cust in sg_nodes[cluster]],axis = 0)\n",
    "          \n",
    "        return sg_nodes, sg_edges, sg_train_nodes, sg_test_nodes, sg_train_features, sg_test_features, sg_train_targets, sg_test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd27fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义gcn\n",
    "class GCN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, activation = torch.nn.functional.relu):\n",
    "        \n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.weight1 = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim))\n",
    "        #self.weight2 = nn.Parameter(torch.Tensor(self.input_dim * 2, self.output_dim))\n",
    "        self.bn1 = nn.BatchNorm1d(self.output_dim)\n",
    "        #self.bn2 = nn.BatchNorm1d(self.output_dim)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        torch.nn.init.kaiming_uniform_(self.weight1)\n",
    "        #torch.nn.init.kaiming_uniform_(self.weight2.weight)\n",
    "        \n",
    "    def forward(self,features):\n",
    "        \n",
    "        output = self.bn1(self.activation(torch.matmul(features,self.weight1)))\n",
    "        #print(output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b685ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class residul_block(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,):\n",
    "        \n",
    "        super(residul_block, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gcn = GCN(self.input_dim, self.output_dim)\n",
    "        self.linear_1 = nn.Linear(output_dim,output_dim * 2)\n",
    "        self.linear_2 = nn.Linear(output_dim * 2,output_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.output_dim * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.gcn(features)\n",
    "        dummy = output\n",
    "        output = self.linear_1(output)\n",
    "        output = self.bn1(output)\n",
    "        output = self.linear_2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = torch.add(output,dummy)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "da5c3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3ad9f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedGCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer GCN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dims, input_channels, output_channels):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :input_channels: Number of features.\n",
    "        :output_channels: Number of target features. \n",
    "        \"\"\"\n",
    "        super(StackedGCN, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.all_dims = [self.input_channels] + self.hidden_dims + [self.output_channels]\n",
    "        for i, _ in enumerate(self.all_dims[:-1]):\n",
    "            self.layers.append(residul_block(self.all_dims[i],self.all_dims[i+1]))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Making a forward pass.\n",
    "        :param edges: Edge list LongTensor.\n",
    "        :param features: Feature matrix input FLoatTensor.\n",
    "        :return predictions: Prediction matrix output FLoatTensor.\n",
    "        \"\"\"\n",
    "        #print(self.layers)\n",
    "        for i, _ in enumerate(self.all_dims[:-2]):\n",
    "            features = torch.nn.functional.relu(self.layers[i](features))\n",
    "            if i>1:\n",
    "                features = torch.nn.functional.dropout(features,0.3)\n",
    "        features = self.layers[i+1](features)\n",
    "        predictions = torch.nn.functional.log_softmax(features, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4e25e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = preprocessing(node_df, edge_df, 7)\n",
    "graph, data_dict = pre_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ee7b6c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['sg_train_features'][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "03db2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_model():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 node_df,\n",
    "                 edge_df,\n",
    "                 num_clusters,\n",
    "                 hidden_dims,\n",
    "                 epochs = 20,\n",
    "                 lr = 0.01,\n",
    "                 device = 'cpu'):\n",
    "        \n",
    "        self.node_df = node_df\n",
    "        self.edge_df = edge_df\n",
    "        self.num_clusters = num_clusters\n",
    "        self.clusters = [cluster for cluster in range(self.num_clusters)]\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        \n",
    "        print('-----------------------*-----------------------')\n",
    "        print('data preprocessing ..\\n')\n",
    "        pre_model = preprocessing(self.node_df, self.edge_df, self.num_clusters)\n",
    "        self.graph, self.data_dict = pre_model.run()\n",
    "        print('preprocessing completed!')\n",
    "        print('-----------------------*-----------------------')\n",
    "        \n",
    "        self.feature_count = int(self.data_dict['sg_train_features'][0].shape[1])  #input dim\n",
    "        self.class_count = int(np.max(self.data_dict['sg_train_targets'][0]) + 1)  # output dim\n",
    "        \n",
    "        print('-----------------------*-----------------------')\n",
    "        print('model construction\\n')\n",
    "        self.creat_model()\n",
    "        self.ToTensor()\n",
    "        print(self.model)\n",
    "        print('-----------------------*-----------------------')\n",
    "        \n",
    "        \n",
    "    def creat_model(self):\n",
    "        \n",
    "        self.model = StackedGCN(self.hidden_dims,self.feature_count,self.class_count)\n",
    "        self.model = self.model.to(self.device)\n",
    "    \n",
    "    def ToTensor(self):\n",
    "        \n",
    "        for cluster in self.clusters:\n",
    "            self.data_dict['sg_nodes'][cluster] = torch.LongTensor(self.data_dict['sg_nodes'][cluster])\n",
    "            self.data_dict['sg_edges'][cluster] = torch.LongTensor(self.data_dict['sg_edges'][cluster]).t()\n",
    "            self.data_dict['sg_train_nodes'][cluster] = torch.LongTensor(self.data_dict['sg_train_nodes'][cluster])\n",
    "            self.data_dict['sg_test_nodes'][cluster] = torch.LongTensor(self.data_dict['sg_test_nodes'][cluster])\n",
    "            \n",
    "            self.data_dict['sg_train_features'][cluster] = torch.FloatTensor(np.array(self.data_dict['sg_train_features'][cluster]))\n",
    "            self.data_dict['sg_train_targets'][cluster] = torch.LongTensor(np.array(self.data_dict['sg_train_targets'][cluster]))\n",
    "            self.data_dict['sg_test_features'][cluster] = torch.FloatTensor(np.array(self.data_dict['sg_test_features'][cluster]))\n",
    "            self.data_dict['sg_test_targets'][cluster] = torch.LongTensor(np.array(self.data_dict['sg_test_targets'][cluster]))\n",
    "        \n",
    "    \n",
    "    def do_forward_pass(self, cluster):\n",
    "        \n",
    "        edges = self.data_dict['sg_edges'][cluster].to(self.device)\n",
    "        macro_nodes = self.data_dict['sg_nodes'][cluster].to(self.device)\n",
    "        train_nodes = self.data_dict['sg_train_nodes'][cluster].to(self.device)\n",
    "        train_features = self.data_dict['sg_train_features'][cluster].to(self.device)\n",
    "        train_target = self.data_dict['sg_train_targets'][cluster].to(self.device).squeeze()\n",
    "        predictions = self.model(train_features)\n",
    "#         print('predictions ',predictions.shape)\n",
    "#         print('train_target ', train_target.shape)\n",
    "#         print('train_nodes', train_nodes.shape)\n",
    "        #average_loss = torch.nn.functional.nll_loss(predictions[train_nodes], train_target[train_nodes])\n",
    "        average_loss = torch.nn.functional.nll_loss(predictions, train_target)\n",
    "        node_count = train_nodes.shape[0]\n",
    "\n",
    "        return average_loss, node_count\n",
    "    \n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        edges = self.data_dict['sg_edges'][cluster].to(self.device)\n",
    "        macro_nodes = self.data_dict['sg_nodes'][cluster].to(self.device)\n",
    "        test_nodes = self.data_dict['sg_test_nodes'][cluster].to(self.device)\n",
    "        test_features = self.data_dict['sg_test_features'][cluster].to(self.device)\n",
    "        test_target = self.data_dict['sg_test_targets'][cluster].to(self.device).squeeze()\n",
    "\n",
    "        prediction = self.model(test_features)\n",
    "        \n",
    "        return prediction, test_target\n",
    "    \n",
    "    def update_average_loss(self, batch_average_loss, node_count):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n",
    "        self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss/self.node_count_seen\n",
    "        \n",
    "        return average_loss\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Training a model.\n",
    "        \"\"\"\n",
    "        print(\"Training started.\\n\")\n",
    "        #epochs = trange(self.epochs, desc = \"Train Loss\")\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            random.shuffle(self.clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in self.clusters:\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_average_loss, node_count = self.do_forward_pass(cluster)\n",
    "                batch_average_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                average_loss = self.update_average_loss(batch_average_loss, node_count)\n",
    "                print(\"Epoch {:03d} Cluster {:03d} Loss: {:.4f}\".format(epoch, cluster, average_loss))\n",
    "            #epochs.set_description(\"Train Loss: %g\" % round(average_loss,4))\n",
    "            self.test()\n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        for cluster in self.clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "            self.predictions.append(prediction.cpu().detach().numpy())\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "        self.predictions = np.concatenate(self.predictions).argmax(1)\n",
    "        score = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "        print('-----------------------*-----------------------')\n",
    "        print(\"test accuracy: \", sum(self.predictions == self.targets)/len(self.targets))\n",
    "        print(\"\\nF-1 score: {:.4f}\".format(score))\n",
    "        print('-----------------------*-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "48b87ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------*-----------------------\n",
      "data preprocessing ..\n",
      "\n",
      "preprocessing completed!\n",
      "-----------------------*-----------------------\n",
      "-----------------------*-----------------------\n",
      "model construction\n",
      "StackedGCN(\n",
      "  (layers): ListModule(\n",
      "    (0): residul_block(\n",
      "      (gcn): GCN(\n",
      "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (linear_2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): residul_block(\n",
      "      (gcn): GCN(\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_1): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): residul_block(\n",
      "      (gcn): GCN(\n",
      "        (bn1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_1): Linear(in_features=2, out_features=4, bias=True)\n",
      "      (linear_2): Linear(in_features=4, out_features=2, bias=True)\n",
      "      (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-----------------------*-----------------------\n"
     ]
    }
   ],
   "source": [
    "a = run_model(node_df,edge_df,num_clusters = 10,hidden_dims = [128,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fc541e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "\n",
      "Epoch 000 Cluster 003 Loss: 0.9521\n",
      "Epoch 000 Cluster 006 Loss: 0.9012\n",
      "Epoch 000 Cluster 004 Loss: 0.9254\n",
      "Epoch 000 Cluster 002 Loss: 0.9120\n",
      "Epoch 000 Cluster 005 Loss: 0.8762\n",
      "Epoch 000 Cluster 008 Loss: 0.8442\n",
      "Epoch 000 Cluster 007 Loss: 0.8344\n",
      "Epoch 000 Cluster 000 Loss: 0.8195\n",
      "Epoch 000 Cluster 009 Loss: 0.8022\n",
      "Epoch 000 Cluster 001 Loss: 0.7897\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.5540540540540541\n",
      "\n",
      "F-1 score: 0.5541\n",
      "-----------------------*-----------------------\n",
      "Epoch 001 Cluster 005 Loss: 1.5781\n",
      "Epoch 001 Cluster 000 Loss: 1.2038\n",
      "Epoch 001 Cluster 004 Loss: 1.1202\n",
      "Epoch 001 Cluster 003 Loss: 1.0155\n",
      "Epoch 001 Cluster 008 Loss: 0.9470\n",
      "Epoch 001 Cluster 009 Loss: 0.8949\n",
      "Epoch 001 Cluster 007 Loss: 0.8645\n",
      "Epoch 001 Cluster 001 Loss: 0.8374\n",
      "Epoch 001 Cluster 006 Loss: 0.8132\n",
      "Epoch 001 Cluster 002 Loss: 0.8004\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 002 Cluster 007 Loss: 0.6575\n",
      "Epoch 002 Cluster 000 Loss: 0.6667\n",
      "Epoch 002 Cluster 008 Loss: 0.6697\n",
      "Epoch 002 Cluster 005 Loss: 0.6755\n",
      "Epoch 002 Cluster 006 Loss: 0.6682\n",
      "Epoch 002 Cluster 001 Loss: 0.6633\n",
      "Epoch 002 Cluster 002 Loss: 0.6673\n",
      "Epoch 002 Cluster 009 Loss: 0.6619\n",
      "Epoch 002 Cluster 004 Loss: 0.6645\n",
      "Epoch 002 Cluster 003 Loss: 0.6624\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 003 Cluster 003 Loss: 0.6463\n",
      "Epoch 003 Cluster 009 Loss: 0.6351\n",
      "Epoch 003 Cluster 004 Loss: 0.6513\n",
      "Epoch 003 Cluster 008 Loss: 0.6573\n",
      "Epoch 003 Cluster 000 Loss: 0.6610\n",
      "Epoch 003 Cluster 001 Loss: 0.6570\n",
      "Epoch 003 Cluster 006 Loss: 0.6546\n",
      "Epoch 003 Cluster 007 Loss: 0.6550\n",
      "Epoch 003 Cluster 002 Loss: 0.6595\n",
      "Epoch 003 Cluster 005 Loss: 0.6628\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 004 Cluster 005 Loss: 0.6952\n",
      "Epoch 004 Cluster 006 Loss: 0.6662\n",
      "Epoch 004 Cluster 001 Loss: 0.6576\n",
      "Epoch 004 Cluster 000 Loss: 0.6616\n",
      "Epoch 004 Cluster 009 Loss: 0.6550\n",
      "Epoch 004 Cluster 003 Loss: 0.6536\n",
      "Epoch 004 Cluster 008 Loss: 0.6564\n",
      "Epoch 004 Cluster 007 Loss: 0.6566\n",
      "Epoch 004 Cluster 002 Loss: 0.6604\n",
      "Epoch 004 Cluster 004 Loss: 0.6630\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 005 Cluster 005 Loss: 0.6930\n",
      "Epoch 005 Cluster 003 Loss: 0.6666\n",
      "Epoch 005 Cluster 001 Loss: 0.6581\n",
      "Epoch 005 Cluster 009 Loss: 0.6499\n",
      "Epoch 005 Cluster 000 Loss: 0.6549\n",
      "Epoch 005 Cluster 004 Loss: 0.6600\n",
      "Epoch 005 Cluster 008 Loss: 0.6620\n",
      "Epoch 005 Cluster 006 Loss: 0.6594\n",
      "Epoch 005 Cluster 007 Loss: 0.6592\n",
      "Epoch 005 Cluster 002 Loss: 0.6625\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 006 Cluster 008 Loss: 0.6744\n",
      "Epoch 006 Cluster 007 Loss: 0.6666\n",
      "Epoch 006 Cluster 004 Loss: 0.6733\n",
      "Epoch 006 Cluster 003 Loss: 0.6656\n",
      "Epoch 006 Cluster 002 Loss: 0.6703\n",
      "Epoch 006 Cluster 005 Loss: 0.6731\n",
      "Epoch 006 Cluster 006 Loss: 0.6691\n",
      "Epoch 006 Cluster 009 Loss: 0.6645\n",
      "Epoch 006 Cluster 000 Loss: 0.6654\n",
      "Epoch 006 Cluster 001 Loss: 0.6632\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 007 Cluster 008 Loss: 0.6734\n",
      "Epoch 007 Cluster 002 Loss: 0.6815\n",
      "Epoch 007 Cluster 000 Loss: 0.6783\n",
      "Epoch 007 Cluster 007 Loss: 0.6738\n",
      "Epoch 007 Cluster 004 Loss: 0.6765\n",
      "Epoch 007 Cluster 006 Loss: 0.6704\n",
      "Epoch 007 Cluster 003 Loss: 0.6665\n",
      "Epoch 007 Cluster 001 Loss: 0.6630\n",
      "Epoch 007 Cluster 009 Loss: 0.6584\n",
      "Epoch 007 Cluster 005 Loss: 0.6618\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 008 Cluster 009 Loss: 0.6159\n",
      "Epoch 008 Cluster 004 Loss: 0.6540\n",
      "Epoch 008 Cluster 006 Loss: 0.6489\n",
      "Epoch 008 Cluster 000 Loss: 0.6567\n",
      "Epoch 008 Cluster 003 Loss: 0.6538\n",
      "Epoch 008 Cluster 007 Loss: 0.6544\n",
      "Epoch 008 Cluster 001 Loss: 0.6516\n",
      "Epoch 008 Cluster 005 Loss: 0.6570\n",
      "Epoch 008 Cluster 002 Loss: 0.6615\n",
      "Epoch 008 Cluster 008 Loss: 0.6628\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 009 Cluster 005 Loss: 0.6930\n",
      "Epoch 009 Cluster 004 Loss: 0.6895\n",
      "Epoch 009 Cluster 007 Loss: 0.6797\n",
      "Epoch 009 Cluster 000 Loss: 0.6775\n",
      "Epoch 009 Cluster 008 Loss: 0.6768\n",
      "Epoch 009 Cluster 006 Loss: 0.6732\n",
      "Epoch 009 Cluster 002 Loss: 0.6748\n",
      "Epoch 009 Cluster 009 Loss: 0.6712\n",
      "Epoch 009 Cluster 001 Loss: 0.6691\n",
      "Epoch 009 Cluster 003 Loss: 0.6671\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 010 Cluster 004 Loss: 0.6856\n",
      "Epoch 010 Cluster 005 Loss: 0.6888\n",
      "Epoch 010 Cluster 007 Loss: 0.6783\n",
      "Epoch 010 Cluster 001 Loss: 0.6671\n",
      "Epoch 010 Cluster 003 Loss: 0.6614\n",
      "Epoch 010 Cluster 008 Loss: 0.6644\n",
      "Epoch 010 Cluster 002 Loss: 0.6698\n",
      "Epoch 010 Cluster 000 Loss: 0.6709\n",
      "Epoch 010 Cluster 006 Loss: 0.6671\n",
      "Epoch 010 Cluster 009 Loss: 0.6620\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 011 Cluster 001 Loss: 0.6333\n",
      "Epoch 011 Cluster 004 Loss: 0.6639\n",
      "Epoch 011 Cluster 008 Loss: 0.6689\n",
      "Epoch 011 Cluster 005 Loss: 0.6760\n",
      "Epoch 011 Cluster 009 Loss: 0.6640\n",
      "Epoch 011 Cluster 002 Loss: 0.6690\n",
      "Epoch 011 Cluster 007 Loss: 0.6675\n",
      "Epoch 011 Cluster 003 Loss: 0.6644\n",
      "Epoch 011 Cluster 006 Loss: 0.6620\n",
      "Epoch 011 Cluster 000 Loss: 0.6631\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 012 Cluster 004 Loss: 0.6864\n",
      "Epoch 012 Cluster 001 Loss: 0.6629\n",
      "Epoch 012 Cluster 005 Loss: 0.6716\n",
      "Epoch 012 Cluster 002 Loss: 0.6759\n",
      "Epoch 012 Cluster 000 Loss: 0.6749\n",
      "Epoch 012 Cluster 006 Loss: 0.6699\n",
      "Epoch 012 Cluster 007 Loss: 0.6686\n",
      "Epoch 012 Cluster 008 Loss: 0.6691\n",
      "Epoch 012 Cluster 003 Loss: 0.6666\n",
      "Epoch 012 Cluster 009 Loss: 0.6627\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 013 Cluster 004 Loss: 0.6876\n",
      "Epoch 013 Cluster 009 Loss: 0.6529\n",
      "Epoch 013 Cluster 005 Loss: 0.6667\n",
      "Epoch 013 Cluster 006 Loss: 0.6593\n",
      "Epoch 013 Cluster 000 Loss: 0.6630\n",
      "Epoch 013 Cluster 007 Loss: 0.6622\n",
      "Epoch 013 Cluster 008 Loss: 0.6645\n",
      "Epoch 013 Cluster 003 Loss: 0.6615\n",
      "Epoch 013 Cluster 001 Loss: 0.6586\n",
      "Epoch 013 Cluster 002 Loss: 0.6625\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 014 Cluster 005 Loss: 0.6991\n",
      "Epoch 014 Cluster 004 Loss: 0.6953\n",
      "Epoch 014 Cluster 002 Loss: 0.6934\n",
      "Epoch 014 Cluster 000 Loss: 0.6871\n",
      "Epoch 014 Cluster 007 Loss: 0.6821\n",
      "Epoch 014 Cluster 003 Loss: 0.6765\n",
      "Epoch 014 Cluster 009 Loss: 0.6710\n",
      "Epoch 014 Cluster 006 Loss: 0.6683\n",
      "Epoch 014 Cluster 001 Loss: 0.6659\n",
      "Epoch 014 Cluster 008 Loss: 0.6666\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 015 Cluster 002 Loss: 0.6881\n",
      "Epoch 015 Cluster 005 Loss: 0.6901\n",
      "Epoch 015 Cluster 003 Loss: 0.6735\n",
      "Epoch 015 Cluster 007 Loss: 0.6699\n",
      "Epoch 015 Cluster 000 Loss: 0.6707\n",
      "Epoch 015 Cluster 009 Loss: 0.6624\n",
      "Epoch 015 Cluster 008 Loss: 0.6643\n",
      "Epoch 015 Cluster 006 Loss: 0.6611\n",
      "Epoch 015 Cluster 001 Loss: 0.6583\n",
      "Epoch 015 Cluster 004 Loss: 0.6616\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 016 Cluster 004 Loss: 0.6956\n",
      "Epoch 016 Cluster 005 Loss: 0.6976\n",
      "Epoch 016 Cluster 007 Loss: 0.6842\n",
      "Epoch 016 Cluster 001 Loss: 0.6716\n",
      "Epoch 016 Cluster 003 Loss: 0.6652\n",
      "Epoch 016 Cluster 006 Loss: 0.6611\n",
      "Epoch 016 Cluster 009 Loss: 0.6555\n",
      "Epoch 016 Cluster 002 Loss: 0.6602\n",
      "Epoch 016 Cluster 000 Loss: 0.6618\n",
      "Epoch 016 Cluster 008 Loss: 0.6631\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 017 Cluster 002 Loss: 0.6904\n",
      "Epoch 017 Cluster 007 Loss: 0.6755\n",
      "Epoch 017 Cluster 000 Loss: 0.6742\n",
      "Epoch 017 Cluster 004 Loss: 0.6770\n",
      "Epoch 017 Cluster 006 Loss: 0.6703\n",
      "Epoch 017 Cluster 005 Loss: 0.6730\n",
      "Epoch 017 Cluster 001 Loss: 0.6690\n",
      "Epoch 017 Cluster 003 Loss: 0.6663\n",
      "Epoch 017 Cluster 008 Loss: 0.6670\n",
      "Epoch 017 Cluster 009 Loss: 0.6633\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 018 Cluster 004 Loss: 0.6867\n",
      "Epoch 018 Cluster 003 Loss: 0.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 Cluster 007 Loss: 0.6620\n",
      "Epoch 018 Cluster 005 Loss: 0.6696\n",
      "Epoch 018 Cluster 009 Loss: 0.6592\n",
      "Epoch 018 Cluster 008 Loss: 0.6622\n",
      "Epoch 018 Cluster 002 Loss: 0.6672\n",
      "Epoch 018 Cluster 000 Loss: 0.6682\n",
      "Epoch 018 Cluster 006 Loss: 0.6649\n",
      "Epoch 018 Cluster 001 Loss: 0.6621\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n",
      "Epoch 019 Cluster 005 Loss: 0.6965\n",
      "Epoch 019 Cluster 008 Loss: 0.6851\n",
      "Epoch 019 Cluster 006 Loss: 0.6692\n",
      "Epoch 019 Cluster 004 Loss: 0.6737\n",
      "Epoch 019 Cluster 002 Loss: 0.6769\n",
      "Epoch 019 Cluster 009 Loss: 0.6685\n",
      "Epoch 019 Cluster 000 Loss: 0.6691\n",
      "Epoch 019 Cluster 003 Loss: 0.6662\n",
      "Epoch 019 Cluster 001 Loss: 0.6635\n",
      "Epoch 019 Cluster 007 Loss: 0.6631\n",
      "-----------------------*-----------------------\n",
      "test accuracy:  0.6238738738738738\n",
      "\n",
      "F-1 score: 0.6239\n",
      "-----------------------*-----------------------\n"
     ]
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a594bee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6126126126126126"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions == targets)/len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c2d1bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.FloatTensor(np.array(data_dict['sg_train_features'][10]))\n",
    "edges = torch.LongTensor(data_dict['sg_edges'][10]).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e7172e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedGCN([128,64],120,2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b8a5c020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackedGCN(\n",
      "  (layers): ListModule(\n",
      "    (0): residul_block(\n",
      "      (gcn): GCN(\n",
      "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (linear_2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): residul_block(\n",
      "      (gcn): GCN(\n",
      "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_1): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): residul_block(\n",
      "      (gcn): GCN(\n",
      "        (bn1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (linear_1): Linear(in_features=2, out_features=4, bias=True)\n",
      "      (linear_2): Linear(in_features=4, out_features=2, bias=True)\n",
      "      (bn1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48446d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    model(features)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af02697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f32f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
